<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>hive-mapjoin探索</title>
    <link href="/2022/03/02/hive-mapjoin%E6%8E%A2%E7%B4%A2/"/>
    <url>/2022/03/02/hive-mapjoin%E6%8E%A2%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<p>#让你真正了解mapjoin参数</p><p>首先创建好small_table和big_table俩个表。直接给俩个表执行join。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs hiveql">set hive.cbo.enable=false;<br>set hive.auto.convert.join=false; <br>set hive.auto.convert.join.noconditionaltask=false; <br>explain select t1.* from small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>接下来分析执行计划，一共俩个stage，第一个stage是join，第二个是进行fetch数据</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">STAGE</span> DEPENDENCIES:<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">1</span> is a root stage<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">0</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">1</span> 其实stage0就是fetch算子，把结果拉到client端<br></code></pre></td></tr></table></figure><p>分析stage-0,看fetch数据就是fetch operator</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">Stage</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Stage-0</span><br>  <span class="hljs-attribute">Fetch Operator</span><br><span class="hljs-attribute">    limit</span><span class="hljs-punctuation">:</span> <span class="hljs-string">-1 代表所有</span><br>    <span class="hljs-attribute">Processor Tree</span><span class="hljs-punctuation">:</span><br>      ListSink <br></code></pre></td></tr></table></figure><p>分析主要的阶段stage-1，这个阶段scan俩个表的数据，同时抽取join key进行了reduce操作。</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs arcade">STAGE PLANS:<br>  Stage: Stage<span class="hljs-number">-1</span><br>    <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Reduce</span>  代表这个stage就是一个mapreduce任务<br>      <span class="hljs-built_in">Map</span> Operator Tree:  这个代表<span class="hljs-built_in">map</span>算子的阶段，<span class="hljs-built_in">map</span>针对上面sql其实就是对俩个表进行scan，同时取对应对字段<br>      <span class="hljs-built_in">Reduce</span> Operator Tree:  这个代表<span class="hljs-built_in">reduce</span>算子的阶段，针对join对字段进行聚合<br></code></pre></td></tr></table></figure><p>先看下Map Operator Tree，需要注意的是join会自动把join字段的null去掉。 id is not null，看TableScan t1和TableScan t2区别就是第一个是有个value表达式，因为select只select了t1表的字段，其他没有什么区别，都是扫描表的数据。</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>            alias: t1<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              Reduce Output Operator<br>                key expressions: id (<span class="hljs-keyword">type</span>: int)<br>                sort <span class="hljs-keyword">order</span>: +<br>                <span class="hljs-built_in">Map</span><span class="hljs-params">-reduce</span> partition columns: id (<span class="hljs-keyword">type</span>: int)<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                value expressions: name (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">string</span>),<span class="hljs-params">...</span>f7 (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">string</span>)<br>          TableScan<br>            alias: t2<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              Reduce Output Operator<br>                key expressions: id (<span class="hljs-keyword">type</span>: int)<br>                sort <span class="hljs-keyword">order</span>: +<br>                <span class="hljs-built_in">Map</span><span class="hljs-params">-reduce</span> partition columns: id (<span class="hljs-keyword">type</span>: int)<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br></code></pre></td></tr></table></figure><p>最后就是reduce阶段</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs sqf">Reduce Operator Tree:<br>  <span class="hljs-built_in">Join</span> Operator<br>    condition map:<br>         Inner <span class="hljs-built_in">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>    <span class="hljs-built_in">keys</span>:<br>      <span class="hljs-number">0</span> id (<span class="hljs-built_in">type</span>: int)<br>      <span class="hljs-number">1</span> id (<span class="hljs-built_in">type</span>: int)<br>    outputColumnNames: <span class="hljs-variable">_col0</span>, <span class="hljs-variable">_col1</span>, <span class="hljs-variable">_col2</span>, <span class="hljs-variable">_col3</span>, <span class="hljs-variable">_col4</span>, <span class="hljs-variable">_col5</span>, <span class="hljs-variable">_col6</span>, <span class="hljs-variable">_col7</span>, <span class="hljs-variable">_col8</span>, <span class="hljs-variable">_col9</span>, <span class="hljs-variable">_col10</span><br>    Statistics: Num rows: <span class="hljs-number">10507</span> Data <span class="hljs-built_in">size</span>: <span class="hljs-number">1142980</span> Basic stats: COMPLETE Column stats: NONE<br>    File Output Operator<br>      compressed: <span class="hljs-literal">false</span><br>      Statistics: Num rows: <span class="hljs-number">10507</span> Data <span class="hljs-built_in">size</span>: <span class="hljs-number">1142980</span> Basic stats: COMPLETE Column stats: NONE<br>      table:<br>          input <span class="hljs-built_in">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat<br>          output <span class="hljs-built_in">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat<br>          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br></code></pre></td></tr></table></figure><p>以上就是没有开始mapjoin，让我们把mapjoin打开试一试，小表的大小是10M。测试一下开启mapjoin。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">set hive.cbo.enable=false;<br>set hive.auto.convert.join=false;<br>set hive.auto.convert.join.noconditionaltask=false;<br>set hive.auto.convert.join=true;<br>explain select t1.* from small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>先分析一下stage之间的依赖</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">STAGE</span> DEPENDENCIES:<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">4</span> is a root stage , consists of Stage-<span class="hljs-number">5</span>, Stage-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">5</span> has a backup stage: Stage-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">3</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">5</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">0</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">3</span>, Stage-<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>首先看stage4，这个stage是root，也就是最顶部的，stage Conditional Operator这是一个条件运算符，包括5和1。这快看不太懂什么意思，我们先看看看stage-1这个后边没有任何依赖。</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-1</span><br>    <span class="hljs-built_in">Map</span> Reduce<br>      <span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>          TableScan<br>      Reduce Operator Tree:<br>        <span class="hljs-keyword">Join</span> Operator<br>          condition <span class="hljs-built_in">map</span>:<br>               Inner <span class="hljs-keyword">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>          keys:<br>            <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>            <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br></code></pre></td></tr></table></figure><p>stage-1可以看出来，跟不开启mapjoin是一摸一样的，可能这个主要是现实最初的执行计划的。没有太多用处，即诶啊来看下stage-0这个主要是代表我们fetch数据的算子，也就是客户端拉取结果的</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">Stage</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Stage-0</span><br>    <span class="hljs-attribute">Fetch Operator</span><br><span class="hljs-attribute">      limit</span><span class="hljs-punctuation">:</span> <span class="hljs-string">-1</span><br>      <span class="hljs-attribute">Processor Tree</span><span class="hljs-punctuation">:</span><br>        ListSink<br></code></pre></td></tr></table></figure><p>跟原来的一样，这个stage-0依赖于3、1，1我们已经看过了，我们直接看3</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-3</span><br>    <span class="hljs-built_in">Map</span> Reduce<br>      <span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>            alias: t2<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">955184</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">103902452</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">955184</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">103902452</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              <span class="hljs-built_in">Map</span> <span class="hljs-keyword">Join</span> Operator<br>                condition <span class="hljs-built_in">map</span>:<br>                     Inner <span class="hljs-keyword">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>                keys:<br>                  <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>                  <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br>                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">1050702</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">114292699</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                File Output Operator<br>                  compressed: <span class="hljs-literal">false</span><br>                  Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">1050702</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">114292699</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                  table:<br>                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat<br>                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat<br>                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br>      <span class="hljs-built_in">Local</span> Work:<br>        <span class="hljs-built_in">Map</span> Reduce <span class="hljs-built_in">Local</span> Work<br></code></pre></td></tr></table></figure><p>这个阶段就是关键点，可以看到这是一个mapreduce任务，但是没有Reduce Operator，使用一个local work替代了，而且join的条件都放在了Map Operator里面了，所以这个就是mapjoin的真正逻辑，接下来看一下local work，因为stage-3依赖于stage-5，接下来看stage-5</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-5</span><br>   <span class="hljs-built_in">Map</span> Reduce <span class="hljs-built_in">Local</span> Work<br>     Alias -&gt; <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Local</span> Tables:<br>       t1<br>         Fetch Operator<br>           limit: <span class="hljs-number">-1</span><br>     Alias -&gt; <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Local</span> Operator Tree:<br>       t1<br>         TableScan<br>           alias: t1<br>           Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">95521</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10390248</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>           Filter Operator<br>             predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>             Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">95521</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10390248</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>             HashTable Sink Operator<br>               keys:<br>                 <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>                 <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br></code></pre></td></tr></table></figure><p>可以看到这个一个mapreduce的本地工作，跟stage-3里面的local work是能对上的，这个很好理解，就是把t1表通过本地加载，条件跟mapreduce里面的是一样的。</p><p>接下来，以后我们查看表的执行计划可以很快的分析出来。接下来让我们验证下看起cbo是否对mapjoin有影响。<br>首先我们对small小表直接插入100M数据，这个数据量不满足了mapjoin对条件，首先看关闭CBO</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.cbo.<span class="hljs-attribute">enable</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.<span class="hljs-attribute">noconditionaltask</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br>explain select t1.* <span class="hljs-keyword">from</span> small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>这个结果跟不开启mapjoin是一致对，因为俩个表对大小都超过了我们设定都阈值。这边可以判断cdp读取hive的元数据没有影响mapjoin的执行，因为我看到有的文章说有时候开启cbo以后mapjoin不生效（不过别人的文章是说的使用tez执行引擎，所以这点还有待验证）</p><p>我们测试开启cbo，开启cbo成本应该是从hive都元数据里面取数据都，但是由于我们是直接往hdfs上面放都数据，元数据small表还是10M</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.cbo.<span class="hljs-attribute">enable</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.mapjoin.smalltable.<span class="hljs-attribute">filesize</span>=460000000;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.<span class="hljs-attribute">noconditionaltask</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.noconditionaltask.<span class="hljs-attribute">size</span>=10000000; <br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br>explain select t1.* <span class="hljs-keyword">from</span> small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>parquet.md</title>
    <link href="/2022/03/02/parquet-md/"/>
    <url>/2022/03/02/parquet-md/</url>
    
    <content type="html"><![CDATA[<p>#PARQUET</p><p>##优势</p><figure class="highlight node-repl"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs node-repl">1、更高的压缩比可以减少磁盘的使用<br>2、更少的IO操作使用映射下推和谓词下推，可以读取需要的列，过滤掉不需要的列，减少不必要的数据扫描，尤其对于表字段比较多的时候优势明显。<br><span class="hljs-meta">&gt;</span> <span class="language-javascript">映射下推：列式存储最突出的优势，数据扫描的时候，只需要扫描需要的列，避免全表扫描</span><br><span class="hljs-meta">&gt;</span> <span class="language-javascript">谓词下推：将过滤条件在最底层执行减少结果集</span><br></code></pre></td></tr></table></figure><p>##如何提升parquet的查询性能</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-number">1</span>、对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以更好的利用最大值最小值实现谓词下推<br><span class="hljs-number">2</span>、减少行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的<span class="hljs-built_in">I</span><span class="hljs-operator">/</span><span class="hljs-built_in">O</span>负载。<br></code></pre></td></tr></table></figure><p>##文件格式<br>row group &gt; column chunk &gt; page</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs css">row group行组：每一个行组包含一定的行数，一般对应一个HDFS文件块，Parquet读写的时候会将整个行组缓存在内存中<br>column chunk列块：在一个行组中，每一列保存在一个列块中，一个列块中的类型都是相同的，不同类型的列块可以使用不同d的算法进行压缩。<br>page页：每一个列块被划分成多个页，页时最小的编码单位，在同一个列块的不同页可能使用不同的编码方式<br><br>一个parquet文件，在文件头和文件尾都是<span class="hljs-number">4</span>个字节的magic <span class="hljs-selector-tag">code</span>，校验是否是parquet文件。<br>然后尾部magic <span class="hljs-selector-tag">code</span>之前是<span class="hljs-selector-tag">footer</span> length，是文件元数据的大小，通过这个大小就可以计算出y元数据<span class="hljs-selector-tag">footer</span>的偏移量<br><span class="hljs-selector-tag">footer</span>中包括：<br><span class="hljs-number">1</span>、每个行组的元信息：由多少个列块组成；<br><span class="hljs-number">2</span>、每个列块的元信息：类型、路径、编码、第一个数据页的位置、第一个索引页的位置、压缩/未压缩大小、以及一些额外的配置kv<br><span class="hljs-number">3</span>、又有三种pag：<br>    数据页主要存储defination levels、repeatition levels、value<br>    字典页：存储列值的编码字段<br>    索引页：用来存储该页<br></code></pre></td></tr></table></figure><p>##parquet的编码格式<br><a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">https://github.com/apache/parquet-format/blob/master/Encodings.md</a></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">0</span>、PLAIN：如果没有其他的有效编码就不进行编码，也就是plain<br><span class="hljs-number">2</span>、PLAIN_DICTIONARY /<span class="hljs-number">8</span>、RLE_DICTIONARY：原始数据的字典编码，字典存储在字典页中对应page中的字典页。字典中的值使用RLE/<span class="hljs-type">bit</span>-packing<br>回合编码存储位整数。<br>   字典页格式（PLAIN）：字典中的条数-按字典顺序-使用普通编码<br>   数据页格式：用于对存储位<span class="hljs-number">1</span>个字节条目id进行编码的位宽（最大<span class="hljs-number">32</span>位），这个id使用RLE/<span class="hljs-type">bit</span>-packing打包编码的值，在<span class="hljs-number">2.0</span>中使用RLE/<span class="hljs-keyword">DICTIONARY</span>   <br><span class="hljs-number">3</span>、Run Length <span class="hljs-keyword">Encoding</span> / <span class="hljs-type">Bit</span>-Packing Hybrid：这种编码使用位打包和运行长度编码的组合来更有效地存储重复值<br>   RLE:行程编码，重复的值计数。<br>   <span class="hljs-type">bit</span>-packing：位打包主要用来解决序列化所需要的位数<br><span class="hljs-number">4</span>、<span class="hljs-type">Bit</span>-packed：仅支持编码重复和定义级别<br><span class="hljs-number">5</span>、Delta <span class="hljs-keyword">Encoding</span>：int32、int64<br><span class="hljs-number">6</span>、Delta-length byte <span class="hljs-keyword">array</span>：BYTE_ARRAY<br><span class="hljs-number">7</span>、Delta Strings：BYTE_ARRAY<br><span class="hljs-number">9</span>、Byte Stream Split： <span class="hljs-type">FLOAT</span> <span class="hljs-type">DOUBLE</span><br></code></pre></td></tr></table></figure><p>##parquet元数据使用什么序列化方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql">parqut的元数据是在整个<span class="hljs-type">row</span> <span class="hljs-keyword">group</span>都写完成之后再进行写入的，这时候写入的文件的元数据信息全部都保存再了内存中，如果要读取的话也是需要把元数据全部都<br>读取到内存中才行。<br>把内存中的数据序列化到磁盘中，使用了thrift的TCompactProtocol高性能序列化方式。<br>thrit相对于其他序列化方式性能比较好，兼容性可能比较差。<br></code></pre></td></tr></table></figure><blockquote><p><a href="https://blog.51cto.com/u_15080016/2620917">https://blog.51cto.com/u_15080016/2620917</a></p></blockquote><p>##Sriping&#x2F;Assembly算法</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stata">打散/组装算法，主要parquet如何打散和组装数据的。这个算法主要通过repetition <span class="hljs-keyword">levels</span>和definition <span class="hljs-keyword">levels</span>来描述文件的拆分和组装。重复级别主要<br>用来描述repeated的节点，在写入的适合等于它和前面的值从那一层节点不共享的。在读取的适合根据该值可以推断出哪一层需要创建一个新的节点；定义级别主要<br>用来记录哪些值使没有的使null值。可以很好的支持嵌套类型的存储。<br>repeated <span class="hljs-keyword">levels</span>主要是用来一列的作用的，defintition <span class="hljs-keyword">levels</span>用来描述一行的数据。主要防止一列有空值导致错位。<br>不同的列跳转使用了有限状态机<br></code></pre></td></tr></table></figure><p>##parquet的bloomfilter<br><a href="https://github.com/apache/parquet-format/blob/master/BloomFilter.md">https://github.com/apache/parquet-format/blob/master/BloomFilter.md</a></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">我们都知道，在parquet和orc等列式存储中，都会用到一些索引和统计信息：记录下一些列的最大值最小值，记录字典等。，来对查询进行谓词下推，<br>直接下推到文件上面，减少数据的传输。谓词下推可以说是数据库查询优化的一大手段。<br>但是在有一些没有最大值最小值，字典太大、数据基数非常大的情况下布隆过滤器可以起到很好的优化操作。<br>布隆过滤器可以过滤掉肯定不存在的数据，可能存在的数据来相应查询。可能存在的数据又可以通过构建<span class="hljs-type">bit</span>数组的大小来决定的。<span class="hljs-type">bit</span>数据占用的空间<br>非常小，以至于用很小的空间就可以达到很好的过滤效果。<br>其实布隆过滤器主要的目标就是为·高基数·的列启用谓词下推，同时使用更少的存储空间。同时如果某一列没有启用布隆过滤器也不会影响性能。<br><br>parquet使用的是一种split block布隆过滤器。就是一个列有多个block，每个block有<span class="hljs-number">8</span>个小块组成，小块的话其实就是<span class="hljs-type">bit</span>数组。然后通过<br>加盐（对应<span class="hljs-type">int</span>就是*对应一个数组保存着<span class="hljs-number">8</span>个<span class="hljs-number">32</span>位的整数一一对应）移位来取得每个<span class="hljs-type">bit</span>数组中的具体位置，然后进行插入。其中就是为了把对应的<br>值进行散列，使用了奇数*常数右移的一种散列整数方法。块越多准确率越高<br><br>针对其他类型的数据，parquet使用xxhash来做的转换<br><br>数据的话也是相当于元数据使用的thrift做的序列化<br><br>bloomfilter存储在了footer下面的<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span>的<span class="hljs-keyword">column</span> chunk下面，作为列的元数据。<br>敏感信息应该使用列加密<br></code></pre></td></tr></table></figure><p>##parquet文件支持哪些压缩方式</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">snappy:</span>google提供<br><span class="hljs-symbol">gzip:</span>基于RFC <span class="hljs-number">1952</span>定义的 GZIP 格式（zlib权威）<br><span class="hljs-symbol">lzo:</span><br><span class="hljs-symbol">brotli:</span><br><span class="hljs-symbol">lz4:</span><br><span class="hljs-symbol">zstd:</span><br><span class="hljs-symbol">lz4_raw:</span><br></code></pre></td></tr></table></figure><p>##parquet支持哪些数据类型<br>文件格式的数据类型</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">BOOLEAN</span>：<span class="hljs-number">1</span> 位布尔值<br><span class="hljs-attribute">INT32</span>：<span class="hljs-number">32</span> 位有符号整数<br><span class="hljs-attribute">INT64</span>：<span class="hljs-number">64</span> 位有符号整数<br><span class="hljs-attribute">INT96</span>：<span class="hljs-number">96</span> 位有符号整数<br><span class="hljs-attribute">FLOAT</span>：IEEE <span class="hljs-number">32</span> 位浮点值<br><span class="hljs-attribute">DOUBLE</span>：IEEE <span class="hljs-number">64</span> 位浮点值<br><span class="hljs-attribute">BYTE_ARRAY</span>：任意长的字节数组。<br></code></pre></td></tr></table></figure><p>逻辑数据类型<br>##parquet的列索引</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs applescript">columnIndex和offsetIndex；索引的结构和长度存储在l了columnchunk中<br>columnIndex：这个可以根据制定的列值直接到数据页。可以用于谓词下推上面。<br>offsetindex：这个可以按照行数进行索引，这个主要用来其他列有了过滤，跳过许多数据，那么这个列也会根据<span class="hljs-built_in">offset</span>去挑过对应的数据。<br><span class="hljs-number">1</span>、索引里面右数据页的最大值和最小值<br><span class="hljs-number">2</span>、针对排序的列只需要读取包含数据相关的确定的数据页<br><span class="hljs-number">3</span>、针对于谓词下推，可以通过字典以及索引找到确定的索引页<br><span class="hljs-number">4</span>、如果不需要操作也不会带来额外的开销<br><span class="hljs-number">5</span>、如果是已经排序的，仅仅存储边界元素<br><span class="hljs-number">6</span>、对于有序的列还会运用二分查找<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>orc.md</title>
    <link href="/2022/03/02/orc-md/"/>
    <url>/2022/03/02/orc-md/</url>
    
    <content type="html"><![CDATA[<p>#ORC(Optimized Record Columnar)<br>##orc支持哪些数据类型</p><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><pre><code class="hljs sql">整数<br>  <span class="hljs-type">boolean</span>：<span class="hljs-number">1</span>bit<br>  tinyint：<span class="hljs-number">8</span>bit<br>  <span class="hljs-type">smallint</span>：<span class="hljs-number">16</span>bit<br>  <span class="hljs-type">int</span>：<span class="hljs-number">32</span>bit<br>  <span class="hljs-type">bigint</span>：<span class="hljs-number">64</span>bit<br>浮点<br>  <span class="hljs-type">float</span>、<span class="hljs-keyword">double</span><br>字符串<br>  string、<span class="hljs-type">char</span>、<span class="hljs-type">varchar</span><br>字节<br>  <span class="hljs-type">binary</span> blobs<br>日期<span class="hljs-operator">/</span>时间<br>  <span class="hljs-type">timestamp</span>、<span class="hljs-type">timestamp</span> <span class="hljs-keyword">with</span> <span class="hljs-keyword">local</span> <span class="hljs-type">time</span> zone、<span class="hljs-type">date</span><br>复合类型<br>  struct、list、map、<span class="hljs-keyword">union</span><br></code></pre></td></tr></table></figure><p>##orc支持哪些索引</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">file <span class="hljs-keyword">level</span>文件级别：有关整个文件中每一列的统计信息（存储在页脚）<br>stripe <span class="hljs-keyword">level</span>条带级别：每个条带的每列值统计信息（存储在页脚）<br><span class="hljs-keyword">row</span> <span class="hljs-keyword">level</span> 行级别：每<span class="hljs-number">10000</span>h行每列中的统计信息（存储在行组）<br>布隆过滤器：针对特定的列使用布隆过滤器有效的进行谓词下推 orc.bloom.<span class="hljs-keyword">filter</span>.<span class="hljs-keyword">columns</span>，他跟<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">index</span>交错分布的，首先根据索引缩小数据范围<br>再使用布隆过滤器进一步过滤数据。<br></code></pre></td></tr></table></figure><p>##acid支持</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">目的不是支持OLTP操作，而是支持百万计事务的更新，不是百万次事务。同时有了acid还可以保证流式数据插入到数据表中。<br>启用acid的表会在对应的目录下面创建增量目录。delta_xxx/bucket_xxx,<br>当delta变多了之后，会自动启用一些小的压缩，将一组事务h合并位一个单独的delta。<br>当增量文件变打了之后，会启动任务去重写基础+增量文件<br>针对orc文件通过id、bucket和row id来保证数据的唯一，当数据update、<span class="hljs-keyword">delete</span>、insert就是用到这三个值，对应的序列化<span class="hljs-number">0</span> 插入 <span class="hljs-number">1</span>更新 <span class="hljs-number">2</span>删除<br>https:<span class="hljs-regexp">//</span>orc.apache.org<span class="hljs-regexp">/docs/</span>acid.html<br></code></pre></td></tr></table></figure><p>##针对hive的操作</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> istari <span class="hljs-keyword">SET</span> FILEFORMAT ORC：将表新分区存储位orc<br><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> istari [<span class="hljs-keyword">PARTITION</span> partition_spec] CONCATENATE：针对分区进行合并，在stripe级别上面进行合并<br>hive <span class="hljs-comment">--orcfiledump &lt;path_to_file&gt;：orc文件信息</span><br>hive <span class="hljs-comment">--orcfiledump -d &lt;path_to_file&gt;：显示数据</span><br><br>一些参数：<br>orc.compress=ZLIB/<span class="hljs-keyword">NONE</span>/SNAPPY<br>orc.compress.size=<span class="hljs-number">262144</span>：针对每个chunk size的压缩大小<br>oorc.stripe.size=<span class="hljs-number">67108864</span>：每个stripe的大小<br>orc.<span class="hljs-keyword">row</span>.<span class="hljs-keyword">index</span>.stride=<span class="hljs-number">10000</span>：索引的条目数<br>orc.<span class="hljs-keyword">create</span>.<span class="hljs-keyword">index</span>=<span class="hljs-keyword">true</span>：是否创建索引<br>orc.bloom.<span class="hljs-keyword">filter</span>.<span class="hljs-keyword">columns</span>=&quot;&quot;：需要开启布隆过滤器的列<br>orc.bloom.<span class="hljs-keyword">filter</span>.fpp=<span class="hljs-number">0.05</span>：布隆过滤器的准确率<br><br>TBLPROPERTIES (&quot;orc.compress&quot;=&quot;NONE&quot;);<br><br>https://orc.apache.org/docs/hive-ddl.html<br>https://orc.apache.org/docs/hive-config.html<br></code></pre></td></tr></table></figure><p>##特点</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">1</span>、ORC是列式存储，有多种文件压缩方式，压缩比例也很高。   <br><span class="hljs-number">2</span>、文件可切分。可切分的意义在于，可以控制task的数量以及减少数据的输入。   <br><span class="hljs-number">3</span>、提供了多种索引：<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">index</span>、bloom <span class="hljs-keyword">filter</span> <span class="hljs-keyword">index</span>。   <br><span class="hljs-number">4</span>、支持复杂的数据结构   <br></code></pre></td></tr></table></figure><p>##为什么列式存储可以加快olap的查询速度</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs arcade">每一列的所有元素都是顺序存储的，这样可以带来的好处就是：   <br><span class="hljs-number">1</span>、查询的时候不需要扫描全部数据，只需要扫描对应的列数据就可以，还有就是orc保存着每一列的统计信息包括<span class="hljs-built_in">min</span> <span class="hljs-built_in">max</span> <span class="hljs-built_in">sum</span>等，这样配合hive的谓词下推优化器<br>可以实现谓词下推到数据结构的底层，完美减少了数据的输入，没有必须的数据再数据读取的时候完全避免了。   <br><span class="hljs-number">2</span>、每一列的成员都是同构的，可以针对不同的数据类型使用更高效的压缩算法，进一步减少I/O。   <br><span class="hljs-number">3</span>、每一类的成员都是同构性，更加实用CPU pipeline的编码方式，减少CPU缓存失效<br></code></pre></td></tr></table></figure><p>##数据模型</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs arcade">ORC的结构中包含了负责类型和原始类型，前者包括list struck <span class="hljs-built_in">map</span> <span class="hljs-built_in">union</span> 后者有<span class="hljs-built_in">boolean</span>、整数、浮点等。struct的孩子可能包括多个孩子节点，<span class="hljs-built_in">map</span>有俩个<br>孩子节点，list有一个孩子节点。每个<span class="hljs-built_in">schema</span>根节点是哟个struck类型， 所有的column按照树的中序遍历顺序编号。<br>&gt; ORC只需要存储<span class="hljs-built_in">schema</span>树中叶子节点的值，而中间的非叶子节点只是做了一层代理，只需要负责孩子节点值的数据读取，只有真正的的叶子节点才会读取数据，然后<br>&gt; 由树节点封装成对应的数据结构返回<br></code></pre></td></tr></table></figure><p>##orc相对rcfile优势</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>、每个task单独输出单一文件，减少了namenode的负载<br><span class="hljs-number">2</span>、支持复杂数据类型<br><span class="hljs-number">3</span>、文件级别的轻量化索引：可以跳过对应row group。定位行数据<br><span class="hljs-number">4</span>、根据数据类型来进行基于<span class="hljs-keyword">block的数据压缩，多种压缩编码 </span>RLE <span class="hljs-keyword">DIRECT</span><br><span class="hljs-keyword"></span><span class="hljs-number">5</span>、使用独立的recordReader并行读取同一个文件<br><span class="hljs-number">6</span>、不必扫描标识就可以对文件进行切分<br><span class="hljs-number">7</span>、限定读写需要的内存<br><span class="hljs-number">8</span>、元数据以protocol <span class="hljs-keyword">buffer存储，允许增加删除域</span><br><span class="hljs-keyword"></span><span class="hljs-number">9</span>、压缩方面支持更多的压缩方式 zlib snappy<br></code></pre></td></tr></table></figure><p>##文件结构</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs clean">尾部开始读取，首先是magic <span class="hljs-keyword">code</span><br>postscript：file footer的长度、文件的压缩方式、压缩大小、元数据的长度、文件的版本号<br>file footer：stripe列表，每个stripe的行数，列的数据类型，列的聚合信息count min max sum<br>    stripe information：offset、索引的长度、数据的长度、footer的长度、以及strope的行数<br>    type information：schema信息是表大成了一颗树结构，通过先序遍历序列化protocol buffer到文件里面<br>    column statistics：行数、最大值、最小值、求和、是否有null，可能对不同的数据类型定义了不同的统计信息<br>    user metadata：这个是用户自己可以设置一些kv存储，方便读取的时候使用。<br>    file metadata：整个列的（可能多个stripe）统计信息，来再数据分片的时候就进行数据谓词下推过滤数据<br>scripe：<br>    index data<br>        列索引：最大值、最小值、以及他们的offset。<br>            行索引 索引：提供了offset，能够定位具体的数据块以及字节位置（可以直接跳过不需要的行数）<br>    row data：<br>        列数据：<br>            列的元数据：<br>            实际数据<br>    scripe footer：每个stream的信息以及编码<br>        Stream：种类、对应的id、大小、字典、行索引<br>        columnEncoding：字典编码、行程编码<br>        <br><br><br><br>和parquet类型，orc也是使用二进制方式存储的，所以不可以直接读取，orc也是自带解析的，包含很多元数据，这些元数据都是同构protoBuffer进行序列化的，<br>###ORC文件<br>保存在文件系统上的普通二进制文件，一个ORC文件中可以由多个stripe，每一个stripe包含多条记录，按照类进行独立存储，对应parquet的row group概念<br>###文件级元数据<br>包括文件描述符信息PostScript、文件meta信息、所有stript的信息和文件schema信息<br>###stipe<br>一组行形成一个stripe，每次读取的时候以行组为单位，一般HDFS的块大小，保存了每一列的索引和数据<br>###stripe元数据<br>保存stripe的位置，每一列在该stripe的统计信息以及所有stream类型和位置<br>###row group<br>索引的最小单位，一个stripe包含多个row group，默认<span class="hljs-number">10000</span>个值组成。<br>###stream<br>一个stream表示文件中一段有效的数据，包括索引和数据俩类，索引stream保存了每个row group的位置和统计信息，数据stream包括多种类型的数据，具体哪几<br>种是由该列类型和编码方式决定。<br>###统计信息<br>ORC文件中保存了三个层级的统计信息，分别为文件级别、stript级别和row group 级别的，他们都可以作为谓词下推的条件，判断是否可以跳过某些数据，在统计<br>信息中都包含数据和是否由null值，并且对于不同的类型数据设置一些特定的统计信息<br>####file level<br>在ORC文件的末尾会记录文件级别的统计信息，会记录整个文件中columns的统计信息，这些统计信息主要是堆查询的优化，也可以作为一些简单的聚合查询的输出结果<br>####stripe level<br>ORC文件会保存每个字段stripe级别的统计信息，ORC reader使用这些统计信息来确定对于查询语句来说，需要读取哪些stripe记录，比如某个stripe的字段统计<br>max(A) = <span class="hljs-number">10</span> min(A) = <span class="hljs-number">4</span>,那么针对于<span class="hljs-keyword">where</span>条件 A&gt;<span class="hljs-number">10</span> 和 A&lt;<span class="hljs-number">4</span>的话，这个stripe都不会被读取<br>####row level<br>为了进一步避免读取不必要的数据，在逻辑上将一个column的index以一个特定给定的值（<span class="hljs-number">10000</span>）分割多个index组。以<span class="hljs-number">10000</span>条记录为一个组，对数据进行统计。<br>hive的查询引擎会将<span class="hljs-keyword">where</span>条件的约束传递给ORC reader，这些reader根据组级别的统计信息，更细粒度过滤掉不需要的数据，<br></code></pre></td></tr></table></figure><p>##数据读取</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"> 读取ORC文件是从尾部开始读取的，第一次读取<span class="hljs-number">16</span>KB的大小，尽可能的把postsript和footer数据读取到内存，最后一个字节保存着postscripe的长度，他的长度<br>不会查过<span class="hljs-number">256</span>字节，postscript保存着整个文件的元数据信息，包括文件的压缩格式、文件内部的每一个压缩块的最大长度（每次分配内存的大小）、footer长度，<br>以及一些版本信息。在postscript和footer之间存储着整个文件的统计信息，这部分统计包括每一个stripe中每一列的的信息，主要统计成员数，最大值、最小值、<br>是否有空值等。   <br>  接下来读取footer信息，它包含了没有给stripe的长度和偏移量，该文件的schema信息、整个文件的统计信息以及每一个row <span class="hljs-keyword">group</span>的行数。   <br>  处理stripe时首先从footer读取每个stripe的起始位置和长度、每一个stripe的footer数据（元数据，记录了<span class="hljs-keyword">index</span>和data的长度），整个stripe被分为<br><span class="hljs-keyword">index</span>和data俩部分。stripe内部是按照row <span class="hljs-keyword">group</span>分块的（每一个row <span class="hljs-keyword">group</span>中有多少行在footer中存储），row <span class="hljs-keyword">group</span>按照列存储。每个row <span class="hljs-keyword">group</span>由多个<br>stream保存数据和索引信息。每一个stream的数据会根据该列的数据类型使用特定的压缩算法保存。   <br>  初始化全部元数据的之后，可以根据includes数组指定需要读取的列编号，可以根据SearchArgument参数指定过滤条件。这中间就通过统计信息过滤掉不需要的<br>数据，这时候也会产生比较多的零散数据，ORC会尽可能的合并小文件，减少IO次数。   <br>  ORC不支持读取特定字段类型中指定的部分。   <br>  使用ORC存储的时候，尽量使用HDFS每一个<span class="hljs-keyword">block</span>保存一个stripe，对于一个orc文件来说，stripe的大小一般设置的比<span class="hljs-keyword">block</span>小，否则的话一个stripe就会分<br>部到不同的<span class="hljs-keyword">block</span>上面去，读取的时候就需要远程读取数据。如果设置的stripe只保存在一个<span class="hljs-keyword">block</span>上面的话，如果当前的<span class="hljs-keyword">block</span>剩余空间不足可以存储一个stripe<br>,ORC的<span class="hljs-keyword">write</span>就会将数据打散保存在在<span class="hljs-keyword">block</span>剩余的空间中，直到这个<span class="hljs-keyword">block</span>存储满，这样的话下雨给stripe就可以从下一个<span class="hljs-keyword">block</span>上面开始存储。    <br>  由于ORC使用了更加精确的索引信息，使得读取数据可以从任意一行开始读取，更细粒度的统计信息可以使ORC文件跳过整个row <span class="hljs-keyword">group</span>，orc默认会对任意数据使用<br>zlib压缩，因此orc的存储空间更小。<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>orc和parquet</title>
    <link href="/2022/02/09/orc-vs-parquet-md/"/>
    <url>/2022/02/09/orc-vs-parquet-md/</url>
    
    <content type="html"><![CDATA[<h1 id="orc-vs-parquet"><a href="#orc-vs-parquet" class="headerlink" title="orc vs parquet"></a>orc vs parquet</h1><p>orc和parquet是我们在hive中常用点列式存储格式，各自有各自点特点，下面介绍一下烈士存储点优势以及它们之间点差异。</p><h2 id="列式存储的优势"><a href="#列式存储的优势" class="headerlink" title="列式存储的优势"></a>列式存储的优势</h2><p>列式存储把每列数据存在一起，同类型的列放在一起，通过LRU、字典、bit-packing等编码可以很大程度上减少数据的存储，对同类型数据的压缩效果也比混合类型的压缩好很多。每列数据在一起，我们在查询的是同可以通过映射下推有效的去除不需要的列，其实也是我们在hive、spark等olap等引擎中最常用的优化手段，通过只读取我们需要的列，可以很大程度上面减少io提高我们分析的性能。其次就是通过列式存储，我们可以对列、块、页等数据结构添加上我们需要的索引，通过这些索引和offset我们就可以很好的在数据文件上面使用谓词下推，进一步过滤掉我们不需要的数据，除了索引之外orc和parquet都使用了布隆过滤器，通过添加对字段的布隆过滤器来过滤掉无用的数据，从而使我们的计算更加高效。</p><h2 id="orc-vs-parquet-1"><a href="#orc-vs-parquet-1" class="headerlink" title="orc vs parquet"></a>orc vs parquet</h2><p>1、支持类型上，parquet通过sriping&#x2F;assembly算法完美支持嵌套结构。orc支持复杂的数据类型，对嵌套类型支持较差。<br>2、索引上，parquet和orc都有统计信息以及offset索引，并且都支持列的布隆过滤器。<br>3、文件结构上，parquet是row group、column chunk、pag。orc是stipe、row group、stream。总体结构上面比较类似<br>4、压缩上面，parquet支持snappy、gzip、lzo、brotli、lz4、zstd、lz4_raw；orc支持snappy、zlib、none三种<br>5、扩展性上，parquet支持的组件扩展性更好，对于spark也是默认的存储。而orc是rcfile的升级版本，对hive的支持性优化性更好。<br>6、parquet是cloudera开发的；orc是hortonworks开发的。现在俩家公司合并来了。<br>7、编码上：parquet支持更多的编码格式，<br>8、元数据：parquet使用thrift的TCompactProtocol进行元数据序列化，orc使用protocol buffer进行元数据的序列化</p><p>parquet相对于orc的优势：parquet通过sriping&#x2F;assembly算法完美支持嵌套类型，像json、thrift、protocolbuffer等通过defintion level和repeated leve方便对其进行编码以及压缩，parquet里面使用到的编码方式也更多：int、byte上可以使用增量编码来对数据进行编码。（如果需要支持复杂类型的旋用parquet更加有优势）。同时parqeut使cloudera和twitter创建的，拥有更广的适配性，像spark默认也是parquet存储的，适配性更好。</p><p>orc想对于parquet的优势：想对于parquet，orc支持acid以及update操作，如果在hive上面使用acid，像update merge delete这样的操作可以完美的支持，hive3对事务的支持更加完善了。因为orc就是为hadoop而生的，对hive的适配性更好，在hive上面使用拥有更好的压缩比例以及更好的查询性能。orc的拥有更细粒度的索引信息，能够更好的提高查询性能。</p>]]></content>
    
    
    
    <tags>
      
      <tag>列式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
