<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>spark背压</title>
    <link href="/2024/01/08/spark%E8%83%8C%E5%8E%8B/"/>
    <url>/2024/01/08/spark%E8%83%8C%E5%8E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="spark的back-pressure机制"><a href="#spark的back-pressure机制" class="headerlink" title="spark的back pressure机制"></a>spark的back pressure机制</h1><p>背压机制主要是保证运行的稳定性，防止突发流量造成的整个系统的扰动，更验证的扰动可能会造成系统崩溃。在保证系统正常运行的情况下，还要保证吞吐量最大化，<br>就需要设置合理的流量门槛。</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>spark的流量控制三种：</p><ol><li>限制receiver接收数据的速率:spark.streaming.receiver.maxRate</li><li>限制direct模式中，读取每个分区的速率大小：spark.streaming.kafka.maxRatePerPartition</li><li>动态流量控制方案，利用了PID控制器：spark.streaming.backpressure.enabled&#x3D;true</li></ol><p>基于receive和direct模式的，字节限制读取数据速率，这种设置比较简单方便，但是参数具体值不好把我，同时也不能动态调整，如果数据接收源又变化，可能需要<br>重启程序进行调节。而back pressure就是动态的流量控制方案，在spark1.5版本的适合加入的，能够根据当前系统的处理速度智能地调节流量阈值，</p><h2 id="backpressure"><a href="#backpressure" class="headerlink" title="backpressure"></a>backpressure</h2><p>源码：抽象类RateController是动态流量的核心，继承了StreamingListener的特征。<br>具体流程：</p><ul><li>继承了StreamingListener，监听StreamingListenerBatchCompleted事件（一个batch处理完毕）</li><li>从处理完成的事件中拿到三个值：处理完成时间戳（precessingEndTime）、实际处理时长（processingDelay）、调度延迟(schedulingDelay)</li><li>从StreamInputInfo实例获取批次输入数据条数：numRecords</li><li>RateEstimator利用上面几个值计算新的阈值，然后通过rpc发布给ReceiverTracker</li></ul><p>##PIDRateEstimator<br>RateEstimator的唯一实现，spark.streaming.backpressure.rateEstimator通过这个参数来配置。</p><p>PID控制器就是比例、积分、微分控制器，通过反馈回路把收集到的数据和一个设定值进行比较，然后计算他们的差值计算新的输入值，最终的目的就是尽量接近或达到<br>设定值。</p><p>计算公式:</p><blockquote><p>三部分：p控制现在、i是纠正曾经、d是管控未来<br>p: 误差就是 t 时刻跟设置值之间的误差<br>i: 误差累积 0+1+…+(t-1)+t，把每个事件的误差相加起来<br>d: 误差差值 就是 t - (t-1) 。就是把当前的误差减去上次的误差就是该值</p></blockquote><p>回头看下spark的实现，三个系数分别是：</p><figure class="highlight abnf"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">kp</span> <span class="hljs-operator">=</span> spark.streaming.backpressure.pid.proportional<span class="hljs-operator">=</span><span class="hljs-number">1.0</span><br><span class="hljs-attribute">ki</span> <span class="hljs-operator">=</span> spark.streaming.backpressure.pid.integral<span class="hljs-operator">=</span><span class="hljs-number">0.2</span><br><span class="hljs-attribute">kd</span> <span class="hljs-operator">=</span> spark.streaming.backpressure.pid.derived<span class="hljs-operator">=</span><span class="hljs-number">0.0</span><br></code></pre></td></tr></table></figure><p>spark种计算逻辑</p><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pony">计算最新的速率<br><span class="hljs-meta">val</span> processingRate = numElements.toDouble / processingDelay * <span class="hljs-number">1000</span><br>计算<span class="hljs-keyword">error</span>，是使用上一次计算出来的速率当成我们的期望值的，这样才能达到一个动态的流量控制的<br><span class="hljs-meta">val</span> <span class="hljs-keyword">error</span> = latestRate - processingRate<br><span class="hljs-comment">//误差累计使用  调度的延迟事件*速度，其实就是针对正常处理溢出的数据，然后除以调度的整体事件就是这些多/少处理的数据的一个速率</span><br><span class="hljs-meta">val</span> historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis<br><span class="hljs-comment">//查误差值就是记录上一次的误差然后用这次的误差-上一次的误差</span><br><span class="hljs-meta">val</span> dError = (<span class="hljs-keyword">error</span> - latestError) / delaySinceUpdate<br><span class="hljs-comment">//带入计算公式,用设定的阈值（其实就是上次的阈值，然后减去pid计算出来的误差值，然后和我们设定的最小值取最大的那个）</span><br><span class="hljs-meta">val</span> newRate = (latestRate - proportional * <span class="hljs-keyword">error</span> - integral * historicalError - derivative * dError).max(minRate) <br></code></pre></td></tr></table></figure><p>spark有俩个参数：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">spark<span class="hljs-selector-class">.streaming</span><span class="hljs-selector-class">.backpressure</span>.initialRate这是是初始化的处理速度<br>spark<span class="hljs-selector-class">.streaming</span><span class="hljs-selector-class">.backpressure</span><span class="hljs-selector-class">.pid</span>.minRate这个是最小的处理速度<br></code></pre></td></tr></table></figure><p>最终的话，计算除了最新的处理速度，通过rpc发送给receiver，然后更新设定的阈值。通过令牌桶完成速度的限制。</p><h2 id="令牌桶算法"><a href="#令牌桶算法" class="headerlink" title="令牌桶算法"></a>令牌桶算法</h2><p>有三部分构成：令牌流、令牌桶、数据流</p><ul><li>令牌流：如果我们设定好了一个速度，那么每隔一段事件就会生成一个令牌</li><li>令牌桶：如果令牌没有消耗，那么令牌就会保留到下来，突发情况来了，可以消耗令牌桶的令牌，有数值</li><li>数据流：来一条数据可能就需要消耗一个令牌，如果没有了令牌就会停止接收数据</li></ul><h3 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h3><ol><li>系统接收到一个单位数据（可以是请求、可以是一条数据、一个包、一个字节等）</li><li>系统拿到数据就会从令牌桶中取出一个令牌</li><li>如果令牌没有了就会将数据不做处理（可以是丢弃，可以是等待）</li></ol><p>列举几种情况</p><ul><li>数据流的速度等于令牌生成的速率，就是正常处理数据</li><li>数据流的速度小于令牌生成的速率，令牌桶里面的令牌会越来越多，直到存满，这里面的令牌可以应对一些突发情况</li><li>数据流的速度大于令牌生成的速度，那么就算桶里面有令牌也会很快耗尽导致服务停止运行。</li></ul><p>另外跟令牌桶算法类似的还有漏桶算法，这个是强制限制处理数据的速率，不像令牌桶可以应对一段时间的突发情况</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>平台优化</title>
    <link href="/2023/08/08/%E5%B9%B3%E5%8F%B0%E4%BC%98%E5%8C%96/"/>
    <url>/2023/08/08/%E5%B9%B3%E5%8F%B0%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="大数据平台优化"><a href="#大数据平台优化" class="headerlink" title="大数据平台优化"></a>大数据平台优化</h1><h2 id="存储优化"><a href="#存储优化" class="headerlink" title="存储优化"></a>存储优化</h2><ul><li>存储格式</li></ul><ol><li><p>text-&gt;parquet：平台文件最开始就是文本存储，因为列式存储的优势很多：相同类型的列放在一起可以使用更适合的编码格式以及压缩算法，有效的减少数据量；在查询上，因为现在很多的查询优化可以直接作用在物理执行计划上，像谓词下推，通过查询优化直接作用到存储上，从源头就减少了数据读取，大大提高了整体的性能；列存储上面还可以对数据进行一些统计和生成索引对sql对查询优化提高明显。</p></li><li><p>常用对列式存储主要式orc和parquet，parquet有更好的嵌套类型实现，支持更多的编码格式，更广泛的适配。orc相对于hive来说更加的紧密，可以提供更好的存储和查询性能，还可以支持hive的acid。orc相对parquet来说对hive对适配性更好，能提供更好对压缩和查询性能，只支持简单对嵌套类型，支持hive对acid。</p></li></ol><ul><li>小文件</li></ul><p>小文件带来的危害：</p><ul><li>大量的小文件元数据是存在namenode内存种的，占用太多的内存导致namenode的性能跟不上，导致系统变慢</li><li>可能对文件的读取写入性能影响，大部分时间都耗在了连接和启动释放他ask上面</li></ul><p>1.分析小文件<br>通过dump出namenode的fsimage文件，然后通过hdfs自带的命令解析，最终放到hive表种，分别对每个租户的目录和hive的warehouse下面进行小文件分析。针对hdfs非hive数据分析出小于10M的文件，单个目录下有2个以上小于10M的文件。针对hive的数据主要是对hive表分区为统计粒度，然后分析出每个分区里面的小文件数量。<br>2.设置小文件输入<br>基本上都是hive表存储，所以合理的设置map输入小文件和并和输出小文件合并，通过读取的时候设置小文件合并可以减少maptask的启动，这样每个map处理的数据量也变多了，这样如果在没有reduce的job种，可以把结果小文件减少。<br>3.设置输出小文件合并<br>在hive执行任务的时候，如果判断最终生成的小文件平均大小小于16M的时候回重新启动一个job去合并小文件，牺牲了一些性能，但是对平台的长远考虑是有好处的<br>4.设置历史外表数据归档<br>我们一般有专门的外表目录存放一些原系统或者其他系统传过来的文本文件，这样的文件可能是我们使用用来加载内表的文件，一般也回保存一段时间，如果比较重要的可能全部保留，所以对一个月以前的数据进行单独的合并压缩操作，使用zstd，减少空间和元数据的占用<br>5.设置内表数据归档<br>我们会把把半年前的数据进行hvie归档操作，这样的话，可以很有效的减少了小文件的数量，同时对一些历史数据很多，但是每天数据量不大的表，单独进行合并分区的操作，这样合并了多个分区，也把小文件合并了。</p><ul><li>压缩，纠错码</li></ul><ol><li><p>数据文件压缩使用zstd压缩，比较新对一种压缩算法，在压缩率和性能上面都比较突出。我们自己测试的来看基本上都是第一。压缩时间（越小越好）：lz4, zstd &lt; lzo &lt; snappy &lt;&lt; gzip-1 &lt; lz4-9 &lt; gzip &lt; gzip-9 &lt; lzo-9 ；压缩率（越大越好）：zstd-10 &gt; zstd &gt;&gt; lz4-9 &gt; gzip-9 &gt; gzip, lzo-9 &gt;&gt; lz4, gzip-1 &gt; snappy, lzo。对于外表数据节省了80%以上的空间。</p></li><li><p>升级cdp之后，针对使用频率比较低的目录，使用纠删码来减少数据的容易，减少存储成本。</p></li></ol><h2 id="计算优化"><a href="#计算优化" class="headerlink" title="计算优化"></a>计算优化</h2><ul><li>资源优化</li></ul><ol><li>调整内存和cpu使用的比例，防止出现cpu用完了，内存还剩不少；内存用完了cpu还剩不少。</li><li>根据文件大小、不同场景适配不同的模版参数。</li><li>设置不同的队列，单独给小任务配置小的任务队列。</li></ol><ul><li>并发优化</li></ul><ol><li>全资产根据血缘关系以及项目重要系数计算优先级，按照优先级加载</li></ol><h2 id="逻辑优化"><a href="#逻辑优化" class="headerlink" title="逻辑优化"></a>逻辑优化</h2><ul><li>空文件加载</li></ul><ol><li>针对文件不在执行yarn任务。直接操作元数据进行当天任务加载。</li></ol><ul><li>清理优化</li></ul><ol><li>不同场景加载执行清理，减少平台整体存储的数据量。</li></ol><ul><li>sql优化</li></ul><ol><li>开启mapjoin</li><li>针对一些当前表使用spark加载，使用单独的队列，实现快速加载</li><li>小于10M的表全部使用本地模式进行加载</li><li>根据历史任务手动设置reduce的数量，切换到tez之后开启动态reduce个数调整，进行设置合理的reduce减少资源的浪费以及小文件的生成。</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数仓QA</title>
    <link href="/2023/06/14/%E6%95%B0%E4%BB%93QA/"/>
    <url>/2023/06/14/%E6%95%B0%E4%BB%93QA/</url>
    
    <content type="html"><![CDATA[<h1 id="数仓问题总结"><a href="#数仓问题总结" class="headerlink" title="数仓问题总结"></a>数仓问题总结</h1><h2 id="从零建设一个数仓"><a href="#从零建设一个数仓" class="headerlink" title="从零建设一个数仓"></a>从零建设一个数仓</h2><ol><li>建设一个数仓首先是要有一个目标，支持哪些业务类型的分析，支持哪些决策，数据的输出方面有了大方向；</li><li>有了目标，就需要对公司的业务进行调研，有哪些业务，有哪些需求，公司目前有哪些数据；</li><li>针对数据的输入，业务调研，数据的应用有了了解，应该就开始进行了技术选型了，首先对于原始数据的采集以及分析组件定下来；</li><li>数据分析人员提炼数据模型，划分数据主题，进行数据的整体规划；</li><li>有了数据模型，还需要定制我们数仓的整体规范像命名规范、开发规范、流程规范，因为本上仓库仓库的只有有合理规范存储起来的数据才能算的上是仓库，只是把数据放到一个地方那可能叫垃圾站。数据规范的同时也需要对整体数仓进行分层。</li></ol><h2 id="数据仓库中最重要的是什么"><a href="#数据仓库中最重要的是什么" class="headerlink" title="数据仓库中最重要的是什么"></a>数据仓库中最重要的是什么</h2><p>其实都挺重要的，如果说最重要的话，我认位是数据质量和数据标准。数据质量是保证数据的完整性、准确性、及时性、一致性的，我们做数仓的目标是什么是为了支撑公司重大决策，如果数据的质量都不能保证那么最终的结果也就不能保证。数据标准这个是我自己做了这几年自己任务比较重要的一个，数据标准其实就是数据的规范，从数据的采集，数据的处理，数据的输出，以及命名规范、开发规范、流程规范等，如果真正想让数据发挥它自己的价值，那么他的数据接入以及输出场景肯定是比较多的，而且对接的部门业务也是特别多的，如果没有吧数仓的标准固定好，那么不同的人不同的业务以及不同的需求可能都有自己个性化的东西，开发效率肯定是跟不上去，而且成本肯定是特别高的，有了好的数据标准，可以解决很多问题，不知道你有没有接触过银行的传统数仓，他们银行原来基于TD做的那一套数仓，数据标准都是特别统计的，包括命名以及码值等，其他银行的标准都是一样的，包括银行的7大主题，相当于行业内都有固定的统一标准，不管是跟业务部门还是其他银行对接业务，沟通和效率上面都是特别高效。</p><h2 id="为什么要分层"><a href="#为什么要分层" class="headerlink" title="为什么要分层"></a>为什么要分层</h2><ol><li>通过数据的分层简化数据的清理流程，让数据更加清楚，逻辑更加清楚，开发效率可以得到一定的提升，如果数据出现问题，对于问题的定位也就更加的简单。</li><li>通过数据的分层增加了数据的冗余，数据冗余的同时其实代表每层数据的可用性，减少了数据重复处理的次数，这样就是典型的使用空间换时间，因为大数据想对于空间更加廉价，减少了重复处理数据带来的内存以及cpu消耗。</li><li>其实数据分层本是一种高效的数据组织形式，通过数据的分层我们可以更好的发挥数据的集成价值，同时发汇历史数据的价值。</li></ol><h1 id="数仓知识"><a href="#数仓知识" class="headerlink" title="数仓知识"></a>数仓知识</h1><h2 id="传统数据库和数据仓库"><a href="#传统数据库和数据仓库" class="headerlink" title="传统数据库和数据仓库"></a>传统数据库和数据仓库</h2><ul><li>应用方面<br>传统数据库主要是存储实时数据，面向事务的。数据仓库主要是存储历史数据，进行数据分析挖掘，是面向分析的。</li><li>数据方面<br>传统数据库存储的数据量较少、数据冗余少，数据范围较小数据仓库存储的数据量很大，数据冗余度搞，数据历史数据很全，同时数据范围大，可能不光包含数据库中的数据还包括日志、埋点等数据。</li><li>建模方面<br>传统数据库一般是满足第三范式要求，通过规范化的主外键设计减少数据的冗余，同时保证数据的完整性。数据仓库一般采用维度建模，主要的还是提升查询性能，不符合三范式的要求。</li></ul><h2 id="数仓如何分层"><a href="#数仓如何分层" class="headerlink" title="数仓如何分层"></a>数仓如何分层</h2><p>数据接入层-&gt;公共数据层-&gt;数据应用层</p><ol><li>数据接入层operation data source</li></ol><ul><li>数据接入层是,主要存放的是贴源数据，跟源系统数据保持一直，数据分为离线数据和实时数据，没有经过复杂的逻辑处理。</li></ul><ol start="2"><li>公共数据层common data model</li></ol><ul><li>dim公共维度层:目的就是抽取公共模型层的公用维度，建立整个数仓的一致性维度，降低数据计算口径和算法的不统一风险。</li><li>dwd明细粒度事实层：以业务过程作为建模驱动法，构建最细粒度的明细表，再设计的时候通常还会考虑冗余一些常用的维度来也就是做大宽表。</li><li>dws公共汇总粒度事实层：以分析的主题对象作为建模驱动，构建公共粒度的汇总指标事实表，以宽表化手段物理化模型。主要存放派生指标（原子指标-订单总数+时间-近一天+修饰符-线上）</li></ul><ol start="3"><li>数据应用层application data service</li></ol><ul><li>通常由ods 和 cdm加工而出，个性化统计指标数据</li></ul><h2 id="ods层表设计"><a href="#ods层表设计" class="headerlink" title="ods层表设计"></a>ods层表设计</h2><p>ods层的表其实不是数仓建模的范畴，因为存放都是贴源数据，但是合理的规划ods层并做好s数据同步是基础。</p><h2 id="dim层设计"><a href="#dim层设计" class="headerlink" title="dim层设计"></a>dim层设计</h2><p>维度层是通过星型模型来构建的，有一个中心维度表也就是主维表，通过主维表关联出其他相关维度表。</p><ol><li>结合业务过程分析定义维度</li><li>确认主维表（中心事实表），一般是由源系统直接同步到ods层的表。</li><li>确认相关维度表，确认好了主维表，通过业务流程的梳理与主维表存在关联关系，确定维度信息形成相关维度表。</li><li>确认维度属性，第一阶段生成主维表的维度属性，第二阶段生成相关维表的维度属性。</li></ol><h2 id="事实表"><a href="#事实表" class="headerlink" title="事实表"></a>事实表</h2><p>事实表遵循维度建模的设计过程：选择业务过程、确认粒度、确认维度、确认事实</p><ol><li>选择业务过程：维度建模，其实就是我们有什么需求，就根据需求去按照规则进行建模，选择业务过程就是明确我们的业务需求，确定我们的事实表的种类。首先我们应该分析业务的生命周期，整个生命周期出来之后我们需要确认我们业务的关键步骤，生命周期可能很长，这时候就需要从这些步骤里面抽取出我们需要的关键步骤。有了关键步骤，可能是一个可能是多个，根据业务的实际情况如果有多个业务过程，那么就应该使用累积快照事实表，如果是单个业务过程我们可能选择事务事实表，如果有固定时间间隔的可能需要周期快照事实表。</li><li>确认粒度：我们选择好了业务过程以及确认好了事实表的种类，需要去声明粒度，其实就是我们每行数据存放的内容，这时候这个粒度应该是最细级别的粒度，如果是粒度是订单，其中我们的的订单表可能有父订单，父订单下面有子订单，这时候我们应该以子订单为粒度，作为每一行数据进行设计表。其实也就是我们使用的主键。</li><li>确认维度：完成了粒度声明，其实知道每行数据的内容，这时候我们需要去描述业务过程中相关的维度信息。</li><li>确定事实：事实表是某一主题的事实记录表，通过与维表关联和自身的度量数据组成。</li><li>确认好事实表之后需要退化维度，针对一些最常用的维度和效率做宽表化处理。</li></ol><h2 id="公共汇总粒度事实层（DWS）"><a href="#公共汇总粒度事实层（DWS）" class="headerlink" title="公共汇总粒度事实层（DWS）"></a>公共汇总粒度事实层（DWS）</h2><p>以分析的主题作为建模驱动，基于上层应用和产品的指标需求构建公共粒度的汇总指标事实表，公共汇总的一个表对应一个派生指标。</p><ol><li>聚集是对原始明细粒度的数据进行汇总</li><li>汇总层是面向分析对象的主题聚集建模</li><li>聚集不跨越事实，聚集的维度和度量跟原始模型保持一直</li><li>聚集可以带来查询性能的提升</li><li>尽量把公共的汇总聚集沉淀到dws层</li><li>以业务过程进行分类</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>指标建设</title>
    <link href="/2023/05/01/%E6%8C%87%E6%A0%87%E5%BB%BA%E8%AE%BE/"/>
    <url>/2023/05/01/%E6%8C%87%E6%A0%87%E5%BB%BA%E8%AE%BE/</url>
    
    <content type="html"><![CDATA[<h1 id="指标建设模型"><a href="#指标建设模型" class="headerlink" title="指标建设模型"></a>指标建设模型</h1><h2 id="AARRR海盗模型"><a href="#AARRR海盗模型" class="headerlink" title="AARRR海盗模型"></a>AARRR海盗模型</h2><ul><li>用户拉新(Acquisition)</li><li>用户激活(Activation)</li><li>用户留存(Retention)</li><li>商业变现(Revenue)</li><li>用户推荐(Refer)</li></ul><h3 id="A-用户拉新"><a href="#A-用户拉新" class="headerlink" title="A 用户拉新"></a>A 用户拉新</h3><p>通过各种推广渠道,以各种方式获取目标用户，并对各种影响渠道的效果做评估，不断优化投入策略，降低获客成本。</p><blockquote><p>新增用户数、激活率、注册转化率、新客留存率、下载量、安装量等</p></blockquote><h3 id="A-用户活跃"><a href="#A-用户活跃" class="headerlink" title="A 用户活跃"></a>A 用户活跃</h3><p>活跃用户是指真正开始使用了产品的价值，我们需要掌握用户的行为数据，监控产品的健康程度。这个模块主要反馈用户进入产品的表现，是产品体验的核心所在，</p><blockquote><p>DAU&#x2F;MAU、日均使用时间、启动app时长、启动app的次数</p></blockquote><h3 id="R-用户留存"><a href="#R-用户留存" class="headerlink" title="R 用户留存"></a>R 用户留存</h3><p>衡量用户粘性和质量的指标</p><blockquote><p>留存率、流失率</p></blockquote><h3 id="R-变现"><a href="#R-变现" class="headerlink" title="R 变现"></a>R 变现</h3><p>主要用来衡量产品商业价值。</p><blockquote><p>生命周期价值、客单价、GMV</p></blockquote><h3 id="R-推荐"><a href="#R-推荐" class="headerlink" title="R 推荐"></a>R 推荐</h3><p>衡量用户自传播和口碑的情况。</p><blockquote><p>邀请率、裂变系数</p></blockquote><h2 id="OSM-Obejective-Strategy-Measurement-模型"><a href="#OSM-Obejective-Strategy-Measurement-模型" class="headerlink" title="OSM(Obejective,Strategy,Measurement)模型"></a>OSM(Obejective,Strategy,Measurement)模型</h2><p>OSM模型是指标体系建设过程中辅助确定核心的重要方法，包含业务目标、业务策略、业务度量。</p><h3 id="Obejective"><a href="#Obejective" class="headerlink" title="Obejective"></a>Obejective</h3><p>目标：思考的就是产品的目标、满足用户什么需求、主要是从客户角度去确定目标。以某一个周边游新增模块举例：目标就是让用户可以利用周末时间方便的短期旅游，能方便的找到自己想游玩的地方并且节省了攻略时间。</p><h2 id="S-Strategy"><a href="#S-Strategy" class="headerlink" title="S Strategy"></a>S Strategy</h2><p>策略：使用什么样的策略可以让用户觉得自己的需求得到了满足。</p><ul><li>个性化推荐： 通过用户点击浏览行为，推荐用户比较热门的景点。</li><li>时间方便：提供不同的方案，一天游玩，俩天游玩，规划好不同方案的出行攻略，这样可以不用用户自己做攻略，把浏览路线花费的大概时间都详细说明。</li><li>坐车方面：来回每天都有来回的专车，早上直接从市里触发，下午返回。中途去其他景点也不需要自己找车，固定路线都是车接车送。</li><li>安全方面：都是由经验丰富的老司机和导游，每个人都会出行保险。</li><li>住宿方便：如果需要过夜的提供不同的住宿规格，豪华型、舒适性都有，不用自己再去搜索附近酒店，价格也比较透明。</li></ul><h2 id="M-measurement"><a href="#M-measurement" class="headerlink" title="M measurement"></a>M measurement</h2><p>度量：有了我们指定的策略，根据策略就可以去指定指标，指标分为过程指标和结果指标。</p><h3 id="结果指标"><a href="#结果指标" class="headerlink" title="结果指标"></a>结果指标</h3><p>衡量用户发生某个动作之后产生的结果，最终就是查看用户的需求是否得到了满足。<br>针对上面的问题思考：下单率、取消率、景点评分情况、服务评分。</p><h4 id="过程指标"><a href="#过程指标" class="headerlink" title="过程指标"></a>过程指标</h4><p>主要用户动作之中产生的指标，其中维度是整体体系不能缺少的东西，有了维度才能去思考有哪些指标。<br>供需比、地点选择、司机服务分、代理机构评分</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>hdfs透明数据加密</title>
    <link href="/2023/04/08/hdfs%E9%80%8F%E6%98%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86/"/>
    <url>/2023/04/08/hdfs%E9%80%8F%E6%98%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86/</url>
    
    <content type="html"><![CDATA[<h1 id="HDFS透明数据加密"><a href="#HDFS透明数据加密" class="headerlink" title="HDFS透明数据加密"></a>HDFS透明数据加密</h1><h2 id="为什么要使用HDFS数据加密和透明数据加密的应用场景"><a href="#为什么要使用HDFS数据加密和透明数据加密的应用场景" class="headerlink" title="为什么要使用HDFS数据加密和透明数据加密的应用场景"></a>为什么要使用HDFS数据加密和透明数据加密的应用场景</h2><p>常见的加密层级</p><ul><li>应用层加密：应用程序加密是一种在应用程序层级进行的加密，可以为特定的应用程序提供定制化的加密保护。这种加密通常需要应用程序本身支持，例如使用加密算法对敏感数据进行加密，如密码和身份验证信息。</li><li>数据库层加密：数据库加密是一种在数据库层级进行的加密，可以保护数据库中的数据安全性。数据库加密通常包括列加密、行加密、表空间加密等技术，可以为不同的数据提供不同的安全级别。</li><li>文件系统层加密：文件系统加密是在文件系统层级进行的加密，通常是通过透明数据加密（TDE）或类似的技术实现的。文件系统加密可以保护数据在磁盘上的存储安全，以防止非授权访问和窃取数据。</li><li>磁盘层加密：磁盘加密技术通过对整个磁盘或分区进行加密，可以防止未经授权的访问者读取或修改存储在磁盘上的数据。</li><li>TLS&#x2F;SSL：传输层安全协议（TLS）或安全套接字层协议（SSL）是一种广泛用于保护数据传输安全的协议。TLS&#x2F;SSL提供了端到端的数据加密和身份验证功能，以防止数据在传输过程中被窃取或篡改。</li></ul><p>HDFS透明数据加密是在文件系统层级进行的加密，因此属于文件系统加密的范畴。它通过透明数据加密（TDE）技术，在不影响HDFS用户操作和应用程序的情况下，对存储在HDFS上的数据进行加密保护，以防止非授权访问和窃取数据。HDFS透明数据加密的加密层级与其他文件系统加密技术类似，例如，Linux文件系统的eCryptfs、Windows文件系统的BitLocker等。在文件系统层级使用加密技术可以保护存储在磁盘上的数据，在一定程度上提高数据的安全性和可靠性。</p><p>HDFS加密可以防止在文件系统或之下的攻击，也叫操作系统级别的攻击（OS-level attacks）。操作系统和磁盘只能与加密的数据进行交互，因为数据已经被HDFS加密了。</p><h2 id="什么是HDFS透明数据加密"><a href="#什么是HDFS透明数据加密" class="headerlink" title="什么是HDFS透明数据加密"></a>什么是HDFS透明数据加密</h2><p>首先理解一下透明的含义，透明是指加密和解密的过程对于应用程序和终端用户是透明的，不需要进行任何额外的操作。这是因为在透明数据加密中，加密和解密的过程是由系统自动完成的，应用程序只需要像平常一样读取或写入数据即可，数据在经过加密或解密后会被透明地传输给应用程序，使得应用程序无感知地使用加密数据。此外，加密和解密的密钥管理也是透明的，HDFS会自动管理密钥，不需要用户手动管理密钥。</p><p>HDFS透明加密（Transparent Encryption）支持端到端的透明加密，启用以后，对于一些需要加密的HDFS目录里的文件可以实现透明的加密和解密，而不需要修改用户的业务代码。端到端是指加密和解密只能通过客户端。对于加密区域里的文件，HDFS保存的即是加密后的文件，文件加密的秘钥也是加密的。让非法用户即使从操作系统层面拷走文件，也是密文，没法查看。</p><h2 id="HDFS透明数据加密的过程"><a href="#HDFS透明数据加密的过程" class="headerlink" title="HDFS透明数据加密的过程"></a>HDFS透明数据加密的过程</h2><ul><li>EZ K（encryption zone key）：加密区域的秘钥，每创建一个HDFS的加密区域也即是一个目录就会生成一个，一般保存在后端的秘钥存储库中，比如数据库。</li><li>DE K（data encryption key）：每个加密区域里的每个文件唯一的加密秘钥。</li><li>EDE K（encrypted data encryption key）：每个加密区域里的每个文件唯一的加密秘钥，然后被EZ Key加密后的秘钥。保存在NameNode的元数据中。<br><img src="/img_1.png" alt="img_1.png"></li><li>KMS （Key Management Server）：是代表HDFS和客户端与后端的秘钥存储进行交互的代理服务。</li></ul><p>下面是写入加密数据的流程:</p><ol><li>client向namenode发送请求创建加密文件</li><li>namenode收到请求后向KMS请求EDEK</li><li>KMS收到请求之后会根据文件所在目录的EZK生成一个EDEK</li><li>把生成好的EDEK发送给namenode</li><li>namenode收到EDEK会把EDEK存放到文件的元数据里面去</li><li>namenode把EDEK发送给客户端</li><li>客户端收到EDEK之后向KMS请求EDK</li><li>KMS根据EDEK和文件所在目录的EZK生成一个DEK发送给客户端</li><li>客户端根据EDK把文件加密发送到datanode上面</li></ol><blockquote><p>DEK是加解密一个文件的密匙，而KMS里存储的EZ key是用来加解密所有文件的密匙（DEK）的密匙。所以，EZ Key是更为重要的数据，只在KMS内部使用（DEK的加解密只在KMS内存进行），不会被传递到外面使用，而HDFS服务端只能接触到EDEK，所以HDFS服务端也不能解密加密区文件。目录下面还可以再次创建EZK,文件会找最近的EZK去生成EDEK.</p></blockquote><p>##HDFS透明数据加密支持的加密算法</p><ul><li>AES-256：一种高级加密标准，采用256位密钥进行加密和解密。</li><li>AES-128：与AES-256相似，但使用128位密钥。</li><li>RC4：一种流密码，使用可变长度的密钥进行加密和解密。</li><li>DESede：一种基于DES（Data Encryption Standard）的加密算法，采用三个56位密钥进行加密和解密。<br>四种加密算法加密速度解密速度以及加密模式对比</li></ul><table><thead><tr><th>加密算法</th><th>加密速度</th><th>解密速度</th><th>加密模式</th></tr></thead><tbody><tr><td>AES-CT</td><td>750.8</td><td>831.1</td><td>流加密</td></tr><tr><td>AES-CBC</td><td>244.3</td><td>311.6</td><td>分组加密</td></tr><tr><td>3DES-CBC</td><td>38.32</td><td>43.18</td><td>分组加密</td></tr><tr><td>RC4</td><td>766.9</td><td>977.9</td><td>流加密</td></tr></tbody></table><p>HDFS中的透明数据加密（TDE）框架是可扩展的，可以支持其他的加密算法。要扩展加密算法，需要进行以下步骤：</p><ol><li>实现新的加密算法：实现一个新的加密算法，该算法必须实现Java加密扩展（Java Cryptography Extension，JCE）的密码接口，并且需要能够与Hadoop框架集成。</li><li>将算法添加到HDFS的加密框架中：需要将新的加密算法添加到HDFS的加密框架中，这需要编辑Hadoop的配置文件（hadoop-common.xml和hdfs-site.xml）。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>hdf</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hive编译学习</title>
    <link href="/2023/02/18/hive%E7%BC%96%E8%AF%91%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/02/18/hive%E7%BC%96%E8%AF%91%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="hive的编译与执行"><a href="#hive的编译与执行" class="headerlink" title="hive的编译与执行"></a>hive的编译与执行</h1><p>我们简单理解hive，就是把一个sql解析成mr任务发送给hadoop去执行。</p><p>hql –&gt; parse –&gt; ast –&gt; semanticAnalyzer –&gt; logical plan –&gt; opreator tree –&gt; logical optimizer –&gt; operator tree –&gt; physical plan –&gt; task tree –&gt; physical optimizer –&gt; task tree</p><h2 id="sql-转换成语法树"><a href="#sql-转换成语法树" class="headerlink" title="sql 转换成语法树"></a>sql 转换成语法树</h2><p>我们提交一条sql，driver收到之后进入到编译阶段，需要把sql转换成语法树<br>通过创建对象 ParseDriver 然后调用的 parse(command,ctx)方法把 sql语句转换成 ast 抽象语法树</p><figure class="highlight java"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment">   * Compile a new query, but potentially reset taskID counter.  Not resetting task counter</span><br><span class="hljs-comment">   * is useful for generating re-entrant QL queries.</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> command  The HiveQL query to compile</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> resetTaskIds Resets taskID counter if true.</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@return</span> 0 for ok</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">compile</span><span class="hljs-params">(String command, <span class="hljs-type">boolean</span> resetTaskIds)</span>&#123;<br>        PerfLogger perfLogger=PerfLogger.getPerfLogger();<br>        perfLogger.PerfLogBegin(CLASS_NAME,PerfLogger.COMPILE);<br>        <span class="hljs-comment">//holder for parent command type/string when executing reentrant queries</span><br>        QueryState queryState=<span class="hljs-keyword">new</span> <span class="hljs-title class_">QueryState</span>();<br>        ...<br>        <span class="hljs-type">ParseDriver</span> <span class="hljs-variable">pd</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ParseDriver</span>();<br>        <span class="hljs-comment">//解析语法树</span><br>        <span class="hljs-type">ASTNode</span> <span class="hljs-variable">tree</span> <span class="hljs-operator">=</span> pd.parse(command, ctx);<br>  &#125;<br></code></pre></td></tr></table></figure><p>接下来让我们看一下parse如何实现的，sql语句如何变成ast，ast到底是什么？<br>进入方法看一看，将进入方法第一步手心拿到了lexer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> ASTNode <span class="hljs-title function_">parse</span><span class="hljs-params">(String command, Context ctx, <span class="hljs-type">boolean</span> setTokenRewriteStream)</span> <br>      <span class="hljs-keyword">throws</span> ParseException &#123;<br>    LOG.info(<span class="hljs-string">&quot;Parsing command: &quot;</span> + command);<br><br>    <span class="hljs-comment">//ANTLRNoCaseStringStream</span><br>    <span class="hljs-type">HiveLexerX</span> <span class="hljs-variable">lexer</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">HiveLexerX</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ANTLRNoCaseStringStream</span>(command));<br>    <span class="hljs-type">TokenRewriteStream</span> <span class="hljs-variable">tokens</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">TokenRewriteStream</span>(lexer);<br>&#125;<br></code></pre></td></tr></table></figure><p>ANTLRNoCaseStringStream这个类是用来做什么？看到antlr很熟悉，学习一下</p><blockquote><p>ANTLR（全名：ANother Tool for Language Recognition）是基于LL(<em>)算法实现的语法解析器生成器（parser generator），用Java语言编写，使用自上而下（top-down）的递归下降LL剖析器方法。由旧金山大学的Terence Parr博士等人于1989年开始发展。如同一般的词法分析器（lexer）和语法分析器（parser），ANTLR可以用来产生树状分析器（tree parsers）。ANTLR 文法定义使用类似EBNF（Extended Backus-Naur Form）的定义方式，形象十分简洁直观。例如: ANTLR用A : a;来表示规则，旧式的方法则是以 A&#x3D;&gt;a 表示，所以ANTLR是以“:”代替了“&#x3D;&gt;”。ANTLR的规则要以分号“;”结束。又如其他ANTLR符号“|”代表“或”的关系，又如“</em>，+”表示可以出现0次或多次。目前Hibernate与WebLogic都是使用ANTLR做为来解析HQL。在NetBeans IDE中更以ANTLR解析C++。Twitter搜索使用ANTLR解析，一天超过200亿次查询。</p></blockquote><p>ANTLRNoCaseStringStream这个把所有字符转化成了大写,HiveLexerX 这是是用来做词法分析，语法分析通过TokenRewriteStream拿到所有tokens(tokens是不可分割的一个个单词)</p><p>接下来就要进行语法解析</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">/构造hive parser<br>    HiveParser parser = <span class="hljs-keyword">new</span> <span class="hljs-constructor">HiveParser(<span class="hljs-params">tokens</span>)</span>;<br>    <span class="hljs-keyword">if</span> (ctx != null) &#123;<br>      parser.set<span class="hljs-constructor">HiveConf(<span class="hljs-params">ctx</span>.<span class="hljs-params">getConf</span>()</span>);<br>    &#125;<br>    parser.set<span class="hljs-constructor">TreeAdaptor(<span class="hljs-params">adaptor</span>)</span>;<br>    <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">HiveParser</span>.</span></span>statement_return r = null;<br>    <span class="hljs-keyword">try</span> &#123;<br>      r = parser.statement<span class="hljs-literal">()</span>;<br>    &#125; catch (RecognitionException e) &#123;<br>      e.print<span class="hljs-constructor">StackTrace()</span>;<br>      throw <span class="hljs-keyword">new</span> <span class="hljs-constructor">ParseException(<span class="hljs-params">parser</span>.<span class="hljs-params">errors</span>)</span>;<br>    &#125;<br><br>    <span class="hljs-keyword">if</span> (lexer.get<span class="hljs-constructor">Errors()</span>.size<span class="hljs-literal">()</span><span class="hljs-operator"> == </span><span class="hljs-number">0</span><span class="hljs-operator"> &amp;&amp; </span>parser.errors.size<span class="hljs-literal">()</span><span class="hljs-operator"> == </span><span class="hljs-number">0</span>) &#123;<br>      <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>info(<span class="hljs-string">&quot;Parse Completed&quot;</span>);<br>    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (lexer.get<span class="hljs-constructor">Errors()</span>.size<span class="hljs-literal">()</span> != <span class="hljs-number">0</span>) &#123;<br>      throw <span class="hljs-keyword">new</span> <span class="hljs-constructor">ParseException(<span class="hljs-params">lexer</span>.<span class="hljs-params">getErrors</span>()</span>);<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>      throw <span class="hljs-keyword">new</span> <span class="hljs-constructor">ParseException(<span class="hljs-params">parser</span>.<span class="hljs-params">errors</span>)</span>;<br>    &#125;<br><br>    ASTNode tree = (ASTNode) r.get<span class="hljs-constructor">Tree()</span>;<br><br></code></pre></td></tr></table></figure><p>首先通过上述tokens 构造了 HiveParser对象<br>执行parser 得到了返回值，看到最后的代码  r.getTree()就是我们要求的语法树<br>针对parse来说，语义语法都是我们事先定义好的，通过antlr最终把一个sql转化成了抽象语法树语法树</p><p>##抽象语法树 –&gt; 逻辑执行计划<br>语法树想要变成执行计划，首先要把抽象语法树变成一个 query block</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">BaseSemanticAnalyzer sem = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">SemanticAnalyzerFactory</span>.</span></span>get(conf, tree);<br><br>     <span class="hljs-comment">// Do semantic analysis and plan generation</span><br>     <span class="hljs-keyword">if</span> (saHooks != null) &#123;<br>       HiveSemanticAnalyzerHookContext hookCtx = <span class="hljs-keyword">new</span> <span class="hljs-constructor">HiveSemanticAnalyzerHookContextImpl()</span>;<br>       hookCtx.set<span class="hljs-constructor">Conf(<span class="hljs-params">conf</span>)</span>;<br>       hookCtx.set<span class="hljs-constructor">UserName(<span class="hljs-params">userName</span>)</span>;<br>       hookCtx.set<span class="hljs-constructor">IpAddress(SessionState.<span class="hljs-params">get</span>()</span>.get<span class="hljs-constructor">UserIpAddress()</span>);<br>       hookCtx.set<span class="hljs-constructor">Command(<span class="hljs-params">command</span>)</span>;<br>       <span class="hljs-keyword">for</span> (HiveSemanticAnalyzerHook hook : saHooks) &#123;<br>         <span class="hljs-comment">//经过分析hook之后</span><br>         tree = hook.pre<span class="hljs-constructor">Analyze(<span class="hljs-params">hookCtx</span>, <span class="hljs-params">tree</span>)</span>;<br>       &#125;<br>       <span class="hljs-comment">//最后的处理逻辑</span><br>       sem.analyze(tree, ctx);<br>       hookCtx.update(sem);<br>       <span class="hljs-keyword">for</span> (HiveSemanticAnalyzerHook hook : saHooks) &#123;<br>         hook.post<span class="hljs-constructor">Analyze(<span class="hljs-params">hookCtx</span>, <span class="hljs-params">sem</span>.<span class="hljs-params">getRootTasks</span>()</span>);<br>       &#125;<br>     &#125; <span class="hljs-keyword">else</span> &#123;<br>       <span class="hljs-comment">//最后的处理逻辑</span><br>       sem.analyze(tree, ctx);<br>     &#125;<br></code></pre></td></tr></table></figure><p>从上面可以看出，通过语义分析器 SemanticAnalyzer 来对 逻辑语法树进行分析，接下来我们看如果分析的</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scss">public void <span class="hljs-built_in">analyze</span>(ASTNode ast, Context ctx) throws SemanticException &#123;<br>    <span class="hljs-comment">//初始化上下文</span><br>    <span class="hljs-built_in">initCtx</span>(ctx);<br>    <span class="hljs-comment">//进行分析前的一些初始化</span><br>    <span class="hljs-built_in">init</span>(true);<br>    <span class="hljs-comment">//解析语法树核心方法</span><br>    <span class="hljs-built_in">analyzeInternal</span>(ast);<br>  &#125;<br></code></pre></td></tr></table></figure><p>首先看一下init方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">(<span class="hljs-type">boolean</span> clearPartsCache)</span> &#123;<br>   <span class="hljs-comment">// clear most members</span><br>   <span class="hljs-comment">//这一步 清理了一些成员变量，置为空</span><br>   reset(clearPartsCache);<br>   <span class="hljs-comment">// init</span><br>   <span class="hljs-comment">//创建了 query block对象</span><br>   <span class="hljs-type">QB</span> <span class="hljs-variable">qb</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">QB</span>(<span class="hljs-literal">null</span>, <span class="hljs-literal">null</span>, <span class="hljs-literal">false</span>);<br>   <span class="hljs-built_in">this</span>.qb = qb;<br> &#125;<br></code></pre></td></tr></table></figure><p>创建好了 qb ，后续分析的时候会使用到，接下来看分析 analyzeInternal(ast)</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-comment">// 1. Generate Resolved Parse tree from syntax tree</span><br>   <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>info(<span class="hljs-string">&quot;Starting Semantic Analysis&quot;</span>);<br>   <span class="hljs-keyword">if</span> (!gen<span class="hljs-constructor">ResolvedParseTree(<span class="hljs-params">ast</span>, <span class="hljs-params">plannerCtx</span>)</span>) &#123;<br>     return;<br>   &#125;<br></code></pre></td></tr></table></figure><p>从抽象语法树生成解析树，进入方法查看如何从抽象语法树生成解析树</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">private <span class="hljs-type">void</span> processPositionAlias(ASTNode ast) throws SemanticException &#123;<br>    <span class="hljs-type">boolean</span> isByPos = <span class="hljs-keyword">false</span>;<br>    //查看配置是否开启hive别名<br>    <span class="hljs-keyword">if</span> (HiveConf.getBoolVar(conf,<br>          HiveConf.ConfVars.HIVE_GROUPBY_ORDERBY_POSITION_ALIAS) == <span class="hljs-keyword">true</span>) &#123;<br>      isByPos = <span class="hljs-keyword">true</span>;<br>    &#125;<br><br>   <br>    <span class="hljs-type">boolean</span> isAllCol;<br>    ASTNode selectNode = <span class="hljs-keyword">null</span>;<br>    ASTNode groupbyNode = <span class="hljs-keyword">null</span>;<br>    ASTNode orderbyNode = <span class="hljs-keyword">null</span>;<br><br>    // <span class="hljs-keyword">get</span> node <span class="hljs-keyword">type</span><br>    //每个节点上面可能会出现  <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> <span class="hljs-built_in">table_name</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-number">1</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-number">1</span><br>    //所以可能会出现  令牌为 <span class="hljs-keyword">select</span>  <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span>的三种情况<br>    <span class="hljs-type">int</span> child_count = ast.getChildCount();<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> child_pos = <span class="hljs-number">0</span>; child_pos &lt; child_count; ++child_pos) &#123;<br>      ASTNode node = (ASTNode) ast.getChild(child_pos);<br>      <span class="hljs-type">int</span> <span class="hljs-keyword">type</span> = node.getToken().getType();<br>      <span class="hljs-keyword">if</span> (<span class="hljs-keyword">type</span> == HiveParser.TOK_SELECT) &#123;<br>        selectNode = node;<br>      &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-keyword">type</span> == HiveParser.TOK_GROUPBY) &#123;<br>        groupbyNode = node;<br>      &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-keyword">type</span> == HiveParser.TOK_ORDERBY) &#123;<br>        orderbyNode = node;<br>      &#125;<br>    &#125;<br>    //然后去分析处理三种情况，<br>    <span class="hljs-keyword">if</span> (selectNode != <span class="hljs-keyword">null</span>) &#123;<br>      <span class="hljs-type">int</span> selectExpCnt = selectNode.getChildCount();<br><br>      // replace <span class="hljs-keyword">each</span> <span class="hljs-keyword">of</span> the position <span class="hljs-keyword">alias</span> <span class="hljs-keyword">in</span> GROUPBY <span class="hljs-keyword">with</span> the actual <span class="hljs-keyword">column</span> <span class="hljs-type">name</span><br>      <span class="hljs-keyword">if</span> (groupbyNode != <span class="hljs-keyword">null</span>) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> child_pos = <span class="hljs-number">0</span>; child_pos &lt; groupbyNode.getChildCount(); ++child_pos) &#123;<br>          ASTNode node = (ASTNode) groupbyNode.getChild(child_pos);<br>          <span class="hljs-keyword">if</span> (node.getToken().getType() == HiveParser.Number) &#123;<br>            //必须开启响应的参数<br>            <span class="hljs-keyword">if</span> (isByPos) &#123;<br>              。。。<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>              warn(&quot;Using constant number  &quot; + node.getText() +<br>                &quot; in group by. If you try to use position alias when hive.groupby.orderby.position.alias is false, the position alias will be ignored.&quot;);<br>            &#125;<br>          &#125;<br>        &#125;<br>      &#125;<br><br>      // replace <span class="hljs-keyword">each</span> <span class="hljs-keyword">of</span> the position <span class="hljs-keyword">alias</span> <span class="hljs-keyword">in</span> ORDERBY <span class="hljs-keyword">with</span> the actual <span class="hljs-keyword">column</span> <span class="hljs-type">name</span><br>      <span class="hljs-keyword">if</span> (orderbyNode != <span class="hljs-keyword">null</span>) &#123;<br>        isAllCol = <span class="hljs-keyword">false</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> child_pos = <span class="hljs-number">0</span>; child_pos &lt; selectNode.getChildCount(); ++child_pos) &#123;<br>          ASTNode node = (ASTNode) selectNode.getChild(child_pos).getChild(<span class="hljs-number">0</span>);<br>          <span class="hljs-keyword">if</span> (node.getToken().getType() == HiveParser.TOK_ALLCOLREF) &#123;<br>            isAllCol = <span class="hljs-keyword">true</span>;<br>          &#125;<br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> child_pos = <span class="hljs-number">0</span>; child_pos &lt; orderbyNode.getChildCount(); ++child_pos) &#123;<br>          ASTNode colNode = (ASTNode) orderbyNode.getChild(child_pos);<br>          ASTNode node = (ASTNode) colNode.getChild(<span class="hljs-number">0</span>);<br>          <span class="hljs-keyword">if</span> (node.getToken().getType() == HiveParser.Number) &#123;<br>            <span class="hljs-keyword">if</span>( isByPos ) &#123;<br>              <span class="hljs-keyword">if</span> (!isAllCol) &#123;<br>                。。。<br>              &#125;<br>            &#125; <span class="hljs-keyword">else</span> &#123; //<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">using</span> position <span class="hljs-keyword">alias</span> <span class="hljs-keyword">and</span> it <span class="hljs-keyword">is</span> a number.<br>              warn(&quot;Using constant number &quot; + node.getText() +<br>                &quot; in order by. If you try to use position alias when hive.groupby.orderby.position.alias is false, the position alias will be ignored.&quot;);<br>            &#125;<br>          &#125;<br>        &#125;<br>      &#125;<br>    &#125;<br><br>    //递归处理处理所有子节点<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> child_pos = <span class="hljs-number">0</span>; child_pos &lt; child_count; ++child_pos) &#123;<br>      processPositionAlias((ASTNode) ast.getChild(child_pos));<br>    &#125;<br>    <span class="hljs-keyword">return</span>;<br>  &#125;<br></code></pre></td></tr></table></figure><p>从上面可以看出，第一步首先分析语法树中的别名，并且进行处理</p><blockquote><p>hive.groupby.orderby.position.alias 这个是针对 group by  order by 别名的配置，可以使用select * from table_name group by 1 order by 1<br>这样的  1需要替换成对应的列</p></blockquote><p>分析完这个查看是否是创建表</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs kotlin"><span class="hljs-comment">// 2. analyze create table command</span><br><span class="hljs-keyword">if</span> (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) &#123;<br>  <span class="hljs-comment">// if it is not CTAS, we don&#x27;t need to go further and just return</span><br>  <span class="hljs-keyword">if</span> ((child =X, plannerCtx)) == <span class="hljs-literal">null</span>) &#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>  &#125;<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>  SessionState.<span class="hljs-keyword">get</span>().setCommandType(HiveOperation.QUERY);<br>&#125;<br></code></pre></td></tr></table></figure><p>如果是创建表的命令，去进行响应的解析，如果不是创建表就是query</p><blockquote><p>分析create table命令。如果它是常规的create-table或类似于create-table的语句，我们将创建一个DDLWork并返回true。如果它是一个create-table-as-select，<br>我们将获取必要的信息（例如SerDe和Storage Format）并将其放入QB中，然后返回false，这表明语义分析器的其余部分需要处理select语句。到SerDe和存储格式。</p></blockquote><p>接下来分析是不是创建视图</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-comment">// 3. analyze create view command</span><br>    <span class="hljs-keyword">if</span> (ast.get<span class="hljs-constructor">Token()</span>.get<span class="hljs-constructor">Type()</span><span class="hljs-operator"> == </span>HiveParser.TOK_CREATEVIEW<span class="hljs-operator"></span><br><span class="hljs-operator">        || </span>(ast.get<span class="hljs-constructor">Token()</span>.get<span class="hljs-constructor">Type()</span><span class="hljs-operator"> == </span>HiveParser.TOK_ALTERVIEW<span class="hljs-operator"> &amp;&amp; </span>ast.get<span class="hljs-constructor">Child(1)</span>.get<span class="hljs-constructor">Type()</span><span class="hljs-operator"> == </span>HiveParser.TOK_QUERY)) &#123;<br>      child = analyze<span class="hljs-constructor">CreateView(<span class="hljs-params">ast</span>, <span class="hljs-params">qb</span>)</span>;<br>      <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">SessionState</span>.</span></span>get<span class="hljs-literal">()</span>.set<span class="hljs-constructor">CommandType(HiveOperation.CREATEVIEW)</span>;<br>      <span class="hljs-keyword">if</span> (child<span class="hljs-operator"> == </span>null) &#123;<br>        return <span class="hljs-literal">false</span>;<br>      &#125;<br>      viewSelect = child;<br>      <span class="hljs-comment">// prevent view from referencing itself</span><br>      viewsExpanded.add(createVwDesc.get<span class="hljs-constructor">ViewName()</span>);<br>    &#125;<br></code></pre></td></tr></table></figure><p>分析完视图进行查询相关的解析</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">Phase1Ctx ctx_1 = init<span class="hljs-constructor">Phase1Ctx()</span>;<br>   <br>   pre<span class="hljs-constructor">ProcessForInsert(<span class="hljs-params">child</span>, <span class="hljs-params">qb</span>)</span>;<br>   <span class="hljs-keyword">if</span> (!<span class="hljs-keyword">do</span><span class="hljs-constructor">Phase1(<span class="hljs-params">child</span>, <span class="hljs-params">qb</span>, <span class="hljs-params">ctx_1</span>, <span class="hljs-params">plannerCtx</span>)</span>) &#123;<br>     <span class="hljs-comment">// if phase1Result false return</span><br>     return <span class="hljs-literal">false</span>;<br>   &#125;<br>   <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>info(<span class="hljs-string">&quot;Completed phase 1 of Semantic Analysis&quot;</span>);<br><br>   <span class="hljs-comment">// 5. Resolve Parse Tree</span><br>   get<span class="hljs-constructor">MetaData(<span class="hljs-params">qb</span>)</span>;<br>   <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>info(<span class="hljs-string">&quot;Completed getting MetaData in Semantic Analysis&quot;</span>);<br><br>   plannerCtx.set<span class="hljs-constructor">ParseTreeAttr(<span class="hljs-params">child</span>, <span class="hljs-params">ctx_1</span>)</span>;<br><br>   return <span class="hljs-literal">true</span>;<br></code></pre></td></tr></table></figure><p>看到上面的代码，通过preProcessForInsert 分析是不是insert，然后处理。 doPhase1进行后续所有节点的分析,然后获取元数据。生成queryBolck完成，<br>抽象语法树生辰解析树也就完成</p><blockquote><p>段1 ：（包括但不限于）：1.获取所有表&#x2F;子查询的所有别名，并在aliasToTabs，aliasToSubq中进行适当的映射。<br>2.获取目标的位置并命名子句“ inclause” + i<br>3.从聚合树的字符串表示形式到实际聚合AST创建映射。<br>4.在destToSelExpr中创建从子句名称到选择表达式AST的映射。<br>5.在aliasToLateralViews中创建从表别名到侧面视图AST的映射。</p></blockquote><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-comment">// 2. Gen OP Tree from resolved Parse Tree</span><br>   Operator sinkOp = gen<span class="hljs-constructor">OPTree(<span class="hljs-params">ast</span>, <span class="hljs-params">plannerCtx</span>)</span>;<br></code></pre></td></tr></table></figure><p>通过调用genOPTree生成逻辑执行计划 Operator （待研究）,然后推断结果的schema</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-comment">// 3. Deduce Resultset Schema</span><br>   <span class="hljs-keyword">if</span> (createVwDesc != null) &#123;<br>     resultSchema = convert<span class="hljs-constructor">RowSchemaToViewSchema(<span class="hljs-params">opParseCtx</span>.<span class="hljs-params">get</span>(<span class="hljs-params">sinkOp</span>)</span>.get<span class="hljs-constructor">RowResolver()</span>);<br>   &#125; <span class="hljs-keyword">else</span> &#123;<br>     resultSchema = convert<span class="hljs-constructor">RowSchemaToResultSetSchema(<span class="hljs-params">opParseCtx</span>.<span class="hljs-params">get</span>(<span class="hljs-params">sinkOp</span>)</span>.get<span class="hljs-constructor">RowResolver()</span>,<br>         <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">HiveConf</span>.</span></span>get<span class="hljs-constructor">BoolVar(<span class="hljs-params">conf</span>, HiveConf.ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES)</span>);<br>   &#125;<br></code></pre></td></tr></table></figure><p>然后为优化器和物理编译器生成解析上下文</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">copyInfoToQueryProperties(queryProperties)<span class="hljs-comment">;</span><br>   ParseContext pCtx = <span class="hljs-keyword">new</span> ParseContext(conf, opToPartPruner, opToPartList, topOps,<br>       <span class="hljs-keyword">new</span> HashSet&lt;JoinOperator&gt;(joinContext.keySet()),<br>       <span class="hljs-keyword">new</span> HashSet&lt;SMBMapJoinOperator&gt;(smbMapJoinContext.keySet()),<br><span class="hljs-built_in">       loadTableWork,</span> loadFileWork, ctx, idToTableNameMap, destTableId, uCtx,<br><span class="hljs-built_in">       listMapJoinOpsNoReducer,</span> prunedPartitions, opToSamplePruner,<br><span class="hljs-built_in">       globalLimitCtx,</span> nameToSplitSample, inputs, rootTasks, opToPartToSkewedPruner,<br><span class="hljs-built_in">       viewAliasToInput,</span> reduceSinkOperatorsAddedByEnforceBucketingSorting,<br><span class="hljs-built_in">       analyzeRewrite,</span> tableDesc, queryProperties)<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure><p>然后执行逻辑优化器</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">if</span> (<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>is<span class="hljs-constructor">DebugEnabled()</span>) &#123;<br>     <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>debug(<span class="hljs-string">&quot;Before logical optimization\n&quot;</span> + <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Operator</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">String(<span class="hljs-params">pCtx</span>.<span class="hljs-params">getTopOps</span>()</span>.values<span class="hljs-literal">()</span>));<br>   &#125;<br>   Optimizer optm = <span class="hljs-keyword">new</span> <span class="hljs-constructor">Optimizer()</span>;<br>   optm.set<span class="hljs-constructor">Pctx(<span class="hljs-params">pCtx</span>)</span>;<br>   optm.initialize(conf);<br>   pCtx = optm.optimize<span class="hljs-literal">()</span>;<br>   FetchTask origFetchTask = pCtx.get<span class="hljs-constructor">FetchTask()</span>;<br>   <span class="hljs-keyword">if</span> (<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>is<span class="hljs-constructor">DebugEnabled()</span>) &#123;<br>     <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>debug(<span class="hljs-string">&quot;After logical optimization\n&quot;</span> + <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Operator</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">String(<span class="hljs-params">pCtx</span>.<span class="hljs-params">getTopOps</span>()</span>.values<span class="hljs-literal">()</span>));<br>   &#125;<br></code></pre></td></tr></table></figure><p>优化物理操作并转化为固定执行引擎</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">if</span> (!ctx.get<span class="hljs-constructor">ExplainLogical()</span>) &#123;<br>      TaskCompiler compiler = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">TaskCompilerFactory</span>.</span></span>get<span class="hljs-constructor">Compiler(<span class="hljs-params">conf</span>, <span class="hljs-params">pCtx</span>)</span>;<br>      compiler.init(conf, console, db);<br>      compiler.compile(pCtx, rootTasks, inputs, outputs);<br>      fetchTask = pCtx.get<span class="hljs-constructor">FetchTask()</span>;<br>    &#125;<br>    <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>info(<span class="hljs-string">&quot;Completed plan generation&quot;</span>);<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>hive归档</title>
    <link href="/2023/01/08/hive%E5%BD%92%E6%A1%A3/"/>
    <url>/2023/01/08/hive%E5%BD%92%E6%A1%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="学习hive归档使用"><a href="#学习hive归档使用" class="headerlink" title="学习hive归档使用"></a>学习hive归档使用</h1><h2 id="文本文件存储测试"><a href="#文本文件存储测试" class="headerlink" title="文本文件存储测试"></a>文本文件存储测试</h2><h3 id="构建文本测试数据"><a href="#构建文本测试数据" class="headerlink" title="构建文本测试数据"></a>构建文本测试数据</h3><p>创建一个分区表和一个没有分区的表</p><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> A(<br>    id <span class="hljs-type">int</span>,<br>    name string<br>)partitioned <span class="hljs-keyword">by</span> (dt string);<br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> B(<br>    id <span class="hljs-type">int</span>,<br>    name string<br>);<br></code></pre></td></tr></table></figure><p>构建测试数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">--往A表里面插入俩条数据，也就是俩个小文件</span><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> A <span class="hljs-keyword">partition</span>(dt<span class="hljs-operator">=</span><span class="hljs-string">&#x27;20210101&#x27;</span>) <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> A <span class="hljs-keyword">partition</span>(dt<span class="hljs-operator">=</span><span class="hljs-string">&#x27;20210101&#x27;</span>) <span class="hljs-keyword">values</span>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;leo&#x27;</span>);<br><span class="hljs-comment">--往B表里面插入俩条数据，也就是俩个小文件</span><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> B <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> B <span class="hljs-keyword">values</span>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;leo&#x27;</span>);<br></code></pre></td></tr></table></figure><p>查看测试数据</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>b<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive          <span class="hljs-number">6</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">09</span>:<span class="hljs-number">10</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>b/<span class="hljs-number">000000</span>_0<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive          <span class="hljs-number">6</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">09</span>:<span class="hljs-number">10</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>b/<span class="hljs-number">000000</span>_0_copy_1<br><br>hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a/dt=<span class="hljs-number">20210101</span><br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive          <span class="hljs-number">6</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">09</span>:<span class="hljs-number">09</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive          <span class="hljs-number">6</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">09</span>:<span class="hljs-number">09</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0_copy_1<br></code></pre></td></tr></table></figure><p>启用archive</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.archive.<span class="hljs-attribute">enabled</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.archive.har.parentdir.<span class="hljs-attribute">settable</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> har.partfile.<span class="hljs-attribute">size</span>=1099511627776;<br></code></pre></td></tr></table></figure><p>测试归档分区表</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">ALTER TABLE A ARCHIVE PARTITION(dt=<span class="hljs-emphasis">&#x27;20210101&#x27;</span>);<br>执行成功，查看执行结果<br><span class="hljs-section">select * from a;</span><br><span class="hljs-section">+-------+---------+-----------+</span><br><span class="hljs-section">| a.id  | a.name  |   a.dt    |</span><br><span class="hljs-section">+-------+---------+-----------+</span><br>| 2     | leo     | 20210101  |<br><span class="hljs-section">| 1     | lee     | 20210101  |</span><br><span class="hljs-section">+-------+---------+-----------+</span><br>归档的文件可以正常执行，查看文件信息<br>hdfs dfs -ls /user/hive/warehouse/a/dt=20210101<br>drwxrwx--x+  - hive hive          0 2021-05-11 09:16 /user/hive/warehouse/a/dt=20210101/data.har<br>变成了正常hdfs归档的文件<br></code></pre></td></tr></table></figure><p>测试归档无分区的表</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs subunit">ALTER TABLE B ARCHIVE;<br><span class="hljs-keyword">Error: </span>Error while compiling statement: FAILED: SemanticException [Error 10110]: ARCHIVE can only be run on partitions (state=42000,code=10110)<br>不支持非分区表的归档<br></code></pre></td></tr></table></figure><p>测试插入数据</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> A <span class="hljs-keyword">partition</span>(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>) <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br>Error: Error <span class="hljs-keyword">while</span> compiling <span class="hljs-keyword">statement</span>: FAILED: SemanticException <span class="hljs-keyword">Insert</span> <span class="hljs-keyword">conflict</span> <span class="hljs-keyword">with</span> existing archive: dt=<span class="hljs-number">20210101</span> (state=<span class="hljs-number">42000</span>,code=<span class="hljs-number">40000</span>)<br>不支持往归档里面插入数据<br></code></pre></td></tr></table></figure><p>解归档插入数据</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs awk">ALTER TABLE A UNARCHIVE PARTITION(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>);<br>INFO  : Compiling command(queryId=hive_20210511101147_9129e779-eaeb-<span class="hljs-number">402</span>c-b43e-dfb6f3df8afa): ALTER TABLE A UNARCHIVE PARTITION(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>)<br>INFO  : Semantic Analysis Completed<br>INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)<br>INFO  : Completed compiling command(queryId=hive_20210511101147_9129e779-eaeb-<span class="hljs-number">402</span>c-b43e-dfb6f3df8afa); Time taken: <span class="hljs-number">0.337</span> seconds<br>INFO  : Executing command(queryId=hive_20210511101147_9129e779-eaeb-<span class="hljs-number">402</span>c-b43e-dfb6f3df8afa): ALTER TABLE A UNARCHIVE PARTITION(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>)<br>INFO  : Starting task [Stage-<span class="hljs-number">0</span>:DDL] <span class="hljs-keyword">in</span> serial mode<br>INFO  : Copying har:<span class="hljs-regexp">//</span>hdfs-nameservice1<span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>data.har to hdfs:<span class="hljs-regexp">//</span>nameservice1<span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>.hive-staging_hive_2021-<span class="hljs-number">05</span>-<span class="hljs-number">11</span>_10-<span class="hljs-number">11</span>-<span class="hljs-number">47</span>_922_5507670282318603233-<span class="hljs-number">620</span>/-ext-<span class="hljs-number">10000</span><br>INFO  : Succefully Copied har:<span class="hljs-regexp">//</span>hdfs-nameservice1<span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>data.har to hdfs:<span class="hljs-regexp">//</span>nameservice1<span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>.hive-staging_hive_2021-<span class="hljs-number">05</span>-<span class="hljs-number">11</span>_10-<span class="hljs-number">11</span>-<span class="hljs-number">47</span>_922_5507670282318603233-<span class="hljs-number">620</span>/-ext-<span class="hljs-number">10000</span><br>INFO  : Moving hdfs:<span class="hljs-regexp">//</span>nameservice1<span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>.hive-staging_hive_2021-<span class="hljs-number">05</span>-<span class="hljs-number">11</span>_10-<span class="hljs-number">11</span>-<span class="hljs-number">47</span>_922_5507670282318603233-<span class="hljs-number">620</span><span class="hljs-regexp">/-ext-10000 to hdfs:/</span><span class="hljs-regexp">/nameservice1/u</span>ser<span class="hljs-regexp">/hive/</span>warehouse<span class="hljs-regexp">/a/</span>dt=<span class="hljs-number">20210101</span>_INTERMEDIATE_EXTRACTED<br>INFO  : Moving hdfs:<span class="hljs-regexp">//</span>nameservice1<span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101 to hdfs:/</span><span class="hljs-regexp">/nameservice1/u</span>ser<span class="hljs-regexp">/hive/</span>warehouse<span class="hljs-regexp">/a/</span>dt=<span class="hljs-number">20210101</span>_INTERMEDIATE_ARCHIVED<br>INFO  : Moving hdfs:<span class="hljs-regexp">//</span>nameservice1<span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101_INTERMEDIATE_EXTRACTED to hdfs:/</span><span class="hljs-regexp">/nameservice1/u</span>ser<span class="hljs-regexp">/hive/</span>warehouse<span class="hljs-regexp">/a/</span>dt=<span class="hljs-number">20210101</span><br>INFO  : Completed executing command(queryId=hive_20210511101147_9129e779-eaeb-<span class="hljs-number">402</span>c-b43e-dfb6f3df8afa); Time taken: <span class="hljs-number">0.241</span> seconds<br>INFO  : OK<br>No rows affected (<span class="hljs-number">0.599</span> seconds)<br>可以插入成功<br>insert into table A partition(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>) values(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br>查看文件<br>hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a/dt=<span class="hljs-number">20210101</span><br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive          <span class="hljs-number">6</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">10</span>:<span class="hljs-number">11</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive          <span class="hljs-number">6</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">10</span>:<span class="hljs-number">11</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0_copy_1<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive          <span class="hljs-number">6</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">10</span>:<span class="hljs-number">13</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0_copy_2<br></code></pre></td></tr></table></figure><p>##parquet构建<br>###构建parquet测试数据</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> A(<br>    id <span class="hljs-type">int</span>,<br>    <span class="hljs-type">name</span> string<br>)partitioned <span class="hljs-keyword">by</span> (dt string)<br>stored <span class="hljs-keyword">as</span> parquet;<br><br><span class="hljs-comment">--往A表里面插入俩条数据，也就是俩个小文件</span><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> A <span class="hljs-keyword">partition</span>(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>) <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> A <span class="hljs-keyword">partition</span>(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>) <span class="hljs-keyword">values</span>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;leo&#x27;</span>);<br><span class="hljs-comment">--往B表里面插入俩条数据，也就是俩个小文件</span><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> B <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> B <span class="hljs-keyword">values</span>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;leo&#x27;</span>);<br></code></pre></td></tr></table></figure><p>查看文件</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs awk">hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a/dt=<span class="hljs-number">20210101</span><br>Found <span class="hljs-number">2</span> items<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">11</span>:<span class="hljs-number">31</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">11</span>:<span class="hljs-number">31</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0_copy_1<br><br>hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>b<br>Found <span class="hljs-number">2</span> items<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">11</span>:<span class="hljs-number">31</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>b/<span class="hljs-number">000000</span>_0<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">11</span>:<span class="hljs-number">31</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>b/<span class="hljs-number">000000</span>_0_copy_1<br></code></pre></td></tr></table></figure><p>执行归档命令</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs gradle">set hive.archive.enabled=<span class="hljs-keyword">true</span>;<br>set hive.archive.har.parentdir.settable=<span class="hljs-keyword">true</span>;<br>set har.partfile.<span class="hljs-keyword">size</span>=<span class="hljs-number">1099511627776</span>;<br><br>ALTER TABLE A ARCHIVE PARTITION(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>);<br>select * <span class="hljs-keyword">from</span> a;<br>+-------+---------+-----------+<br>| a.id  | a.name  |   a.dt    |<br>+-------+---------+-----------+<br>| <span class="hljs-number">2</span>     | leo     | <span class="hljs-number">20210101</span>  |<br>| <span class="hljs-number">1</span>     | lee     | <span class="hljs-number">20210101</span>  |<br>+-------+---------+-----------+<br><span class="hljs-number">2</span> rows selected (<span class="hljs-number">0.475</span> seconds)<br><br>查看文件<br>hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a/dt=<span class="hljs-number">20210101</span><br>Found <span class="hljs-number">1</span> items<br>drwxrwx--x+  - hive hive          <span class="hljs-number">0</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">13</span>:<span class="hljs-number">58</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>data.har<br><br>插入数据<br>insert <span class="hljs-keyword">into</span> table A partition(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>) values(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br>Error: Error <span class="hljs-keyword">while</span> compiling statement: FAILED: SemanticException Insert conflict with existing archive: dt=<span class="hljs-number">20210101</span> (state=<span class="hljs-number">42000</span>,code=<span class="hljs-number">40000</span>)<br><br>执行解归档<br>ALTER TABLE A UNARCHIVE PARTITION(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>);<br><br>查看文件<br>hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a/dt=<span class="hljs-number">20210101</span><br>Found <span class="hljs-number">3</span> items<br>drwxrwx--x+  - hive hive          <span class="hljs-number">0</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">14</span>:<span class="hljs-number">01</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>.hive-staging_hive_2021-<span class="hljs-number">05</span>-<span class="hljs-number">11</span>_13-<span class="hljs-number">58</span>-<span class="hljs-number">09</span>_515_6677546453372938669-<span class="hljs-number">627</span><br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">14</span>:<span class="hljs-number">01</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">14</span>:<span class="hljs-number">01</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0_copy_<br><br>执行插入可以正常插入<br>insert <span class="hljs-keyword">into</span> table A partition(dt=<span class="hljs-string">&#x27;20210101&#x27;</span>) values(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;lee&#x27;</span>);<br>select * <span class="hljs-keyword">from</span> a;<br> <br>hdfs dfs -ls <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a/dt=<span class="hljs-number">20210101</span><br>Found <span class="hljs-number">4</span> items<br>drwxrwx--x+  - hive hive          <span class="hljs-number">0</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">14</span>:<span class="hljs-number">01</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span>.hive-staging_hive_2021-<span class="hljs-number">05</span>-<span class="hljs-number">11</span>_13-<span class="hljs-number">58</span>-<span class="hljs-number">09</span>_515_6677546453372938669-<span class="hljs-number">627</span><br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">14</span>:<span class="hljs-number">01</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">14</span>:<span class="hljs-number">01</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0_copy_1<br>-rwxrwx--x+  <span class="hljs-number">3</span> hive hive        <span class="hljs-number">410</span> <span class="hljs-number">2021</span>-<span class="hljs-number">05</span>-<span class="hljs-number">11</span> <span class="hljs-number">14</span>:<span class="hljs-number">02</span> <span class="hljs-regexp">/user/</span>hive<span class="hljs-regexp">/warehouse/</span>a<span class="hljs-regexp">/dt=20210101/</span><span class="hljs-number">000000</span>_0_copy_2<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>yarn公平调度和容量调度</title>
    <link href="/2022/10/08/yarn%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%92%8C%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6/"/>
    <url>/2022/10/08/yarn%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%92%8C%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="公平调度和容量调度"><a href="#公平调度和容量调度" class="headerlink" title="公平调度和容量调度"></a>公平调度和容量调度</h1><h2 id="max-min-fairness-share"><a href="#max-min-fairness-share" class="headerlink" title="max-min fairness share"></a>max-min fairness share</h2><p>最大值最小化公平分配算法：尽可能满足最小的需求，然后把剩下的资源公平的分配给其他的需求。</p><figure class="highlight routeros"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><pre><code class="hljs routeros">资源总数/用户数=每个用户的平均资源<br>1、用户的平均资源都不能满足最小的资源需求，那么每个用户分到的资源就是平均资源。<br>2、如果能满足一些用户的需求，同时这些用户使用不到平均资源量，那么可以把剩下的资源平均分给其他的任务<br>举个例子：<br>总资源：10 有4隔用户分别需求是：1 2 4 5<br>10/<span class="hljs-attribute">4</span>=2.5,每个用户分的平均资源是2.5<br>用户 1 2 都用不到2.5，他们分配的资源就是1+<span class="hljs-attribute">2</span>=3，身下2隔资源<br>用户3 4 的资源都不满足，所以2，2.5+2&gt;4,用户3可以满足，所以最后剩下的0.5+2.<span class="hljs-attribute">5</span>=3是用户4分的的资源<br></code></pre></td></tr></table></figure><h2 id="调度算法DRF"><a href="#调度算法DRF" class="headerlink" title="调度算法DRF"></a>调度算法DRF</h2><p>dominant resource fairness:主要资源公平调度,主要是多种资源共同参与调度，不在是单一的内存、CPU、IO</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs tap">拿一个yarn上面的任务距离：<br>系统拥有<span class="hljs-number"> 9 </span>CORE<span class="hljs-number"> 18 </span>MEMORY<br>A任务需要<span class="hljs-number"> 1 </span>CORE<span class="hljs-number"> 4 </span>MEMORY，1/9 &lt; 4/18，那么A任务的Dominant Resource就是MEMORY = 2/9<br>B任务需要<span class="hljs-number"> 3 </span>CORE<span class="hljs-number"> 1 </span>MEMORY，3/9 &gt; 1/18，那么B任务的Diminant Resource就是CORE = 3/9<br>想要公平的调度那么最好结果就是 A*3 + B*2 = (3+6)9 CORE (12+2)14MEMORY<br></code></pre></td></tr></table></figure><p>分配步骤</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-number">1</span>、启动了一个A任务<br><span class="hljs-number">2</span>、<span class="hljs-number">2</span>/<span class="hljs-number">9</span> &gt; <span class="hljs-number">0</span>，所以下一个启动一个B任务<br><span class="hljs-number">3</span>、<span class="hljs-number">2</span><span class="hljs-regexp">/9 &lt; 1/</span><span class="hljs-number">3</span>,A的权重还是比B小，所以还是启动一个A任务<br><span class="hljs-number">4</span>、<span class="hljs-number">4</span><span class="hljs-regexp">/9 &gt; 1/</span><span class="hljs-number">3</span>,这时候A比B的权重大了，启动一个B任务<br><span class="hljs-number">5</span>、<span class="hljs-number">4</span><span class="hljs-regexp">/9 &lt; 2/</span><span class="hljs-number">3</span>,这时候B的权重大了，启动一个A任务<br><span class="hljs-number">6</span>、<span class="hljs-number">6</span><span class="hljs-regexp">/9 = 2/</span><span class="hljs-number">3</span>,这时候已经没有办法分配新任务了<br>整理的逻辑尽可能的根据主要的权重资源去分析占比，然后轮流去分配<br></code></pre></td></tr></table></figure><blockquote><p><a href="https://zhuanlan.zhihu.com/p/40245793">https://zhuanlan.zhihu.com/p/40245793</a></p></blockquote><h2 id="公平调度策略"><a href="#公平调度策略" class="headerlink" title="公平调度策略"></a>公平调度策略</h2><p>这个调度策略旨在 队列 任务 容器 方面左到公平调度。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs">1、队列，资源分配的情况首先是队列之间的资源协调，每个平台上面可能会有多个队列，每个队列的使用情况可能不一样，所以如何选择队列去给这个队列分配资源就是在我们分配资源的第一步。<br>2、任务，这个是比队列更细的粒度。在队列里面有面临着多个任务的提交，这时候多个任务需要去在次队列里面去分配。<br>3、容器就是最小的资源粒度了。<br></code></pre></td></tr></table></figure><p>公平调度的策略是应用在上面三个方面的分配的。每个阶段都是遵循着公平调度策列的。下面讲一下该策略</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs tap">首先理解一下几个指标：<br>1、实际的最小资源份额：思考一下什么是一个队列和一个任务的实际的最小资源份额，每个队列和任务肯定是有自己需要的资源量的，拿一个任务来说，该任务所在的队列可能只有10 C<span class="hljs-number"> 40 </span>R,但是它需要的资源量是<span class="hljs-number"> 20 </span>C<span class="hljs-number"> 80 </span>R,计算这个任务的最小资源份额肯定是不能超过它所在的队列的，所以它最小的资源份额只能是10 C<span class="hljs-number"> 40 </span>R。<br>2、是否饥饿：如何判断这个任务是不是处于饥饿状态呢？还是继续用上面的例子，可能第一次分配得到的<span class="hljs-number"> 5 </span>C<span class="hljs-number"> 20 </span>R，但是该任务需要的最小资源是10 C<span class="hljs-number"> 40 </span>R，得到的资源没有满足最小的资源份额，可以判断这个任务就处于饥饿的状态。<br>3、资源分配比：相当于我实际使用资源跟我需要的资源对比，其实就是 资源的使用量/最小的资源份额。<br>4、资源使用权重比：资源使用量/权重<br></code></pre></td></tr></table></figure><p>具体的资源分配流程</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 1c">                        首先判断是否有队列/程序/container是否处于饥饿<br>                                               <span class="hljs-string">|</span><br>如果只有一个处于饥饿，那么肯定直接给这个分配资源 - - <span class="hljs-string">| - - 如果都不饥饿，根据资源使用权重比分配</span><br>                                               <span class="hljs-string">|</span><br>如果都处于饥饿，那么就按照资源的分配比来分配，资源分配比小，那就优先分配，相同的话就按照提交时间<br><br></code></pre></td></tr></table></figure><p>这就是公平调度的策略</p><p>##公平调度和容器调度<br>其实随着现在俩种调度器的互相借鉴，互补不足，俩种的功能越来越同质化。像hortonword默认使用的是容器调度，cloudera默认使用的是fair，目前高版本的hadoop默认使用的是capacity scheduler。像他们都是层级队列支持最大最小资源、资源抢占资源共享、不同的调度策略、支持优先级等。</p><p>容器调度：共享集群</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs">1、支持给节点打标签，支持计算和存储分离<br>2、不同队列通过占比的方式配置<br></code></pre></td></tr></table></figure><p>公平调度：公平调度</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs">1、队列内部支持FAIR（其实是最大最小化公平算法根据内存）的调度策略，容器调度里面没有<br>2、通过配置直接大小配置<br>3、限制每个用户的运行个数<br>4、支持为每个对垒设置不同的调度策略<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>yarn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hdfs数据在读写时候的可见性</title>
    <link href="/2022/06/08/hdfs%E6%95%B0%E6%8D%AE%E5%9C%A8%E8%AF%BB%E5%86%99%E6%97%B6%E5%80%99%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7/"/>
    <url>/2022/06/08/hdfs%E6%95%B0%E6%8D%AE%E5%9C%A8%E8%AF%BB%E5%86%99%E6%97%B6%E5%80%99%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="hdfs数据在读写时候的可见性"><a href="#hdfs数据在读写时候的可见性" class="headerlink" title="hdfs数据在读写时候的可见性"></a>hdfs数据在读写时候的可见性</h1><p>最近跑的好好的程序突然报错了，查了下原因了读取hdfs文件的是空文件，导致解析报错，但是报错之后查看文件的时候发现文件时有数据的，开始以为写文件没有关闭<br>连接导致的读文件的时候读不出数据来，后来才发现，确实时没有关闭连接，但是写客户端已经写完数据了，也执行了flush了，但是没有关闭连接。趁着这个机会研究<br>了一下hdfs在读写数据的可见性。</p><h2 id="跟随源码去看一下hdfs写文件的流程"><a href="#跟随源码去看一下hdfs写文件的流程" class="headerlink" title="跟随源码去看一下hdfs写文件的流程"></a>跟随源码去看一下hdfs写文件的流程</h2><p>我们写文件首先要获取DFSOutputStream,通过FileSystem.create()&#x2F;append()来获取。create的时候是创建文件，如果返回来了DFSOutputStream的话，<br>那么文件应该是读取完成了，看下源码（最终调用了DFSClient的create方法）：</p><figure class="highlight java"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> DFSOutputStream <span class="hljs-title function_">create</span><span class="hljs-params">(String src, </span><br><span class="hljs-params">                             FsPermission permission,</span><br><span class="hljs-params">                             EnumSet&lt;CreateFlag&gt; flag, </span><br><span class="hljs-params">                             <span class="hljs-type">boolean</span> createParent,</span><br><span class="hljs-params">                             <span class="hljs-type">short</span> replication,</span><br><span class="hljs-params">                             <span class="hljs-type">long</span> blockSize,</span><br><span class="hljs-params">                             Progressable progress,</span><br><span class="hljs-params">                             <span class="hljs-type">int</span> buffersize,</span><br><span class="hljs-params">                             ChecksumOpt checksumOpt,</span><br><span class="hljs-params">                             InetSocketAddress[] favoredNodes)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>                             <br>    <span class="hljs-comment">//这个是检查fileSystem是否已经被关闭，如果关闭了抛出异常</span><br>    checkOpen();<br>    <span class="hljs-comment">//给文件默认权限</span><br>    <span class="hljs-keyword">if</span> (permission == <span class="hljs-literal">null</span>) &#123;<br>      permission = FsPermission.getFileDefault();<br>    &#125;<br>    <span class="hljs-type">FsPermission</span> <span class="hljs-variable">masked</span> <span class="hljs-operator">=</span> permission.applyUMask(dfsClientConf.uMask);<br>    <span class="hljs-keyword">if</span>(LOG.isDebugEnabled()) &#123;<br>      LOG.debug(src + <span class="hljs-string">&quot;: masked=&quot;</span> + masked);<br>    &#125;<br>    <span class="hljs-comment">//这个步骤就是根据namenode创建文件，然后根据返回的HdfsFileStatus来创建输出流</span><br>    <span class="hljs-keyword">final</span> <span class="hljs-type">DFSOutputStream</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> DFSOutputStream.newStreamForCreate(<span class="hljs-built_in">this</span>,<br>        src, masked, flag, createParent, replication, blockSize, progress,<br>        buffersize, dfsClientConf.createChecksum(checksumOpt),<br>        getFavoredNodesStr(favoredNodes));<br>    beginFileLease(result.getFileId(), result);<br>    <span class="hljs-keyword">return</span> result;<br>  &#125;<br></code></pre></td></tr></table></figure><p>来看一下newStreamCreate</p><figure class="highlight d"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs d">TraceScope <span class="hljs-keyword">scope</span> = dfsClient.getPathTraceScope(<span class="hljs-string">&quot;newStreamForCreate&quot;</span>, src);<br>    <span class="hljs-keyword">try</span> &#123;<br>      HdfsFileStatus stat = <span class="hljs-literal">null</span>;<br>      boolean shouldRetry = <span class="hljs-literal">true</span>;<br>      <span class="hljs-keyword">int</span> retryCount = CREATE_RETRY_COUNT;<br>      <span class="hljs-keyword">while</span> (shouldRetry) &#123;<br>        shouldRetry = <span class="hljs-literal">false</span>;<br>        <span class="hljs-keyword">try</span> &#123;<br>          <span class="hljs-comment">//跟namenode rpc通信创建文件，然后返回HdfsFileStatus</span><br>          stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,<br>              <span class="hljs-keyword">new</span> EnumSetWritable&lt;CreateFlag&gt;(flag), createParent, replication,<br>              blockSize, SUPPORTED_CRYPTO_VERSIONS);<br>          <span class="hljs-keyword">break</span>;<br>        &#125; <span class="hljs-keyword">catch</span> (RemoteException re) &#123;<br>            ...<br>        &#125;<br>      &#125;<br>      Preconditions.checkNotNull(stat, <span class="hljs-string">&quot;HdfsFileStatus should not be null!&quot;</span>);<br>      <span class="hljs-comment">//根据HDFSFileStatus来创建输出流</span><br>      <span class="hljs-keyword">final</span> DFSOutputStream <span class="hljs-keyword">out</span> = <span class="hljs-keyword">new</span> DFSOutputStream(dfsClient, src, stat,<br>          flag, progress, checksum, favoredNodes);<br>      <span class="hljs-keyword">out</span>.start();<br>      <span class="hljs-keyword">return</span> <span class="hljs-keyword">out</span>;<br></code></pre></td></tr></table></figure><p>后边有个out.start(),启动了一个线程，这个线程是做什么的呢?</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">out.start<span class="hljs-literal">()</span>点进去其实调用了<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DataStreamer</span>.</span></span>start<span class="hljs-literal">()</span>,DataStreamer类继成了Daemon，Daemon继成了Thread，其实就是启动了一个线程，所以我们去看<br>run<span class="hljs-literal">()</span>方法。代码很长，下边英文翻译就是这个线程是真正把数据打到datanode上面的渠道，并且最终关闭和datanode连接，并且恢复也是这个线程做的：<br><span class="hljs-number">1</span>、等待队列中是否有数据，如果没有数据就一直等待，同时还会判断是否超时。<br> <span class="hljs-keyword">while</span> ((!streamerClosed<span class="hljs-operator"> &amp;&amp; </span>!hasError<span class="hljs-operator"> &amp;&amp; </span>dfsClient.clientRunning <br>    <span class="hljs-comment">//size 不等于 0的时候就会退出循环</span><span class="hljs-operator"></span><br><span class="hljs-operator">    &amp;&amp; </span>dataQueue.size<span class="hljs-literal">()</span><span class="hljs-operator"> == </span><span class="hljs-number">0</span><span class="hljs-operator"> &amp;&amp; </span><br><span class="hljs-operator">    </span>(stage != BlockConstructionStage.DATA_STREAMING<span class="hljs-operator"> || </span><br><span class="hljs-operator">     </span>stage<span class="hljs-operator"> == </span>BlockConstructionStage.DATA_STREAMING<span class="hljs-operator"> &amp;&amp; </span><br><span class="hljs-operator">     </span>now - lastPacket &lt; dfsClient.get<span class="hljs-constructor">Conf()</span>.socketTimeout/<span class="hljs-number">2</span>))<span class="hljs-operator"> || </span>doSleep ) &#123;<br>     <br>     <br>  long timeout = dfsClient.get<span class="hljs-constructor">Conf()</span>.socketTimeout/<span class="hljs-number">2</span> - (now-lastPacket);<br>  timeout = timeout &lt;= <span class="hljs-number">0</span> ? <span class="hljs-number">1000</span> : timeout;<br>  timeout = (stage<span class="hljs-operator"> == </span>BlockConstructionStage.DATA_STREAMING)?<br>     timeout : <span class="hljs-number">1000</span>;<br>  <span class="hljs-keyword">try</span> &#123;<br>    dataQueue.wait(timeout);<br>  &#125; catch (InterruptedException  e) &#123;<br>    <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>warn(<span class="hljs-string">&quot;Caught exception &quot;</span>, e);<br>  &#125;<br>  doSleep = <span class="hljs-literal">false</span>;<br>  now = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Time</span>.</span></span>monotonic<span class="hljs-constructor">Now()</span>;<br>&#125;<br><span class="hljs-number">2</span>、拿到要发送的pockage<br><span class="hljs-comment">//如果没有数据文件，就会发送心跳，与datanode保持连接</span><br><span class="hljs-keyword">if</span> (dataQueue.is<span class="hljs-constructor">Empty()</span>) &#123;<br>  one = create<span class="hljs-constructor">HeartbeatPacket()</span>;<br>  <span class="hljs-keyword">assert</span> one != null;<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>  <span class="hljs-comment">//拿到队列中的第一个数据包</span><br>  one = dataQueue.get<span class="hljs-constructor">First()</span>; <span class="hljs-comment">// regular data packet</span><br>  long parents<span class="hljs-literal">[]</span> = one.get<span class="hljs-constructor">TraceParents()</span>;<br>  <span class="hljs-keyword">if</span> (parents.length &gt; <span class="hljs-number">0</span>) &#123;<br>    scope = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Trace</span>.</span></span>start<span class="hljs-constructor">Span(<span class="hljs-string">&quot;dataStreamer&quot;</span>, <span class="hljs-params">new</span> TraceInfo(0, <span class="hljs-params">parents</span>[0])</span>);<br>  &#125;<br>&#125;<br><span class="hljs-number">3</span>、向namenode注册一个block，同时开启线程去接收从datanode返回的ack<br> <span class="hljs-keyword">if</span> (stage<span class="hljs-operator"> == </span>BlockConstructionStage.PIPELINE_SETUP_CREATE) &#123;<br>    <span class="hljs-comment">//针对创建文件的实现</span><br>    <span class="hljs-keyword">if</span>(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>is<span class="hljs-constructor">DebugEnabled()</span>) &#123;<br>      <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>debug(<span class="hljs-string">&quot;Allocating new block&quot;</span>);<br>    &#125;<br>    set<span class="hljs-constructor">Pipeline(<span class="hljs-params">nextBlockOutputStream</span>()</span>);<br>    init<span class="hljs-constructor">DataStreaming()</span>;<br>  &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (stage<span class="hljs-operator"> == </span>BlockConstructionStage.PIPELINE_SETUP_APPEND) &#123;<br>    <span class="hljs-comment">//针对append文件的实现</span><br>    <span class="hljs-keyword">if</span>(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>is<span class="hljs-constructor">DebugEnabled()</span>) &#123;<br>      <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>debug(<span class="hljs-string">&quot;Append to block &quot;</span> + block);<br>    &#125;<br>    setup<span class="hljs-constructor">PipelineForAppendOrRecovery()</span>;<br>    init<span class="hljs-constructor">DataStreaming()</span>;<br>  &#125;<br> <span class="hljs-number">4</span>、等待所有已经发送的pakcage ack已经收到<br>  <span class="hljs-keyword">if</span> (one.is<span class="hljs-constructor">LastPacketInBlock()</span>) &#123;<br>    <span class="hljs-comment">// wait for all data packets have been successfully acked</span><br>    synchronized (dataQueue) &#123;<br>      <span class="hljs-keyword">while</span> (!streamerClosed<span class="hljs-operator"> &amp;&amp; </span>!hasError<span class="hljs-operator"> &amp;&amp; </span><br><span class="hljs-operator">          </span>ackQueue.size<span class="hljs-literal">()</span> != <span class="hljs-number">0</span><span class="hljs-operator"> &amp;&amp; </span>dfsClient.clientRunning) &#123;<br>        <span class="hljs-keyword">try</span> &#123;<br>          <span class="hljs-comment">// wait for acks to arrive from datanodes</span><br>          dataQueue.wait(<span class="hljs-number">1000</span>);<br>        &#125; catch (InterruptedException  e) &#123;<br>          <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>warn(<span class="hljs-string">&quot;Caught exception &quot;</span>, e);<br>        &#125;<br>      &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> (streamerClosed<span class="hljs-operator"> || </span>hasError<span class="hljs-operator"> || </span>!dfsClient.clientRunning) &#123;<br>      continue;<br>    &#125;<br>    stage = BlockConstructionStage.PIPELINE_CLOSE;<br>  &#125;<br><span class="hljs-number">5</span>、发送package<br>  Span span = null;<br>  synchronized (dataQueue) &#123;<br>    <span class="hljs-comment">// 发送钱需要从队列中移除要发送的package</span><br>    <span class="hljs-keyword">if</span> (!one.is<span class="hljs-constructor">HeartbeatPacket()</span>) &#123;<br>      span = scope.detach<span class="hljs-literal">()</span>;<br>      one.set<span class="hljs-constructor">TraceSpan(<span class="hljs-params">span</span>)</span>;<br>      dataQueue.remove<span class="hljs-constructor">First()</span>;<br>      ackQueue.add<span class="hljs-constructor">Last(<span class="hljs-params">one</span>)</span>;<br>      dataQueue.notify<span class="hljs-constructor">All()</span>;<br>    &#125;<br>  &#125;<br><br>  <span class="hljs-keyword">if</span> (<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>is<span class="hljs-constructor">DebugEnabled()</span>) &#123;<br>    <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DFSClient</span>.</span><span class="hljs-module"><span class="hljs-identifier">LOG</span>.</span></span>debug(<span class="hljs-string">&quot;DataStreamer block &quot;</span> + block +<br>        <span class="hljs-string">&quot; sending packet &quot;</span> + one);<br>  &#125;<br><br>  <span class="hljs-comment">//发送数据到datanode</span><br>  TraceScope writeScope = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Trace</span>.</span></span>start<span class="hljs-constructor">Span(<span class="hljs-string">&quot;writeTo&quot;</span>, <span class="hljs-params">span</span>)</span>;<br><span class="hljs-number">6</span>、更新已经发送的统计信息<br><span class="hljs-number">7</span>、判断block是否已经写满<br></code></pre></td></tr></table></figure><p>上面基本上就是OutputStream线程做的内容，总结起来就是扫描queue向datanode发送数据，同时接收datanode返回的ack。</p><p>我们都知道block一般都是3副本的，针对副本的操作都是datanode接收到数据之后，由datanode去按照pipline去传递的，就不在由客户端发送。</p><blockquote><p><a href="https://jxy.me/2015/06/09/hdfs-data-visibility/">https://jxy.me/2015/06/09/hdfs-data-visibility/</a> hdfs文件可见性</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2pc paxos zab</title>
    <link href="/2022/05/08/2pc%20paxos%20zab/"/>
    <url>/2022/05/08/2pc%20paxos%20zab/</url>
    
    <content type="html"><![CDATA[<h1 id="2pc-paxos-zab"><a href="#2pc-paxos-zab" class="headerlink" title="2pc paxos zab"></a>2pc paxos zab</h1><h2 id="2pc"><a href="#2pc" class="headerlink" title="2pc"></a>2pc</h2><p>俩阶段提交协议，主要解决分布式事务一致性算法。</p><ul><li>2：俩阶段</li><li>p：准备阶段</li><li>c：提交阶段</li></ul><p>应用场景：关系型数据库事务实现、flink的checkpoint机制</p><p>2PC算法思路可以概括：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否需要操作还是中止事务。主要用来解决分布式事务的原子性问题，其中2阶段分为：准备阶段和提交阶段。</p><p>第一阶段：准备阶段<br>事务协调者给每个参与者发送prepare消息，参与者有俩中情况可以返回，第一种可以直接返回失败；第二种是在本地执行事务，不提交，饭后返回成功。</p><p>第二阶段：提交阶段<br>协调者收到参与者返回之后的消息，根据参与者返回的消息，确认本次操作每个参与者是提交还是中止</p><h2 id="paxos"><a href="#paxos" class="headerlink" title="paxos"></a>paxos</h2><p>paxos是莱斯利兰伯特发明的一种分布式共识算法，是第一个有完整数学证明的共识算法。</p><p>理解：paxos算法允许系统在出现网络分区，只要其中一个分区的节点个数保持过半，对外就能保证可用以及数据的最终一致性（最终所有节点拥有相同的更新），解决单点失败故障的同时，尽可能减少对吞吐量和延迟的影响。</p><p>应用场景：选主、replication一致性</p><p>proposer:提出者<br>acceptor:决策者<br>learner:学习者</p><p>paxos类似于俩阶段提交，主要也是分为俩个阶段：prepare和accept，但是其实能算上一个阶段是增加了一个学习阶段</p><p>第一阶段：propare阶段<br>proposer提出一个编号为N的提议并发给半数以上的acceptor，acceptor收到提议后有俩种情况。</p><ol><li>acceptor没有接受过提案，直接接受该编号N</li><li>acceptor接受过提案N&#96;，那么就先判断编号跟N的大小关系：<ul><li>N&#96; &gt; N 直接回复error</li><li>N&#96; &lt; N 这时候还会有俩中情况，因为不确实是否已经确认accept已经accept某值<ul><li>如果有，那么吧这个值返回给proposer</li><li>如果没有，直接返回promise</li></ul></li></ul></li></ol><p>第二阶段：接受阶段</p><ol><li>proposer:如果accept返回promise的消息里面没有任何value，那么就自己确定value去给超过半数的accept发送accept</li><li>proposer:如果accept返回promise的消息里面有value，那么就取返回消息种最大编号N的value值作为本次accept发送的请求的value</li><li>acceptor:只要没有收到过比proposer编号大的请求，那么就接受本次的请求</li><li>acceptor:如果收到过比本次请求还打的编码，那么就会拒接本次请求</li></ol><p>第三阶段：学习阶段<br>只要某个提议本超过半数的acceptor接受，那么proposer就会发送消息到learner进行学习。</p><h2 id="zab"><a href="#zab" class="headerlink" title="zab"></a>zab</h2><ul><li>z:zookeeper</li><li>a:atomic原子</li><li>b:broadcast广播</li></ul><p>zab是zookeeper专门设计的一种奔溃回复的原子广播协议，是保证zookeeper一致性的核心算法。</p><p>应用场景：负载均衡、协调通知、集群管理、选举、分布式锁、分布式队列</p><ul><li>消息广播</li></ul><ol><li>客户端发送请求到zk集群，如果该节点是不是leader，那么该follower就会把请求转发给leader。</li><li>leader收到请求之后会进行广播该请求，吧每个请求封装成一个事务，并分配一个zxid</li><li>leader内部会为每个follower分配一个单独的队列，主要用来保证事务的顺序，然后吧本次事务放到队列里面</li><li>follower收到leader发送的事务之后会把事务以日志的形式写入到磁盘，写入成功之后给leader发送ack响应</li><li>leader收到半数的ack响应之后就会发送一个commit请求给所有的follower，follower收到之后对本地的事务进行提交</li><li>leader上面也进行事务的提交</li></ol><ul><li>崩溃恢复：选举leader 和 数据同步</li></ul><ol><li>检测节点处于looking阶段，开始选举leader</li><li>如果是新启动的集群那么每个节点都会投票给自己，初始值zxid为0，epoch是本地配置的节点id；如果是运行期间那么每个节点上面的zxid不再为0，发送自己已经到达的zxid</li><li>最终获取半数以上同意有最新事务的作为本次选取的leader</li><li>最后进行数据同步，有四种数据同步策略：差异化同步、先回滚再差异化同步、回滚同步、全量同步</li></ol><h2 id="paxos-和-zab"><a href="#paxos-和-zab" class="headerlink" title="paxos 和 zab"></a>paxos 和 zab</h2><ol><li>paxos算法是用于构建一个分布式一致性状态机的，而zab主要用于构建一个分布式的主备系统</li><li>paxos是一种通用的分布式共识算法，zab协议是专门为zk原子广播和崩溃回复设计的一种协议</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>flink的exactly-once</title>
    <link href="/2022/03/22/flink%E7%9A%84exactly-once/"/>
    <url>/2022/03/22/flink%E7%9A%84exactly-once/</url>
    
    <content type="html"><![CDATA[<h1 id="Flink端到端的精确一次处理语义"><a href="#Flink端到端的精确一次处理语义" class="headerlink" title="Flink端到端的精确一次处理语义"></a>Flink端到端的精确一次处理语义</h1><h2 id="端到端精确一次处理语义EOS"><a href="#端到端精确一次处理语义EOS" class="headerlink" title="端到端精确一次处理语义EOS"></a>端到端精确一次处理语义EOS</h2><p>一个完整的应用包括source、flink、sink三部分，如果要做到端到端的精确一次处理，其实主要是保证最终sink端的数据不会缺少，也不会重复。之前的步骤主要是为了保证sink端的输出的。针对于source端，只需要保证数据的可重复读取就行，像kafka我们可以保存offset来进行数据的重复读取，这样有一些已经读取的数据就算处理失败了，也还可以进行重复读取。flink端的话主要是针对数据的处理，主要是能够记录下当时处理的状态，就算处理失败了也可以对数据重新按照历史的状态重新处理，而sink端需要支持一些事务或者一些回滚的操作，如果数据失败了，直接进行回滚到某一个时间段的状态，然后从新处理数据就可以。</p><h2 id="2PC-俩阶段提交协议"><a href="#2PC-俩阶段提交协议" class="headerlink" title="2PC 俩阶段提交协议"></a>2PC 俩阶段提交协议</h2><p>俩阶段提交协议(Two-phase Commit,2PC)是很常用的解决分布式事务问题的方式，它可以保证在分布式事务中，要么参与进程都提交事务，要么都取消，即实现ACID 中的A原子性。</p><p>在数据一致的环境下，其代表含义是：要么所有备份数据同时更改某个值，要么都不改，以此来达到强一致性。</p><p>俩段提交中有俩个重要的角色，协调者和参与者，其协调者只能有一个，起到分布式事务中的协调管理作用，参与者多个。</p><h3 id="第一阶段：表决阶段"><a href="#第一阶段：表决阶段" class="headerlink" title="第一阶段：表决阶段"></a>第一阶段：表决阶段</h3><p>1、协调者向所有参与者发送一个vote request的消息。</p><p>2、参与者接收到vote request消息，向协调者发送vote commit消息作为回应，告诉协调者自己已经做好提交准备，如果参与者没有准备好或遇到其他故障，就<br>返回一个vote abort消息，告诉协调者目前无法提交事务</p><h3 id="第二阶段：提交阶段"><a href="#第二阶段：提交阶段" class="headerlink" title="第二阶段：提交阶段"></a>第二阶段：提交阶段</h3><p>1、协调者收集到了各个参与者的表决消息，如果所有参与者一致认为可以提交事务，那么协调者决定事务的最终提交，在此情形下协调者向所有参与者发送要给global commit消息，通知参与者进行本地提交，如果所有参与者中有任意一个返回消息是vote abort，协调者就会取消事务，向所有参与者广播一条global abort的消息 通知所有参与者取消事务。</p><p>2、每个提交了表决信息的参与者等候协调者返回消息，如果参与者接受到一个global commit的消息，那就参与提交本地事务，否则收到global abort就取消</p><h2 id="2PC-flink中应用"><a href="#2PC-flink中应用" class="headerlink" title="2PC flink中应用"></a>2PC flink中应用</h2><p>1、当checkpoint启动时，jobmanager会将检查分界线（checkpoint barrier）注入到数据流中，checkpoint barrier会在算子间传递下去：</p><figure class="highlight markdown"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><pre><code class="hljs markdown">pre-commit  <br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><br><span class="hljs-code">            jobmanager</span><br><span class="hljs-code">                |</span><br><span class="hljs-code">                | 注入 checkpoint barrier</span><br><span class="hljs-code">                ⬇</span><br><span class="hljs-code">kafka =&gt;   | datasource  =&gt; window =&gt;  datasink  |   =&gt; kafka</span><br><span class="hljs-code">外部系统      fink source   operator   flink sink       外部系统</span><br></code></pre></td></tr></table></figure><p>2、flink kafka source负责保存kafka消费的offset，当checkpoint成功时flink负责提交这些写入，否则就终止取消他们。当checkpoint完成位移保存，它会将checkpoint barrier传递给下一个operator，然后每个算子会对当前状态做个快照，保存状态到后端:   </p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs coq">            jobmanager<br>                |<br>                <span class="hljs-type">| 注入barrier</span><br>                ⬇<br>kafka =&gt;   | <span class="hljs-type">datasource</span>  =&gt; window =&gt;  datasink  |   <span class="hljs-type">=&gt; kafka</span><br>外部系统      fink source   operator   flink sink       外部系统<br>                |    <span class="hljs-type">↘         ↙</span><br><span class="hljs-type">                |       ↘   ↙</span><br><span class="hljs-type">                ⬇     传递barrier</span><br>          保存offset快照<br>         （state backend）<br></code></pre></td></tr></table></figure><p>3、flink处理完就到了sink端，每个内部的transform任务遇到checkpoint barrier时，都会把状态保存到checkpoint里面。数据处理到sink端的时候，sink首先把数据写入到外部的kafka， 这些数据都是预提交。   </p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">     jobmanager<br>         ↘         传递barrier 传递barrier<br>注入barrier ↘           ↗  ↖      ↗ ↖<br>              ↘      ↗       ↖ ↗       ↖<br>kafka =&gt;   |<span class="hljs-string"> datasource  =&gt; window =&gt;  datasink  </span>|<span class="hljs-string">   =&gt; kafka</span><br><span class="hljs-string">外部系统      fink source   operator   flink sink       外部系统</span><br><span class="hljs-string">                </span>|<span class="hljs-string">            </span>|<span class="hljs-string">            </span>|<br>                |<span class="hljs-string">            </span>|<span class="hljs-string">            </span>|<br>                ⬇            ⬇            ⬇<br>            offset快照    offset快照    offset快照<br>          ===============state backend===============<br></code></pre></td></tr></table></figure><blockquote><p>此时pre commit预提交阶段下sink在保存状态到状态后端的同时还需要提交他们的外部事务</p></blockquote><p>4、当所有算子任务快照完成，也就是这次的checkpoint完成时，jobmanager会向所有任务发送通知，确认这次checkpoint完成，此时pre commit才算完成。 然后就进入了第二个阶段：commit，该阶段JobManager会为应用中每个operator发起checkpoint已完成的回调函数。   </p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">                         jobmanager<br>                           ↙ | ↘ <br>                         ↙   |    ↘  通知<span class="hljs-keyword">checkpoint</span>完成<br>                      ↙      ⬇       ↘<br>kafka =&gt;   | datasource  =&gt; <span class="hljs-keyword">window</span> =&gt;  datasink  |   =&gt; kafka<br>外部系统      fink source   <span class="hljs-keyword">operator</span>   flink sink       外部系统<br>                                            ↘             ↗<br>                                                ↘       ↗<br>                                                    ↘ ↗<br>                                                提交外部事务<br></code></pre></td></tr></table></figure><blockquote><p>flink的jobmanager协调各个TaskManager进行checkout存储，checkpoint保存在stateBackend中</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="预提交"><a href="#预提交" class="headerlink" title="预提交"></a>预提交</h3><p>Flink消费到kafka的数据之后，就会开启一个kafka的事务，正常写入kafka的数据都是未提交阶段。</p><h3 id="commit"><a href="#commit" class="headerlink" title="commit"></a>commit</h3><p>如果所有的operator完成各自的pre commit，他们会发送一个commit操作</p><h3 id="pre-commit失败"><a href="#pre-commit失败" class="headerlink" title="pre commit失败"></a>pre commit失败</h3><p>如果任何一个pre commit失败，所有的pre commit都会终止，flink也会回滚到上一个checkpoint</p><h3 id="pre-commit-finish"><a href="#pre-commit-finish" class="headerlink" title="pre commit finish"></a>pre commit finish</h3><p>当所有的operator完成，sink就会收到checkpoint barrier，sink保存当前状态，存入到checkpoint里面，通知jobmanater，并提交外部事务，用于提交<br>外部检查点的数据</p><h3 id="tow-commit"><a href="#tow-commit" class="headerlink" title="tow commit"></a>tow commit</h3><p>JobManager收到所有任务的通知，发出确认消息，表示checkpoint已经完成，sink收到jobmanager的确认消息，最终commit这段数据。</p><h3 id="close"><a href="#close" class="headerlink" title="close"></a>close</h3><p>外部系统kafka关闭事务，提交的数据可以正常消费。</p><blockquote><p>一旦pre commit提交完成，必须要确保commit也要成功，operatr和外部系统都需要对此进行保证，如果commit失败，flink就会崩溃，然后根据用户的重启 策略进行重启，之后重试commit，这个过程非常重要，因为commit无法成功执行，就可能出现丢数据的情况，因此所有operator必须对checkpoint最终执行结果达成共识（所有的operator都必须确认数据要么全部执行成功，要么终止进行回滚）</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>flink</category>
      
    </categories>
    
    
    <tags>
      
      <tag>flink 2PC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>flink的checkpint机制学习</title>
    <link href="/2022/03/14/flink%E7%9A%84checkpint%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/03/14/flink%E7%9A%84checkpint%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="flink分布式快照-Checkpoint"><a href="#flink分布式快照-Checkpoint" class="headerlink" title="flink分布式快照(Checkpoint)"></a>flink分布式快照(Checkpoint)</h1><p>实现精确一次处理语义有俩种方式：1、分布式快照&#x2F;状态检查点 2、至少一次事件传递以及重复数据删除机制</p><blockquote><p>Exactly-Once并不是字面意思，一条数据被精确的处理一次，而是保证sink端的数据只执行了一次完成的事务。说白了就是有效一次：可以保证引擎 管理的状态只提交一次到持久化后端</p></blockquote><h2 id="Chandy-Lamport-算法"><a href="#Chandy-Lamport-算法" class="headerlink" title="Chandy-Lamport 算法"></a>Chandy-Lamport 算法</h2><p>核心思想：通过每个节点记录自己状态和于自己相关的状态来合并出全局状态。</p><p>算法的应用条件</p><ul><li>节点之间是一个有向图</li><li>数据有序</li></ul><p>大概流程：因为是有向图，所以节点之间存在输入&#x2F;输出链路，通过一个maker来进行新老「有序」 数据的区分，来做某一个时间点的全局快照。某个节点触发snapshot这时候会生成一个maker，接下来的数据收到的数据也会记录下来，然后把maker传递下去，其他下游节点收到maker之后首先snapshot自己当前的状态，然后传递maker到下游。在传递maker之后，记录下自己在maker之后收到的信息。整个snapshot结束的标志是所有的节点都记录下了自己的snapshot同时也记录之后来的消息（其实就是于自己相关的节点的状态）整个snapshot结束。</p><p>看了许多博客上面都用「至少一次事件事件传递以及重复数据删除」这个机制跟分布式快照做对比，理解一下这个机制，我个人理解其实就是字面意思：保证数据至少进行一次完整都处理，如果一个事件处理多次，需要把重复生成都数据删除。</p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h3><figure class="highlight applescript"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><pre><code class="hljs applescript">暂停处理新流入数据，将数据缓存起来<br>     <span class="hljs-comment">--&gt;  </span><br>     将算子子任务的本地状态<span class="hljs-keyword">copy</span>到一个远程持久化存储上面<br>          <span class="hljs-comment">--&gt;</span><br>          继续处理新流入的数据，包括刚才缓存的数据<br></code></pre></td></tr></table></figure><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h4 id="checkpoint-barrier"><a href="#checkpoint-barrier" class="headerlink" title="checkpoint barrier"></a>checkpoint barrier</h4><p>checkpoint barrier检查分界点：就是插入到数据流中的一个标志，不会影响正常数据流的状态，每个barrier都是由checkpoint 的id，这样可以区分每次 checkpoint的范围。</p><p>可以看下面的图理解一下上面对于检查分界点的含义</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">        (checkpint barrier） (barrier)<br>                 |<span class="hljs-string">            </span>|<br>          cp n+1 |<span class="hljs-string">    cp n    </span>|<span class="hljs-string"> cp n-1</span><br><span class="hljs-string">数据元素：   □ □ □ </span>|<span class="hljs-string">   □ □ □ □  </span>|<span class="hljs-string"> □ □</span><br><span class="hljs-string">         ---------------------------&gt;</span><br><span class="hljs-string">         新数据                 老数据 </span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><h4 id="checkpoint-coordinator"><a href="#checkpoint-coordinator" class="headerlink" title="checkpoint coordinator"></a>checkpoint coordinator</h4><p>负责触发checkpoint，收集各个子任务的checkpoint情况，以及2PC的协调者。本身也是ha的。</p><h4 id="State-Backend"><a href="#State-Backend" class="headerlink" title="State Backend"></a>State Backend</h4><p>负责持久化存储快照，是一个组件，通过不同的实现来进行快照和恢复。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">memoryStateBackend:<br>    基于内存的分布式快照。不能存储过大的数据，只适用于测试不适用于生产，默认的持久化策略<br>    env.set<span class="hljs-constructor">StateBackend(<span class="hljs-params">new</span> MemoryStateBackend(MAX_MEM_STATE_SIZE)</span>)<br>FsStateBackend:<br>    fs都知道，文件系统，这个就是把快站持久化到第三方的文件系统上（hdfs s3等）。支持异步快照<br>    env.set<span class="hljs-constructor">StateBackend(<span class="hljs-params">new</span> FsStateBackend(<span class="hljs-string">&quot;hdfs://namenode:port/flink-checkpoints/job1/&quot;</span>)</span>)<br>    flink的状态本地仍然存在，可以保证快读读写，也能保证大容量故障恢复<br>RockDBStateBackend:<br>    rockbd是一个k-v数据库，面对更大的状态快照存储，rockdb支持更大容量的状态存储。<br>    env.set<span class="hljs-constructor">StateBackend(<span class="hljs-params">new</span> RocksDBStateBackend(<span class="hljs-params">checkpointPath</span>, <span class="hljs-params">enableIncrementalCheckpointing</span>)</span>)<br>    该持久化策略支持增量快照，只针对发生变化的状态，这样可以明显的降低快照时间。代价就是重启恢复更长。<br><br><br></code></pre></td></tr></table></figure><h3 id="详细流程"><a href="#详细流程" class="headerlink" title="详细流程"></a>详细流程</h3><p>首先，checkpoint coordinator触发一次checkpoint，发送给各个子任务</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs lasso">checkpoint<span class="hljs-params">-coordinator</span><br>   👇         👇<br>   👇         👇 triger checkpoint<br>   👇         👇<br>source1     source2<br>   |          |<br>opeartor    operator<br><br>     <span class="hljs-params">...</span><span class="hljs-params">...</span><span class="hljs-params">...</span>.<br>     <span class="hljs-params">...</span><span class="hljs-params">...</span><span class="hljs-params">...</span><br>   <br>sink   sink   sink<br></code></pre></td></tr></table></figure><p>各个source子任务收到cc请求之后，会将自己的状态写入到state backend，完成一次快照，然后向cc汇报自己已经快照完成，完成之后向下游广播barrier</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs pf">       cc<br>       ↑↓<br><span class="hljs-number">3</span> ack  ↑↓ <span class="hljs-number">1</span>、发送checkpoint消息<br>       ↑↓<br>     source -&gt; <span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span> -&gt; <span class="hljs-number">1</span>,<span class="hljs-number">3</span> ... -&gt; sink<br>        ↘ <span class="hljs-number">2</span>、source收到之后会像<span class="hljs-keyword">state</span> backend写入自己的状态<br>        <span class="hljs-keyword">state</span> backend<br><br><br></code></pre></td></tr></table></figure><p>下游算子可能有很多个，source把cp id广播给了所有的下游算子。下游所有的算子里面都会有cp id，但是算子的处理进度也可能会不同，所以就需要进行对其操作接下来我们看下如何进行数据对齐：</p><p>用俩个barrier来举例，一个barrier已经到达，另外一个还未到达</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs coq">| <span class="hljs-type">代表barrier</span><br><br>barrier已经到达<br><span class="hljs-number">1</span>,<span class="hljs-number">2</span> |<span class="hljs-type">↘ </span><br><span class="hljs-type">       ↘</span><br><span class="hljs-type">      operator</span> --&gt; .... --&gt; sink   <br>       ↗<br>|<span class="hljs-type">a</span>,b ↗ <br>barrier未到达<br><br></code></pre></td></tr></table></figure><p>这时候就需要进行对齐操作,如何对齐呢，上边barrier已经到达了，所以此时在barrier之后来电1 2需要在内存中缓存下来，下边的barrier在a、b之后，所以可以<br>继续向后传播：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs xl"><br><span class="hljs-number">1</span>  ↘ <br>      ↘<br>     （缓存：<span class="hljs-number">2</span>）<br>     <span class="hljs-function"><span class="hljs-title">operator</span> --&gt;</span> <span class="hljs-function"><span class="hljs-title">b</span> .... --&gt;</span> sink   <br>      ↗<br> |a ↗ <br><br></code></pre></td></tr></table></figure><p>对齐之后,上一阶段的checkpoint数据都已经流入到下一个算子：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs xl"><span class="hljs-number">1</span>  ↘ <br>      ↘<br>     （缓存：<span class="hljs-number">2</span>,<span class="hljs-number">1</span>）<br>     <span class="hljs-function"><span class="hljs-title">operator</span> --&gt;</span> | <span class="hljs-function"><span class="hljs-title">a</span> b .... --&gt;</span> sink   <br>      ↗           👇<br> c ↗           barrier传递<br></code></pre></td></tr></table></figure><p>对齐之后该算子就可以执行快照了，把状态存入到state backend里面去，一层一层传递下去：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pf"><span class="hljs-number">1</span>  ↘ <br>      ↘<br>     （缓存：<span class="hljs-number">2</span>,<span class="hljs-number">1</span>）<br>     operator --&gt; | a b .... --&gt; sink   <br>      ↗ 👇        👇<br> c ↗    👇     barrier传递<br>   <span class="hljs-keyword">state</span> backend<br></code></pre></td></tr></table></figure><blockquote><p>对齐操作：<br>1、当前算子进入了第一个cpid&#x3D;n的barrier，但是其他barrier还没有到<br>2、这时候通道barrier已经到了的数据还会接着来数据，这时候的数据会缓存到内存中，直到所有的barrier到齐<br>3、如果所有barrier都到了，那么就开始执行快照操作<br>4、快照完成，处理缓存中的数据，同时也处理新到的数据</p></blockquote><p>如果所有的sink都完成快照之后，证明cpid&#x3D;n该checkpoint执行结束，这时候cc会向state backend写入本次checkpoint的元数据，结束</p><h3 id="快照如何优化"><a href="#快照如何优化" class="headerlink" title="快照如何优化"></a>快照如何优化</h3><p>思考一下，上面的快照有什么问题？</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs">1、由于每次快照操作都会暂停处理新数据，如果快照事件比较长的话造成效率不高。<br>2、对齐操作，每个通道的数据不一样，这样就会造成数据堵塞<br></code></pre></td></tr></table></figure><h4 id="异步快照"><a href="#异步快照" class="headerlink" title="异步快照"></a>异步快照</h4><p>主要是为了解决第一个问题，快照时间的问题，只要所有barrier到达，barrier就立刻向下传播并且开始处理新数据，快照启动一个异步线程去执行，不会造成停顿。   </p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pf">barrier到达 --&gt; 立刻传播<br>     👇<br>同时启动一个线程进行异步操作：<br>该线程负责向<span class="hljs-keyword">state</span> backend上面copy数据，copy完成也是该线程向cc发送确认消息。<br></code></pre></td></tr></table></figure><p>由于状态可能在持续的变化中，后续的算子也在运行，如何保证该阶段的内存数据呢，最简单的就是向下传递的时候对当时的状态复制一份，但是这样又比较占用空间， flink使用的是 copy on wirte的优化策略：如果这份内存数据没有变化，那就没必要复制一份数据，如果内存中的数据变化，那么再申请内存维护俩分数据：快照和更新后的数据</p><h4 id="跳过对齐"><a href="#跳过对齐" class="headerlink" title="跳过对齐"></a>跳过对齐</h4><p>跳过对齐就是允许barrier没全部到达就允许barrier向下传播，但是为了保证数据的一致性，需要把较慢的元素一起快照，如果发生故障，这些数据也会重新处理</p><h2 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h2><p>flink的checkpoint都封装到了CheckpointConfig里面</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile">val cpConfig: CheckpointConfig = <span class="hljs-keyword">env</span>.getCheckpointConfig<br></code></pre></td></tr></table></figure><p>使用至少一次语义</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">env<span class="hljs-selector-class">.getCheckpointConfig</span><span class="hljs-selector-class">.setCheckpointingMode</span>(CheckpointingMode.AT_LEAST_ONCE)<br></code></pre></td></tr></table></figure><p>超时关闭checkpoint</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">env</span>.getCheckpointConfig.setCheckpointTimeout(<span class="hljs-number">3600</span>*<span class="hljs-number">1000</span>)<br></code></pre></td></tr></table></figure><p>checkpoint间隔</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">env</span>.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="hljs-number">60</span>*<span class="hljs-number">1000</span>)<br></code></pre></td></tr></table></figure><p>一起进行的checkpoint数量</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">env<span class="hljs-selector-class">.getCheckpointConfig</span><span class="hljs-selector-class">.setMaxConcurrentCheckpoints</span>(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>主动取消恢复(默认是故障恢复)</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">env<span class="hljs-selector-class">.getCheckpointConfig</span><span class="hljs-selector-class">.enableExternalizedCheckpoints</span>(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)<br>DELETE_ON_CANCELLATION:cancel job时候，删除检查点<br></code></pre></td></tr></table></figure><p>checkpoint失败是否需要重启</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">env.getCheckpointConfig.set<span class="hljs-constructor">FailOnCheckpointingErrors(<span class="hljs-params">false</span>)</span>z<br></code></pre></td></tr></table></figure><p>修改state backend</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">env.set<span class="hljs-constructor">StateBackend(<span class="hljs-params">new</span> FsStateBackend(<span class="hljs-string">&quot;hdfs://namenode:9000/flink/checkpoints&quot;</span>)</span>);<br><span class="hljs-keyword">new</span> <span class="hljs-constructor">MemoryStateBackend()</span>;<br><span class="hljs-keyword">new</span> <span class="hljs-constructor">RocksDBStateBackend(<span class="hljs-params">filebackend</span>, <span class="hljs-params">true</span>)</span>;<span class="hljs-comment">//需要添加第三方依赖</span><br></code></pre></td></tr></table></figure><p>修改默认state backend:flink-conf.yaml</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pf"><span class="hljs-keyword">state</span>.backend: filesystem<br><span class="hljs-keyword">state</span>.checkpoints.dir: hdfs://namenode:<span class="hljs-number">9000</span>/flink/checkpoints<br><span class="hljs-keyword">state</span>.backend的值可以是下面几种：jobmanager(MemoryStateBackend), filesystem(FsStateBackend), rocksdb(RocksDBStateBackend)<br></code></pre></td></tr></table></figure><p>失败延迟重启</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">val env = ExecutionEnvironment<span class="hljs-selector-class">.getExecutionEnvironment</span>()<br>env<span class="hljs-selector-class">.setRestartStrategy</span>(RestartStrategies<span class="hljs-selector-class">.fixedDelayRestart</span>(<br>  <span class="hljs-number">3</span>, <span class="hljs-comment">// 重启次数</span><br>  Time<span class="hljs-selector-class">.of</span>(<span class="hljs-number">10</span>, TimeUnit.SECONDS) <span class="hljs-comment">// 延迟时间间隔</span><br>))<br></code></pre></td></tr></table></figure><p>失败率重启</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">val env = ExecutionEnvironment<span class="hljs-selector-class">.getExecutionEnvironment</span>()<br>env<span class="hljs-selector-class">.setRestartStrategy</span>(RestartStrategies<span class="hljs-selector-class">.fixedDelayRestart</span>(<br>  <span class="hljs-number">3</span>, <span class="hljs-comment">// 重启次数</span><br>  Time<span class="hljs-selector-class">.of</span>(<span class="hljs-number">10</span>, TimeUnit.SECONDS) <span class="hljs-comment">// 延迟时间间隔</span><br>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>flink</category>
      
    </categories>
    
    
    <tags>
      
      <tag>flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hash算法和一致性hash算法</title>
    <link href="/2022/03/10/hash%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/"/>
    <url>/2022/03/10/hash%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="hash算法和一致性hash算法"><a href="#hash算法和一致性hash算法" class="headerlink" title="hash算法和一致性hash算法"></a>hash算法和一致性hash算法</h1><p>hash表就是我们经常使用的map结构，给定一个key值，通过映射规则找到一个表中的位置来访问该元素，这样查找速度很快。映射规则就是散列函数，存放数据的叫<br>散列表。最常用就是key进行hash取值，然后对表的长度进行取余。最后得到的就是散列表的位置。</p><h2 id="HASH"><a href="#HASH" class="headerlink" title="HASH"></a>HASH</h2><p>Hash就是散列，给定任意长度的输入通过散列算法，变成固定长度的输出，得到散列值。这种转换其实是一个压缩映射，散列值的空间通常会远小于输入的空间，不同<br>的输入可能会散列成相同的输出。常用于信息安全领域中的加密算法。</p><p>我们使用数组：寻址容易，插入删除比较复杂。链表：插入删除简单，寻址困难。如果我们像实现一个查找和插入性能都比较好的数据结构呢？hash表就使这样一种算法。</p><p>散列表通常是一个数组，数组里面使保存着一个指针，指针指向一个链表结构。这个链表就是为了保存相同hash值的输入。我们插入一个元素，首先对这个元素求散列<br>值，这个散列值就对应数组里面的index，通过index可以找到对应的链表，把这个元素放入到这个链表里面。</p><p>散列值就是hash函数实现，求出这个散列值如何映射到数组中的index，下面介绍几种常用的方法</p><ol><li>除数散列法:这种就是我们熟知的 index &#x3D; hash值 % list.length</li><li>平方散列法：index &#x3D; hash值 * hash值 &gt;&gt; 28</li><li>斐波那契散列法：通过斐波那契数列找到一个理想的乘数 16（40503）、32（2654435769）、64（11400714819323198485）。index &#x3D; hash值 * 乘数 &gt;&gt; 28</li></ol><h3 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h3><p>还有一些优秀的算法升级，针对不同的问题我们可以采用不同的优化，如果我们有一组数据的hash碰撞比较多，那么对于碰撞到同一个index的元素就非常多，虽然能 很快定位到链表的位置，但是链表查找起来又比较费劲，这时候如何优化？</p><p>d-left hashing就可以缓解上面出现的情况。d是多个的意思，意思是我们可以把hash表拆分成多个小的hash表。每个hash表都有自己的hash函数。d&#x3D;2的时候， 如果有一个值会求俩个hash值，然后分别去查找俩个hash表中对应index的链表元素，找到一个元素较小的进行存储。这样就可以加快查找速度，但是我们每次查找 的时候需要进行俩个hash值的计算，同事需要查找俩个链表的数据。</p><h2 id="Blizzard-MPQ文件搜索算法"><a href="#Blizzard-MPQ文件搜索算法" class="headerlink" title="Blizzard MPQ文件搜索算法"></a>Blizzard MPQ文件搜索算法</h2><p>暴雪使用的hash算法更加精妙：hash表中不是用一个hash值来映射，而是用三个hash值来映射的。同时底层没有使用链表结构使用顺移的方法，如果当前index不存 在那就移动到下一个，直到把所有的都遍历过了，回到最初是的位置，没有的话就是没有。</p><h2 id="一致性hash算法"><a href="#一致性hash算法" class="headerlink" title="一致性hash算法"></a>一致性hash算法</h2><p>一致性hash算法是为了解决hash算法局限性而出现的，它能够尽可能小的改变已经存在的服务器请求和处理请求服务器之间的映射关系。一致性hash算法解决了在分布式哈希表（Distribute Hash Table，DHT）中存在的动态伸缩等问题。</p><h2 id="一致性hash算法和hash算法的关系"><a href="#一致性hash算法和hash算法的关系" class="headerlink" title="一致性hash算法和hash算法的关系"></a>一致性hash算法和hash算法的关系</h2><p>一致性hash算法是再哈希算法的基础之上提出的，在动态变化的分布式环境中，普通的hash不满足几个条件：平衡性、单调性、分散性。</p><ol><li>平衡性：hash的结果值应该平均分不到集群中各个机器上，实现机器的负载均衡。</li><li>单调性：如果机器新增或者减少，不能影响已经存在的hash值，否则会影响服务的正常运行</li><li>分散性：是指数据应该分散地存放在分布式中的各个节点中，不用每个借鉴全部保留。</li></ol><p>一致性hash算法在增加减少机器的可以减少数据移动的开销，尽可能保持数据hash到的机器相同。</p><p>如果部分节点中数据比较多，数据分布不均衡，会将这样的节点进行分裂，对原有的数据一份为二、不需要对全部的数据进行hash和划分。</p><p>另外，这样在服务器少、分布不合理的时候，还是会出现某些节点的压力比较大的情况，那么就需要虚拟节点的出现，我们可以对机器分别取不同的hash值，使数据尽<br>可能的分散，来实现负载均衡。</p>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hash</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常用压缩编码</title>
    <link href="/2022/03/08/%E5%B8%B8%E7%94%A8%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81/"/>
    <url>/2022/03/08/%E5%B8%B8%E7%94%A8%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<h1 id="常用压缩算法"><a href="#常用压缩算法" class="headerlink" title="常用压缩算法"></a>常用压缩算法</h1><p>计算机的存储是通过字节来存储的，一个字节是8位，但是我们现实中需要表示的字符是各种各样的，那么如何用有限的字节来表示呢？这个时候编码就出来了，通过使用各种字节的组合来表示出我们需要的字符。而压缩算法是针对我们的存储进行压缩，减少存储的占用，通过对数据使用压缩算法可以有效对减少数据对存储，以及网络io。</p><h2 id="字典编码"><a href="#字典编码" class="headerlink" title="字典编码"></a>字典编码</h2><p>通过一个窗口，分为字典区、待匹配区，如果字典区能够匹配到待匹配区的几个字符，就把待匹配的字符使用索引来代替，窗口不断移动向前移动，直接结束。</p><h3 id="压缩过程"><a href="#压缩过程" class="headerlink" title="压缩过程"></a>压缩过程</h3><ul><li>待压缩的字符串：A B A B C B A B A B C A D   </li><li>窗口大小：10   </li><li>*：代表填充的，为了观察出窗口向前移动   </li><li>|| ||：这个里面是窗口的10个元素</li><li>|：这个是代表字典区和待匹配区的分隔符，字典去大小是6，待匹配是4<br>下面是待压缩的字符串，以及窗口分布情况。<figure class="highlight less"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs less">|| * * * * * * | <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> || <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br></code></pre></td></tr></table></figure>1、刚开始窗口里面只有4个待匹配字符，字典区是空的，所以肯定匹配不到，A就是第一次匹配之后的结果。<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* || * * * * * <span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br><span class="hljs-selector-attr">[A]</span><br></code></pre></td></tr></table></figure>2、字典区只有A，所以B也匹配不到，所以跟第一步相同，继续向前移动,B直接写入<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * || * * * * <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> | <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> || <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br><span class="hljs-selector-attr">[A,B]</span><br></code></pre></td></tr></table></figure>3、字典已经有AB字符串了，待匹配区是ABCB，这个时AB已经在字典区出现了，所以可以使用索引(4,2)表示，4代表字典区A的下表，2代表匹配了几个字符串。另外还需要把待匹配区AB后面的字符C一并写入结果，那就是(4,2)C。这个时候其实是3个字符写入了结果，所以窗口向前移动三位。<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * * * * || * <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> | <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br><span class="hljs-selector-attr">[A,B,(4,2)C]</span><br></code></pre></td></tr></table></figure>4、这时候待匹配区是BABA，BAB正好字典是可以匹配到到，跟上面一样，使用索引代替就是[2,3]A,同时窗口向前移动4位<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * * * * * <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> || * * *<br><span class="hljs-selector-attr">[A,B,(4,2)C,(2,3)A]</span><br></code></pre></td></tr></table></figure>5、待匹配区到BC也在字典中有，所以可以得到结果(0,2)A，窗口向前移动三位<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * * * * * * * * || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">D</span> * * * ||<br><span class="hljs-selector-attr">[A,B,(4,2)C,(2,3)A,(0,2)A]</span><br></code></pre></td></tr></table></figure>6、最后一步，只有D了，匹配不到直接写入<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[<span class="hljs-name">A</span>,B,(<span class="hljs-name">4</span>,<span class="hljs-number">2</span>)C,(<span class="hljs-name">2</span>,<span class="hljs-number">3</span>)A,(<span class="hljs-name">0</span>,<span class="hljs-number">2</span>)A,D]<br></code></pre></td></tr></table></figure><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3></li></ul><p>优点：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs">压缩比例相对较高，当然需要跟滑动窗口，缓冲区大小，以及数据有关系。<br>解压速度比较快<br></code></pre></td></tr></table></figure><p>缺点：</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs x86asm">压缩过程比较耗时，同时匹配过程比较耗费<span class="hljs-meta">cpu</span><br></code></pre></td></tr></table></figure><h3 id="解压过程"><a href="#解压过程" class="headerlink" title="解压过程"></a>解压过程</h3><p>解压过程其实也比较简单，其实就是把[A,B,(4,2)C,(2,3)A,(0,2)A,D]这个结果进行解释就行</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs armasm">A      : * * * * * * * <span class="hljs-title">||</span> * * * * * A <span class="hljs-title">||</span><br><span class="hljs-keyword">B</span>      : * * * * * * * <span class="hljs-title">||</span> * * * * A <span class="hljs-keyword">B</span> <span class="hljs-title">||</span><br>(<span class="hljs-number">4</span>,<span class="hljs-number">2</span>)C : * * * * * * * <span class="hljs-title">||</span> * A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C <span class="hljs-title">||</span> <br>(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)A : * * * * A <span class="hljs-keyword">B</span> A <span class="hljs-title">||</span> <span class="hljs-keyword">B</span> C <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> A <span class="hljs-title">||</span><br>(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>)A : * A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C <span class="hljs-keyword">B</span> <span class="hljs-title">||</span> A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C A <span class="hljs-title">||</span><br>D      : A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C <span class="hljs-keyword">B</span> A <span class="hljs-title">||</span> <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C A D <span class="hljs-title">||</span><br></code></pre></td></tr></table></figure><h2 id="BYTEDICT"><a href="#BYTEDICT" class="headerlink" title="BYTEDICT"></a>BYTEDICT</h2><p>字节词典编码：顾名思义就是使用简单字节替换原来比较长对值。这种在数据唯一值比较少对情况，尤其是少于256的话，单个byte就可以表示所有出现的值，对符合这种情况的数据会有非常高效对压缩。举例说明：</p><p>原始数据，有一批数据含有字段province字段，省份不会超过34个省。例如这个字段是固定长度10个字节。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">province    长度<br>hebei       <span class="hljs-number">10</span><br><span class="hljs-keyword">beijing </span>    <span class="hljs-number">10</span><br><span class="hljs-keyword">shandong </span>   <span class="hljs-number">10</span><br>省略<br></code></pre></td></tr></table></figure><p>把34个省份生成一个字典 例如0:hebei 1:beijing 2:shandong。存储的时候只需要记录下字典，然后记录对应的索引值就行。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">province    长度<br>hebei       <span class="hljs-number">1</span><br><span class="hljs-keyword">beijing </span>    <span class="hljs-number">1</span><br><span class="hljs-keyword">shandong </span>   <span class="hljs-number">1</span><br>省略<br></code></pre></td></tr></table></figure><p>直接缩小了10倍。如果我们的列式存储orc parquet有一些字段符合这种条件的话，使用这种压缩数据量直接减少10倍，同时最后都是单个字节再配合其他的压缩算法，可以轻松实现10倍以上的数据压缩。</p><h2 id="RLE"><a href="#RLE" class="headerlink" title="RLE"></a>RLE</h2><p>RLE全称run length encoding，这个压缩算法很常用，中文名字是行程长度压缩算法。理解起来也比较简单，就是把相同的字符使用数字表示重复次数。</p><p>例如：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">aaaaaaaaaabbbbbcccccccccc</span><br></code></pre></td></tr></table></figure><p>利用RLE会被压缩成:</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs llvm"><span class="hljs-number">10</span>a<span class="hljs-number">5</span>b<span class="hljs-number">10</span><span class="hljs-keyword">c</span><br></code></pre></td></tr></table></figure><p>这个算法也可以跟其他算法很好的配合，例如就是上面的字典压缩，压缩完成之后可以使用rle再对其进行一次编码进一步压缩。字节词典编码压缩之后的数据，同样可以使用RLE再次压缩。</p><h2 id="Fixed-Bit-Length-Packing"><a href="#Fixed-Bit-Length-Packing" class="headerlink" title="Fixed Bit Length Packing"></a>Fixed Bit Length Packing</h2><p>固定位长算法，减少一些无用对位来减少数据存储，例如1、2、3、4转化成二进制是00000001、00000010、00000011、00000100，可以看到高4位都是0000其实是无用对，那么通过该压缩编码就可以得到0001、0010、0011、0100。</p><h2 id="delta"><a href="#delta" class="headerlink" title="delta"></a>delta</h2><p>增量编码，通过记录相邻数据之间对差异来减少数据量。举例说明：</p><table><thead><tr><th>数据</th><th>占用字节</th><th>差值</th><th>压缩值</th><th>压缩大小</th></tr></thead><tbody><tr><td>1</td><td>4</td><td></td><td>1</td><td>1+4</td></tr><tr><td>5</td><td>4</td><td></td><td>4</td><td>1</td></tr><tr><td>50</td><td>4</td><td></td><td>45</td><td>1</td></tr><tr><td>200</td><td>4</td><td></td><td>150</td><td>1+4</td></tr><tr><td>185</td><td>4</td><td></td><td>-15</td><td>1</td></tr></tbody></table><p>原属数据4*5&#x3D;20个字节。压缩后5+1+1+5+1&#x3D;13个字节。1个字节代表-128～127，所以只要是差值在这个范围内，都可以使用一个字节代表；如果差值超过了这个值那么就重新定制基准，1+4的这个1代表一个标识。可以从上面看到，使用压缩编码对差值比较均匀对压缩性能比较好。</p><p>后续学到新对了继续补充</p><blockquote><p>其中有对参考aws文档：<br><a href="https://docs.aws.amazon.com/zh_cn/redshift/latest/dg/c_Compression_encodings.html">https://docs.aws.amazon.com/zh_cn/redshift/latest/dg/c_Compression_encodings.html</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编码</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>calcite学习</title>
    <link href="/2022/03/08/calcite%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/03/08/calcite%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="calcite学习"><a href="#calcite学习" class="headerlink" title="calcite学习"></a>calcite学习</h1><h2 id="什么是calcite？"><a href="#什么是calcite？" class="headerlink" title="什么是calcite？"></a>什么是calcite？</h2><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><pre><code class="hljs sql">calcite是一个数据库动态管理系统，它提供了一个通用的<span class="hljs-keyword">sql</span>解析、<span class="hljs-keyword">sql</span>校验、<span class="hljs-keyword">sql</span>优化、<span class="hljs-keyword">sql</span>执行的数据库的能力。架构上面是一个灵活的、嵌入式的、可扩展的<br>可以很方便的集成到其他大型项目中去。它的核心是一个关系代数（也是数据库的理论基础），把任何一个查询表示成由关系运算符组成的树。<span class="hljs-keyword">sql</span>转换成关系代数之<br>后，可以进行RBO以及CBO进行规划优化，在保证语义不变的情况下利用一些规则降低成本，来达到一个<span class="hljs-keyword">sql</span>优化的目的。<br></code></pre></td></tr></table></figure><h2 id="什么是关系代数"><a href="#什么是关系代数" class="headerlink" title="什么是关系代数"></a>什么是关系代数</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">关系代数是关系型数据库操作的理论基础，关系代数支持并、差、笛卡尔积、投影和选择等基本运算。在处理一个<span class="hljs-keyword">sql</span>语句的时候，首先将sqlzhu转换成关系表达式，<br>然后通过规则匹配进行相应的优化。<br></code></pre></td></tr></table></figure><h2 id="calcite有哪些优化器"><a href="#calcite有哪些优化器" class="headerlink" title="calcite有哪些优化器"></a>calcite有哪些优化器</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">sql</span>的俩种优化器的类型，RBO CBO,RBO对应这规则优化器根据现在的一些规则对<span class="hljs-keyword">sql</span>进行优化，CBO是成本优化器，他不只是给根据我们写好的规则还可以根据数据<br>的实际情况（包括rownumber cpu io memory）来计算所需要的资源，然后智能选取最小的资源方案。起始俩者根本的不同就是根据<span class="hljs-keyword">cost</span>在继续调整更好的规则去优化<span class="hljs-keyword">sql</span>。缺点就是可能需要一定的计算<br>资源提前算出<span class="hljs-keyword">cost</span>。calcite中的hepPlanner就是RBO优化器；VolcanoPlanner就是CBO优化器。<br></code></pre></td></tr></table></figure><h2 id="RBO"><a href="#RBO" class="headerlink" title="RBO"></a>RBO</h2><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">RBO:</span>Rule-Base Optimizer,基于规则的优化器。<br>基于规则的优化，起始就是结构的匹配和替换，通过匹配语法树上面结构，然后根据结构的特性保持语义不变的大前提下进行变幻乃至替换。RBO中有自己的一套优化<br>规则顺序，无论表数据量是怎么样的。<br></code></pre></td></tr></table></figure><h2 id="CBO"><a href="#CBO" class="headerlink" title="CBO"></a>CBO</h2><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">在RBO中因为都是人为的经验所得的规则，有写系统使用的就是在代码中人工定义了一些规则的优先级来进行<span class="hljs-keyword">bi变换，不能保证变幻之后是否比原来更优，是否性能会</span><br><span class="hljs-keyword"></span>更好，灵活性也相对差一些。而CBO是基于cost（统计信息和代价模型）的优化器。是可能不同的计算方案会有不同的成本，不同的方案其实也是基于不同规则之间的变换来得到的，计算所有<br>规则的成本，选择成本最低的方案为最后的方案得到了最优的结果。主要应用在离线场景<br></code></pre></td></tr></table></figure><h2 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">1</span>、解析<span class="hljs-keyword">sql</span>，把<span class="hljs-keyword">sql</span>转化成为AST<br><span class="hljs-number">2</span>、语法检查，根据数据库中的元数据进行语法验证<br><span class="hljs-number">3</span>、语义分析，构建逻辑执行计划<br><span class="hljs-number">4</span>、逻辑计划优化，优化器的核心，根据前面生成的逻辑计划按照相应的<span class="hljs-keyword">rule</span>进行有优化<br><span class="hljs-number">5</span>、物理执行<br></code></pre></td></tr></table></figure><h2 id="calcite的处理流程"><a href="#calcite的处理流程" class="headerlink" title="calcite的处理流程"></a>calcite的处理流程</h2><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-number">1</span>、sql <span class="hljs-number">2</span> sqlnode:吧sql转换成为抽象语法树<br><span class="hljs-number">2</span>、sqlnode validate：对转换的sqlnode进行校验<br><span class="hljs-number">3</span>、sqlnode <span class="hljs-number">2</span> relnode：把抽象语法树转换成为关系代数<br><span class="hljs-number">4</span>、decorrelate <span class="hljs-literal">and</span> trim field：去相关，去掉不用的字段<br><span class="hljs-number">5</span>、optimize replace rel.root traitset<br><span class="hljs-number">6</span>、顺序应用多个program：执行子查询消除、调用relDecorrelator 和 relFileldTrimmer进行去相关消除字段<br><span class="hljs-number">7</span>、然后会有俩次setroot：<br><span class="hljs-number">8</span>、第一次setroot<br>    a、通过深度遍历，从叶子节点往上遍历<br>    b、首先遍历到叶子节点，为该relnode创建一个relset和一个包含初始relnode的relsubset<br>    c、注册rule对初始的relnode从下往上进行<span class="hljs-keyword">match</span>，<span class="hljs-keyword">match</span>的rule放到<span class="hljs-built_in">queue</span>里面去<br>    d、在这个过程中会初始化cost和维护importance(下边还会init，不知道什么适合确切用到)<br>    e、非叶子节点不同的是会有了input，input会替换为对应的subset<br><span class="hljs-number">9</span>、change <span class="hljs-keyword">trait</span>：rel.root修改对应的<span class="hljs-keyword">trait</span>转成对应的类型，并对转换后的<span class="hljs-keyword">trait</span>添加新的subset（新添加的subset是空的）<br><span class="hljs-number">10</span>、在上面新建出来的relsubset，重新调用setroot，发现<span class="hljs-keyword">trait</span>之间的差异并注册conventer到上面空的subset里面<br><span class="hljs-number">11</span>、然后进入findbestrel<br><span class="hljs-number">12</span>、首先初始化所有的relset的importance<br><span class="hljs-number">13</span>、应用<span class="hljs-built_in">queue</span>中的规则，知道<span class="hljs-built_in">queue</span>中没有数据，找到最优的rel treecc<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>sql优化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>calcite</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hivesql优化</title>
    <link href="/2022/03/04/hivesql%E4%BC%98%E5%8C%96/"/>
    <url>/2022/03/04/hivesql%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="如何写好一个hql"><a href="#如何写好一个hql" class="headerlink" title="如何写好一个hql"></a>如何写好一个hql</h1><p>  作为一个数据开发工程师，hive sql是我们必备的技能，可能大家都知道一些基本的优化方法（例如：使用分区、小表join大表、不使用distinct、where条件尽量写到自查询里面减少数据量等等），但是你有没有想过为什么？是不是真的对执行效率有提升。</p><p>  下面为大家介绍一下hive的优化器以及一些常见的sql技巧。</p><h2 id="常见的优化器"><a href="#常见的优化器" class="headerlink" title="常见的优化器"></a>常见的优化器</h2><p>  如果你想查看hive的优化器，可以从github上面拉一份hive的源码，在org.apache.hadoop.hive.ql.optimizer目录下可以看到hive里面有哪些逻辑优化器。 </p><h3 id="列裁剪优化器"><a href="#列裁剪优化器" class="headerlink" title="列裁剪优化器"></a>列裁剪优化器</h3><p>官方解释    </p><blockquote><p>Implementation of one of the rule-based optimization steps. ColumnPruner gets<br>the current operator tree. The  tree is traversed to find out the columns<br>used for all the base tables. If all the columns for a table are not used, a<br>select is pushed on top of that table (to select only those columns). Since<br>this changes the row resolver, the tree is built again. This can be optimized<br>later to patch the tree</p></blockquote><p>  我们sql中都会用到列裁剪。所谓列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。以我们的日历记录表为例：</p><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> uid,uname,sex<br><span class="hljs-keyword">from</span> user_info<br><span class="hljs-keyword">where</span> dt <span class="hljs-operator">&gt;=</span> <span class="hljs-string">&#x27;20190201&#x27;</span> <span class="hljs-keyword">and</span> dt <span class="hljs-operator">&lt;=</span> <span class="hljs-string">&#x27;20190224&#x27;</span><br><span class="hljs-keyword">and</span> age <span class="hljs-operator">=</span> <span class="hljs-number">18</span>;<br></code></pre></td></tr></table></figure><p>  当列很多或者数据量很大时，如果select * 或者不指定分区，全列扫描和全表扫描效率都很低。</p><p>  Hive中与列裁剪优化相关的配置项是hive.optimize.cp，与分区裁剪优化相关的则是hive.optimize.pruner，默认都是true。在HiveSQL解析阶段对应的则是ColumnPruner逻辑优化器。</p><h3 id="PDD-Predicate-Pushdown"><a href="#PDD-Predicate-Pushdown" class="headerlink" title="PDD(Predicate Pushdown)"></a>PDD(Predicate Pushdown)</h3><p>  谓词下推优化器，在许多数据库中都会使用到，简单说就是把后面的查询条件前置，以下面sql来讲：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <br>    a.<span class="hljs-operator">*</span>, b.<span class="hljs-operator">*</span> <br><span class="hljs-keyword">from</span> a <span class="hljs-keyword">join</span> b <br><span class="hljs-keyword">on</span>  (a.col1 <span class="hljs-operator">=</span> b.col1) <br><span class="hljs-keyword">where</span> a.col1 <span class="hljs-operator">&gt;</span> <span class="hljs-number">20</span> <span class="hljs-keyword">and</span> b.col2 <span class="hljs-operator">&gt;</span> <span class="hljs-number">40</span><br></code></pre></td></tr></table></figure><p>  大部分人可能认为应该通过将 a.col1 &gt; 20 and b.col2 &gt; 40 放到a表和b表里做子查询，减少数据量输入，这样做没有任何问题，但是上面这种写法，通过谓词下推优化器可以实现在读取a表和b表的同时将不符合条件的数据过滤掉。所以有时候不需要通过写自查询减少数据量输入，上面这种语法更加干净整洁。</p><h3 id="mapjoin"><a href="#mapjoin" class="headerlink" title="mapjoin"></a>mapjoin</h3><blockquote><p>Map joins have restrictions on which joins can be converted  as memory restrictions</p></blockquote><p>  简单来说map join就是把小表加入到内存中，直接把相同的key进行join处理，减少shuffle过程，可以极大提高工作效率，适用于码表或者一些大表和小表join的情况。下面是执行流程图：</p><p>  1.先启动Task A；Task A启动一个MapReduce的local task；通过该local task把small table data的数据读取进来；之后会生成一个HashTable Files；之后将该文件加载到分布式缓存中；  </p><p>  2.启动MapJoin Task，读大表的数据，每读一个就会和Distributed Cache中的数据关联一次，关联上后进行输出，整个阶段中没有reduce 和 shuffle。   </p><h3 id="SkewJoinOptimizer"><a href="#SkewJoinOptimizer" class="headerlink" title="SkewJoinOptimizer"></a>SkewJoinOptimizer</h3><p>  数据倾斜优化器：主要应用在发生倾斜的任务中。数据倾斜的情况相信大家也经常遇到，如mapreduce任务进度长时间等待在99%或者一些内存溢出的情况等。产生数据倾斜的原因有很多种，倾斜的原理是很多相同的key用同一个reduce处理，导致处理的任务过大，如共有200亿数据，有100亿为男生 100亿为女生</p><p>上述情况发生了数据倾斜，两个reduce承受了所有的压力，不会有第三个reduce处理数据。   </p><p>  hive倾斜的优化器把一个shuffle拆分成两个shuffle过程： </p><p>1、第一个shuffle过程：给key增加一个随机数，因此生成的hash值也不尽相同，相同的随机数+相同的原key生成的hash值依然一样，如此数据就会放到一起；</p><p>2、第二个shuffle过程：将前边的随机数去掉，重新聚合可以得到想要的结果。</p><blockquote><p>通过分批处理解决数据倾斜问题的方案也是在spark等其他大数据计算引擎中通用且有效的方法。</p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>  hive的优化器其实很多，通过学习hive优化器的原理，让我们可以写出效率更高的sql，如果有兴趣的话可以从github下载hive源码去学习更多优化器的详细内容。</p><h2 id="hql语法进阶与常用小技巧"><a href="#hql语法进阶与常用小技巧" class="headerlink" title="hql语法进阶与常用小技巧"></a>hql语法进阶与常用小技巧</h2><h3 id="CTE查询"><a href="#CTE查询" class="headerlink" title="CTE查询"></a>CTE查询</h3><p>  通过as将查询语句作为一个临时存储表给后边的查询使用，可以使你的sql更加灵活简洁。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">WITH <br>t1 as (select * from t1 where name = &#x27;1&#x27;),<br>t2 as (select * from t2 where name = &#x27;2&#x27;)<br>select * from t1 join t2 on t1.id = t2.id<br></code></pre></td></tr></table></figure><h3 id="列匹配正则表达式"><a href="#列匹配正则表达式" class="headerlink" title="列匹配正则表达式"></a>列匹配正则表达式</h3><p>  一个表一千多个列痛苦不痛苦？通过设置  SET hive.support.quoted.identifiers &#x3D; none；可以输出正则匹配到的列并排除一些不需要的列。</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">SELECT</span> <span class="hljs-symbol">`^o.*`</span> <span class="hljs-keyword">from</span> table_name;<br></code></pre></td></tr></table></figure><h3 id="多表插入"><a href="#多表插入" class="headerlink" title="多表插入"></a>多表插入</h3><p>  有时候我们会遇到不同的查询条件或者需要将一个大表中的数据拆分到不同的数据表中，如果每个数据表写一个sql会造成资源浪费效率也比较低，<br>这时候from语法就来了，如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">from table_name<br>insert overwrite table table1 where id &gt; 100<br>insert overwrite table table2 where name = &#x27;lee&#x27;<br></code></pre></td></tr></table></figure><blockquote><p>可以一次读取table_name表数据插入不同表中</p></blockquote><h3 id="cube-rollup"><a href="#cube-rollup" class="headerlink" title="cube rollup"></a>cube rollup</h3><p>  很多时候我们除了需要在报表中罗列出每个具体项的数据，还需要进行汇总，并且是不同维度的汇总。如果在展示表格的时候汇总，可能会比较慢，我们一般是把结果计算出来之后，以’ALL’或者’总计’,’汇总’等字样作为项的名称，然后放入汇总值。如果仅仅是所有行的汇总，一次聚合就搞定。但是不同维度的汇总就会很麻烦,这时候就轮到cube rollup出场了。</p><h4 id="cube函数"><a href="#cube函数" class="headerlink" title="cube函数"></a>cube函数</h4><p>  cube(a,b,c)则首先会对(a,b,c)进行group by，然后依次是(a,b),(a,c),(a),(b,c),(b),(c),最后在对全表进行group by，他会统计所选列中值的所有组合的聚合，用cube函数就可以完成所有维度的聚合工作。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a,b,c from table_name group by a,b,c with cube<br></code></pre></td></tr></table></figure><blockquote><p>如果我们想要手动实现cube函数需要把所有维度的聚合都用union all来汇总.<br>可以说cube函数方便了用户的使用.<br>但是我并不用知道所有维度的聚合,我就想要col1,(col2,col3)的怎么办?</p></blockquote><p>grouping sets</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a,b,c from table_name group by a,b,c grouping sets(a,(b,c)); <br></code></pre></td></tr></table></figure><h4 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h4><p>  rullup函数是cube的子集,以最左侧维度为主,按照顺序依次进行聚合.<br>例如聚合的维度为 col1,col2,col3 使用rollup聚合的字段分别为 col1,(col1,col2),(col1,col3),(col1,col2,col3)。    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a,b,c from table_name group by a,b,c with rollup;<br></code></pre></td></tr></table></figure><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>  提到行转列相信大家首先想到的是explode，配合lateral view可以实现一行变多行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a, b, c from table_name t lateral view explode(t.e)  as c;<br>--可以配合split使用<br>select a, b, c from table_name t lateral view explode(split(t.e,&#x27;,&#x27;))  as c;<br>--可以配合json<br></code></pre></td></tr></table></figure><p>  另外lateral view 还可以配置json_tuple使用，抽取json的多个字段。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">SELECT <br>    *<br>from<br>(select  *  from  table_name ) t1<br>lateral view json_tuple(json_field,&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;) state_json as a,b,c<br></code></pre></td></tr></table></figure><blockquote><p>这样会把json里面的三个value加载到已有字段后</p></blockquote><p>  行转列对应hive里面的udtf，如果现有的函数满足不了你的需求，可以开发一个udtf来实现自定义的行转列操作。</p><h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>  多行变成一行，其实就是聚合操作，通过group by操作可以实现聚合操作，有时候我们需要把字段信息也保留下来，这时候就用到了collect_ws、collect_set。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">--根据a b 聚合，对c去重<br>select a,b,collect_set(c) from table_name group by a,b;<br>--根据a b 聚合，然后把c放到一个数据<br>select a,b,collect_list(c) from table_name group by a,b;<br>--根据a b 聚合，然后使用，拼接c字段<br>select a,b,concat_ws(&quot;,&quot;,collect_list(c)) from table_name group by a,b;<br></code></pre></td></tr></table></figure><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><p>  窗口函数是用于分析的一类函数，要理解窗口函数要先从聚合函数说起。 大家都知道聚合函数是将某列中多行的值合并为一行，比如sum、count等。而窗口函数则可以在本行内做运算，得到多行的结果，即每一行对应一行的值。 通用的窗口函数可以用下面的语法来概括：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">Function() Over (Partition By Column1,Column2,Order By Column3);<br></code></pre></td></tr></table></figure><p>  窗口函数分三类：聚合型窗口函数、分析型窗口函数、取值型窗口函数</p><h4 id="聚合型"><a href="#聚合型" class="headerlink" title="聚合型"></a>聚合型</h4><p>  聚合型即SUM(),MIN(),MAX(),AVG(),COUNT()这些常见的聚合函数。 聚合函数配合窗口函数使用可以使计算更加灵活。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select *,sum/count/max/min/avg(file_name) over(partition by file_name order by field_name) from table_name<br></code></pre></td></tr></table></figure><h4 id="分析型"><a href="#分析型" class="headerlink" title="分析型"></a>分析型</h4><p>  分析型即RANk(),ROW_NUMBER(),DENSE_RANK()等常见的排序用的窗口函数，不过他们也是有区别的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select *,<br>--俩个元素相同会跳过下一个序列号<br>rank() over (order by create_time) as user_rank,<br>--生成连续的序列号<br>row_number() over (order by create_time) as user_row_number,<br>--俩个元素相同不会跳过下一个序列号<br>dense_rank() over (order by create_time) as user_dense_rank<br>from table_name<br></code></pre></td></tr></table></figure><h4 id="取值型"><a href="#取值型" class="headerlink" title="取值型"></a>取值型</h4><p>  这几个函数可以通过字面意思可知，LAG是迟滞的意思，用于统计窗口内往上第n行值；LEAD是LAG的反义词，用于统计窗口内往下第n行值；FIRST_VALUE是该列到目前为止的首个值，而LAST_VALUE是到目前行为止的最后一个值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">--以贷款为例<br>SELECT *,<br>--取上一笔贷款的日期,缺失默认填NULL<br>LAG(orderdate, 1) OVER(PARTITION BY name ORDER BY orderdate) AS last_dt,<br>--取下一笔贷款的日期,缺失指定填&#x27;1970-1-1&#x27;<br>LEAD(orderdate, 1,&#x27;1970-1-1&#x27;) OVER(PARTITION BY name ORDER BY orderdate) AS next_dt,<br>--去第一个的日期<br>FIRST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS first_dt,<br>--取最后一个日期<br>LAST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS latest_dt<br>from table_name<br></code></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>本文主要分享了hive的优化器和SQL使用技巧。最后，不管你是SQL boy or  SQL girl，只要掌握一些技巧，相信都能够Happy SQL querying 😊。</p>]]></content>
    
    
    <categories>
      
      <category>sql优化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hive-mapjoin探索</title>
    <link href="/2022/03/02/hive-mapjoin%E6%8E%A2%E7%B4%A2/"/>
    <url>/2022/03/02/hive-mapjoin%E6%8E%A2%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="让你真正了解mapjoin参数"><a href="#让你真正了解mapjoin参数" class="headerlink" title="让你真正了解mapjoin参数"></a>让你真正了解mapjoin参数</h1><p>首先创建好small_table和big_table俩个表。直接给俩个表执行join。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs hiveql">set hive.cbo.enable=false;<br>set hive.auto.convert.join=false; <br>set hive.auto.convert.join.noconditionaltask=false; <br>explain select t1.* from small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>接下来分析执行计划，一共俩个stage，第一个stage是join，第二个是进行fetch数据</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">STAGE</span> DEPENDENCIES:<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">1</span> is a root stage<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">0</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">1</span> 其实stage0就是fetch算子，把结果拉到client端<br></code></pre></td></tr></table></figure><p>分析stage-0,看fetch数据就是fetch operator</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">Stage</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Stage-0</span><br>  <span class="hljs-attribute">Fetch Operator</span><br><span class="hljs-attribute">    limit</span><span class="hljs-punctuation">:</span> <span class="hljs-string">-1 代表所有</span><br>    <span class="hljs-attribute">Processor Tree</span><span class="hljs-punctuation">:</span><br>      ListSink <br></code></pre></td></tr></table></figure><p>分析主要的阶段stage-1，这个阶段scan俩个表的数据，同时抽取join key进行了reduce操作。</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs arcade">STAGE PLANS:<br>  Stage: Stage<span class="hljs-number">-1</span><br>    <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Reduce</span>  代表这个stage就是一个mapreduce任务<br>      <span class="hljs-built_in">Map</span> Operator Tree:  这个代表<span class="hljs-built_in">map</span>算子的阶段，<span class="hljs-built_in">map</span>针对上面sql其实就是对俩个表进行scan，同时取对应对字段<br>      <span class="hljs-built_in">Reduce</span> Operator Tree:  这个代表<span class="hljs-built_in">reduce</span>算子的阶段，针对join对字段进行聚合<br></code></pre></td></tr></table></figure><p>先看下Map Operator Tree，需要注意的是join会自动把join字段的null去掉。 id is not null，看TableScan t1和TableScan t2区别就是第一个是有个value表达式，因为select只select了t1表的字段，其他没有什么区别，都是扫描表的数据。</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>            alias: t1<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              Reduce Output Operator<br>                key expressions: id (<span class="hljs-keyword">type</span>: int)<br>                sort <span class="hljs-keyword">order</span>: +<br>                <span class="hljs-built_in">Map</span><span class="hljs-params">-reduce</span> partition columns: id (<span class="hljs-keyword">type</span>: int)<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                value expressions: name (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">string</span>),<span class="hljs-params">...</span>f7 (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">string</span>)<br>          TableScan<br>            alias: t2<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              Reduce Output Operator<br>                key expressions: id (<span class="hljs-keyword">type</span>: int)<br>                sort <span class="hljs-keyword">order</span>: +<br>                <span class="hljs-built_in">Map</span><span class="hljs-params">-reduce</span> partition columns: id (<span class="hljs-keyword">type</span>: int)<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br></code></pre></td></tr></table></figure><p>最后就是reduce阶段</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs sqf">Reduce Operator Tree:<br>  <span class="hljs-built_in">Join</span> Operator<br>    condition map:<br>         Inner <span class="hljs-built_in">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>    <span class="hljs-built_in">keys</span>:<br>      <span class="hljs-number">0</span> id (<span class="hljs-built_in">type</span>: int)<br>      <span class="hljs-number">1</span> id (<span class="hljs-built_in">type</span>: int)<br>    outputColumnNames: <span class="hljs-variable">_col0</span>, <span class="hljs-variable">_col1</span>, <span class="hljs-variable">_col2</span>, <span class="hljs-variable">_col3</span>, <span class="hljs-variable">_col4</span>, <span class="hljs-variable">_col5</span>, <span class="hljs-variable">_col6</span>, <span class="hljs-variable">_col7</span>, <span class="hljs-variable">_col8</span>, <span class="hljs-variable">_col9</span>, <span class="hljs-variable">_col10</span><br>    Statistics: Num rows: <span class="hljs-number">10507</span> Data <span class="hljs-built_in">size</span>: <span class="hljs-number">1142980</span> Basic stats: COMPLETE Column stats: NONE<br>    File Output Operator<br>      compressed: <span class="hljs-literal">false</span><br>      Statistics: Num rows: <span class="hljs-number">10507</span> Data <span class="hljs-built_in">size</span>: <span class="hljs-number">1142980</span> Basic stats: COMPLETE Column stats: NONE<br>      table:<br>          input <span class="hljs-built_in">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat<br>          output <span class="hljs-built_in">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat<br>          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br></code></pre></td></tr></table></figure><p>以上就是没有开始mapjoin，让我们把mapjoin打开试一试，小表的大小是10M。测试一下开启mapjoin。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">set hive.cbo.enable=false;<br>set hive.auto.convert.join=false;<br>set hive.auto.convert.join.noconditionaltask=false;<br>set hive.auto.convert.join=true;<br>explain select t1.* from small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>先分析一下stage之间的依赖</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">STAGE</span> DEPENDENCIES:<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">4</span> is a root stage , consists of Stage-<span class="hljs-number">5</span>, Stage-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">5</span> has a backup stage: Stage-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">3</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">5</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">0</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">3</span>, Stage-<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>首先看stage4，这个stage是root，也就是最顶部的，stage Conditional Operator这是一个条件运算符，包括5和1。这快看不太懂什么意思，我们先看看看stage-1这个后边没有任何依赖。</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-1</span><br>    <span class="hljs-built_in">Map</span> Reduce<br>      <span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>          TableScan<br>      Reduce Operator Tree:<br>        <span class="hljs-keyword">Join</span> Operator<br>          condition <span class="hljs-built_in">map</span>:<br>               Inner <span class="hljs-keyword">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>          keys:<br>            <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>            <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br></code></pre></td></tr></table></figure><p>stage-1可以看出来，跟不开启mapjoin是一摸一样的，可能这个主要是现实最初的执行计划的。没有太多用处，即诶啊来看下stage-0这个主要是代表我们fetch数据的算子，也就是客户端拉取结果的</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">Stage</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Stage-0</span><br>    <span class="hljs-attribute">Fetch Operator</span><br><span class="hljs-attribute">      limit</span><span class="hljs-punctuation">:</span> <span class="hljs-string">-1</span><br>      <span class="hljs-attribute">Processor Tree</span><span class="hljs-punctuation">:</span><br>        ListSink<br></code></pre></td></tr></table></figure><p>跟原来的一样，这个stage-0依赖于3、1，1我们已经看过了，我们直接看3</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-3</span><br>    <span class="hljs-built_in">Map</span> Reduce<br>      <span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>            alias: t2<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">955184</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">103902452</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">955184</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">103902452</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              <span class="hljs-built_in">Map</span> <span class="hljs-keyword">Join</span> Operator<br>                condition <span class="hljs-built_in">map</span>:<br>                     Inner <span class="hljs-keyword">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>                keys:<br>                  <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>                  <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br>                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">1050702</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">114292699</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                File Output Operator<br>                  compressed: <span class="hljs-literal">false</span><br>                  Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">1050702</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">114292699</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                  table:<br>                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat<br>                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat<br>                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br>      <span class="hljs-built_in">Local</span> Work:<br>        <span class="hljs-built_in">Map</span> Reduce <span class="hljs-built_in">Local</span> Work<br></code></pre></td></tr></table></figure><p>这个阶段就是关键点，可以看到这是一个mapreduce任务，但是没有Reduce Operator，使用一个local work替代了，而且join的条件都放在了Map Operator里面了，所以这个就是mapjoin的真正逻辑，接下来看一下local work，因为stage-3依赖于stage-5，接下来看stage-5</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-5</span><br>   <span class="hljs-built_in">Map</span> Reduce <span class="hljs-built_in">Local</span> Work<br>     Alias -&gt; <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Local</span> Tables:<br>       t1<br>         Fetch Operator<br>           limit: <span class="hljs-number">-1</span><br>     Alias -&gt; <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Local</span> Operator Tree:<br>       t1<br>         TableScan<br>           alias: t1<br>           Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">95521</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10390248</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>           Filter Operator<br>             predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>             Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">95521</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10390248</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>             HashTable Sink Operator<br>               keys:<br>                 <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>                 <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br></code></pre></td></tr></table></figure><p>可以看到这个一个mapreduce的本地工作，跟stage-3里面的local work是能对上的，这个很好理解，就是把t1表通过本地加载，条件跟mapreduce里面的是一样的。</p><p>接下来，以后我们查看表的执行计划可以很快的分析出来。接下来让我们验证下看起cbo是否对mapjoin有影响。<br>首先我们对small小表直接插入100M数据，这个数据量不满足了mapjoin对条件，首先看关闭CBO</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.cbo.<span class="hljs-attribute">enable</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.<span class="hljs-attribute">noconditionaltask</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br>explain select t1.* <span class="hljs-keyword">from</span> small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>这个结果跟不开启mapjoin是一致对，因为俩个表对大小都超过了我们设定都阈值。这边可以判断cdp读取hive的元数据没有影响mapjoin的执行，因为我看到有的文章说有时候开启cbo以后mapjoin不生效（不过别人的文章是说的使用tez执行引擎，所以这点还有待验证）</p><p>我们测试开启cbo，开启cbo成本应该是从hive都元数据里面取数据都，但是由于我们是直接往hdfs上面放都数据，元数据small表还是10M</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.cbo.<span class="hljs-attribute">enable</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.mapjoin.smalltable.<span class="hljs-attribute">filesize</span>=460000000;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.<span class="hljs-attribute">noconditionaltask</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.noconditionaltask.<span class="hljs-attribute">size</span>=10000000; <br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br>explain select t1.* <span class="hljs-keyword">from</span> small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>sql优化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>parquet</title>
    <link href="/2022/03/02/parquet-md/"/>
    <url>/2022/03/02/parquet-md/</url>
    
    <content type="html"><![CDATA[<h1 id="PARQUET"><a href="#PARQUET" class="headerlink" title="PARQUET"></a>PARQUET</h1><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><figure class="highlight node-repl"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs node-repl">1、更高的压缩比可以减少磁盘的使用<br>2、更少的IO操作使用映射下推和谓词下推，可以读取需要的列，过滤掉不需要的列，减少不必要的数据扫描，尤其对于表字段比较多的时候优势明显。<br><span class="hljs-meta">&gt;</span> <span class="language-javascript">映射下推：列式存储最突出的优势，数据扫描的时候，只需要扫描需要的列，避免全表扫描</span><br><span class="hljs-meta">&gt;</span> <span class="language-javascript">谓词下推：将过滤条件在最底层执行减少结果集</span><br></code></pre></td></tr></table></figure><h2 id="如何提升parquet的查询性能"><a href="#如何提升parquet的查询性能" class="headerlink" title="如何提升parquet的查询性能"></a>如何提升parquet的查询性能</h2><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-number">1</span>、对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以更好的利用最大值最小值实现谓词下推<br><span class="hljs-number">2</span>、减少行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的<span class="hljs-built_in">I</span><span class="hljs-operator">/</span><span class="hljs-built_in">O</span>负载。<br></code></pre></td></tr></table></figure><h2 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h2><p>row group &gt; column chunk &gt; page</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs css">row group行组：每一个行组包含一定的行数，一般对应一个HDFS文件块，Parquet读写的时候会将整个行组缓存在内存中<br>column chunk列块：在一个行组中，每一列保存在一个列块中，一个列块中的类型都是相同的，不同类型的列块可以使用不同d的算法进行压缩。<br>page页：每一个列块被划分成多个页，页时最小的编码单位，在同一个列块的不同页可能使用不同的编码方式<br><br>一个parquet文件，在文件头和文件尾都是<span class="hljs-number">4</span>个字节的magic <span class="hljs-selector-tag">code</span>，校验是否是parquet文件。<br>然后尾部magic <span class="hljs-selector-tag">code</span>之前是<span class="hljs-selector-tag">footer</span> length，是文件元数据的大小，通过这个大小就可以计算出y元数据<span class="hljs-selector-tag">footer</span>的偏移量<br><span class="hljs-selector-tag">footer</span>中包括：<br><span class="hljs-number">1</span>、每个行组的元信息：由多少个列块组成；<br><span class="hljs-number">2</span>、每个列块的元信息：类型、路径、编码、第一个数据页的位置、第一个索引页的位置、压缩/未压缩大小、以及一些额外的配置kv<br><span class="hljs-number">3</span>、又有三种pag：<br>    数据页主要存储defination levels、repeatition levels、value<br>    字典页：存储列值的编码字段<br>    索引页：用来存储该页<br></code></pre></td></tr></table></figure><h2 id="parquet的编码格式"><a href="#parquet的编码格式" class="headerlink" title="parquet的编码格式"></a>parquet的编码格式</h2><p><a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">https://github.com/apache/parquet-format/blob/master/Encodings.md</a></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">0</span>、PLAIN：如果没有其他的有效编码就不进行编码，也就是plain<br><span class="hljs-number">2</span>、PLAIN_DICTIONARY /<span class="hljs-number">8</span>、RLE_DICTIONARY：原始数据的字典编码，字典存储在字典页中对应page中的字典页。字典中的值使用RLE/<span class="hljs-type">bit</span>-packing<br>回合编码存储位整数。<br>   字典页格式（PLAIN）：字典中的条数-按字典顺序-使用普通编码<br>   数据页格式：用于对存储位<span class="hljs-number">1</span>个字节条目id进行编码的位宽（最大<span class="hljs-number">32</span>位），这个id使用RLE/<span class="hljs-type">bit</span>-packing打包编码的值，在<span class="hljs-number">2.0</span>中使用RLE/<span class="hljs-keyword">DICTIONARY</span>   <br><span class="hljs-number">3</span>、Run Length <span class="hljs-keyword">Encoding</span> / <span class="hljs-type">Bit</span>-Packing Hybrid：这种编码使用位打包和运行长度编码的组合来更有效地存储重复值<br>   RLE:行程编码，重复的值计数。<br>   <span class="hljs-type">bit</span>-packing：位打包主要用来解决序列化所需要的位数<br><span class="hljs-number">4</span>、<span class="hljs-type">Bit</span>-packed：仅支持编码重复和定义级别<br><span class="hljs-number">5</span>、Delta <span class="hljs-keyword">Encoding</span>：int32、int64<br><span class="hljs-number">6</span>、Delta-length byte <span class="hljs-keyword">array</span>：BYTE_ARRAY<br><span class="hljs-number">7</span>、Delta Strings：BYTE_ARRAY<br><span class="hljs-number">9</span>、Byte Stream Split： <span class="hljs-type">FLOAT</span> <span class="hljs-type">DOUBLE</span><br></code></pre></td></tr></table></figure><h2 id="parquet元数据使用什么序列化方式"><a href="#parquet元数据使用什么序列化方式" class="headerlink" title="parquet元数据使用什么序列化方式"></a>parquet元数据使用什么序列化方式</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql">parqut的元数据是在整个<span class="hljs-type">row</span> <span class="hljs-keyword">group</span>都写完成之后再进行写入的，这时候写入的文件的元数据信息全部都保存再了内存中，如果要读取的话也是需要把元数据全部都<br>读取到内存中才行。<br>把内存中的数据序列化到磁盘中，使用了thrift的TCompactProtocol高性能序列化方式。<br>thrit相对于其他序列化方式性能比较好，兼容性可能比较差。<br></code></pre></td></tr></table></figure><blockquote><p><a href="https://blog.51cto.com/u_15080016/2620917">https://blog.51cto.com/u_15080016/2620917</a></p></blockquote><h2 id="Sriping-x2F-Assembly算法"><a href="#Sriping-x2F-Assembly算法" class="headerlink" title="Sriping&#x2F;Assembly算法"></a>Sriping&#x2F;Assembly算法</h2><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stata">打散/组装算法，主要parquet如何打散和组装数据的。这个算法主要通过repetition <span class="hljs-keyword">levels</span>和definition <span class="hljs-keyword">levels</span>来描述文件的拆分和组装。重复级别主要<br>用来描述repeated的节点，在写入的适合等于它和前面的值从那一层节点不共享的。在读取的适合根据该值可以推断出哪一层需要创建一个新的节点；定义级别主要<br>用来记录哪些值使没有的使null值。可以很好的支持嵌套类型的存储。<br>repeated <span class="hljs-keyword">levels</span>主要是用来一列的作用的，defintition <span class="hljs-keyword">levels</span>用来描述一行的数据。主要防止一列有空值导致错位。<br>不同的列跳转使用了有限状态机<br></code></pre></td></tr></table></figure><h2 id="parquet的bloomfilter"><a href="#parquet的bloomfilter" class="headerlink" title="parquet的bloomfilter"></a>parquet的bloomfilter</h2><p><a href="https://github.com/apache/parquet-format/blob/master/BloomFilter.md">https://github.com/apache/parquet-format/blob/master/BloomFilter.md</a></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">我们都知道，在parquet和orc等列式存储中，都会用到一些索引和统计信息：记录下一些列的最大值最小值，记录字典等。，来对查询进行谓词下推，<br>直接下推到文件上面，减少数据的传输。谓词下推可以说是数据库查询优化的一大手段。<br>但是在有一些没有最大值最小值，字典太大、数据基数非常大的情况下布隆过滤器可以起到很好的优化操作。<br>布隆过滤器可以过滤掉肯定不存在的数据，可能存在的数据来相应查询。可能存在的数据又可以通过构建<span class="hljs-type">bit</span>数组的大小来决定的。<span class="hljs-type">bit</span>数据占用的空间<br>非常小，以至于用很小的空间就可以达到很好的过滤效果。<br>其实布隆过滤器主要的目标就是为·高基数·的列启用谓词下推，同时使用更少的存储空间。同时如果某一列没有启用布隆过滤器也不会影响性能。<br><br>parquet使用的是一种split block布隆过滤器。就是一个列有多个block，每个block有<span class="hljs-number">8</span>个小块组成，小块的话其实就是<span class="hljs-type">bit</span>数组。然后通过<br>加盐（对应<span class="hljs-type">int</span>就是*对应一个数组保存着<span class="hljs-number">8</span>个<span class="hljs-number">32</span>位的整数一一对应）移位来取得每个<span class="hljs-type">bit</span>数组中的具体位置，然后进行插入。其中就是为了把对应的<br>值进行散列，使用了奇数*常数右移的一种散列整数方法。块越多准确率越高<br><br>针对其他类型的数据，parquet使用xxhash来做的转换<br><br>数据的话也是相当于元数据使用的thrift做的序列化<br><br>bloomfilter存储在了footer下面的<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span>的<span class="hljs-keyword">column</span> chunk下面，作为列的元数据。<br>敏感信息应该使用列加密<br></code></pre></td></tr></table></figure><h2 id="parquet文件支持哪些压缩方式"><a href="#parquet文件支持哪些压缩方式" class="headerlink" title="parquet文件支持哪些压缩方式"></a>parquet文件支持哪些压缩方式</h2><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">snappy:</span>google提供<br><span class="hljs-symbol">gzip:</span>基于RFC <span class="hljs-number">1952</span>定义的 GZIP 格式（zlib权威）<br><span class="hljs-symbol">lzo:</span><br><span class="hljs-symbol">brotli:</span><br><span class="hljs-symbol">lz4:</span><br><span class="hljs-symbol">zstd:</span><br><span class="hljs-symbol">lz4_raw:</span><br></code></pre></td></tr></table></figure><h2 id="parquet支持哪些数据类型"><a href="#parquet支持哪些数据类型" class="headerlink" title="parquet支持哪些数据类型"></a>parquet支持哪些数据类型</h2><p>文件格式的数据类型</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">BOOLEAN</span>：<span class="hljs-number">1</span> 位布尔值<br><span class="hljs-attribute">INT32</span>：<span class="hljs-number">32</span> 位有符号整数<br><span class="hljs-attribute">INT64</span>：<span class="hljs-number">64</span> 位有符号整数<br><span class="hljs-attribute">INT96</span>：<span class="hljs-number">96</span> 位有符号整数<br><span class="hljs-attribute">FLOAT</span>：IEEE <span class="hljs-number">32</span> 位浮点值<br><span class="hljs-attribute">DOUBLE</span>：IEEE <span class="hljs-number">64</span> 位浮点值<br><span class="hljs-attribute">BYTE_ARRAY</span>：任意长的字节数组。<br></code></pre></td></tr></table></figure><p>逻辑数据类型</p><h2 id="parquet的列索引"><a href="#parquet的列索引" class="headerlink" title="parquet的列索引"></a>parquet的列索引</h2><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs applescript">columnIndex和offsetIndex；索引的结构和长度存储在l了columnchunk中<br>columnIndex：这个可以根据制定的列值直接到数据页。可以用于谓词下推上面。<br>offsetindex：这个可以按照行数进行索引，这个主要用来其他列有了过滤，跳过许多数据，那么这个列也会根据<span class="hljs-built_in">offset</span>去挑过对应的数据。<br><span class="hljs-number">1</span>、索引里面右数据页的最大值和最小值<br><span class="hljs-number">2</span>、针对排序的列只需要读取包含数据相关的确定的数据页<br><span class="hljs-number">3</span>、针对于谓词下推，可以通过字典以及索引找到确定的索引页<br><span class="hljs-number">4</span>、如果不需要操作也不会带来额外的开销<br><span class="hljs-number">5</span>、如果是已经排序的，仅仅存储边界元素<br><span class="hljs-number">6</span>、对于有序的列还会运用二分查找<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>列式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>orc</title>
    <link href="/2022/03/02/orc/"/>
    <url>/2022/03/02/orc/</url>
    
    <content type="html"><![CDATA[<h1 id="ORC-Optimized-Record-Columnar"><a href="#ORC-Optimized-Record-Columnar" class="headerlink" title="ORC(Optimized Record Columnar)"></a>ORC(Optimized Record Columnar)</h1><h2 id="orc支持哪些数据类型"><a href="#orc支持哪些数据类型" class="headerlink" title="orc支持哪些数据类型"></a>orc支持哪些数据类型</h2><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><pre><code class="hljs sql">整数<br>  <span class="hljs-type">boolean</span>：<span class="hljs-number">1</span>bit<br>  tinyint：<span class="hljs-number">8</span>bit<br>  <span class="hljs-type">smallint</span>：<span class="hljs-number">16</span>bit<br>  <span class="hljs-type">int</span>：<span class="hljs-number">32</span>bit<br>  <span class="hljs-type">bigint</span>：<span class="hljs-number">64</span>bit<br>浮点<br>  <span class="hljs-type">float</span>、<span class="hljs-keyword">double</span><br>字符串<br>  string、<span class="hljs-type">char</span>、<span class="hljs-type">varchar</span><br>字节<br>  <span class="hljs-type">binary</span> blobs<br>日期<span class="hljs-operator">/</span>时间<br>  <span class="hljs-type">timestamp</span>、<span class="hljs-type">timestamp</span> <span class="hljs-keyword">with</span> <span class="hljs-keyword">local</span> <span class="hljs-type">time</span> zone、<span class="hljs-type">date</span><br>复合类型<br>  struct、list、map、<span class="hljs-keyword">union</span><br></code></pre></td></tr></table></figure><h2 id="orc支持哪些索引"><a href="#orc支持哪些索引" class="headerlink" title="orc支持哪些索引"></a>orc支持哪些索引</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">file <span class="hljs-keyword">level</span>文件级别：有关整个文件中每一列的统计信息（存储在页脚）<br>stripe <span class="hljs-keyword">level</span>条带级别：每个条带的每列值统计信息（存储在页脚）<br><span class="hljs-keyword">row</span> <span class="hljs-keyword">level</span> 行级别：每<span class="hljs-number">10000</span>h行每列中的统计信息（存储在行组）<br>布隆过滤器：针对特定的列使用布隆过滤器有效的进行谓词下推 orc.bloom.<span class="hljs-keyword">filter</span>.<span class="hljs-keyword">columns</span>，他跟<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">index</span>交错分布的，首先根据索引缩小数据范围<br>再使用布隆过滤器进一步过滤数据。<br></code></pre></td></tr></table></figure><h2 id="acid支持"><a href="#acid支持" class="headerlink" title="acid支持"></a>acid支持</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">目的不是支持OLTP操作，而是支持百万计事务的更新，不是百万次事务。同时有了acid还可以保证流式数据插入到数据表中。<br>启用acid的表会在对应的目录下面创建增量目录。delta_xxx/bucket_xxx,<br>当delta变多了之后，会自动启用一些小的压缩，将一组事务h合并位一个单独的delta。<br>当增量文件变打了之后，会启动任务去重写基础+增量文件<br>针对orc文件通过id、bucket和row id来保证数据的唯一，当数据update、<span class="hljs-keyword">delete</span>、insert就是用到这三个值，对应的序列化<span class="hljs-number">0</span> 插入 <span class="hljs-number">1</span>更新 <span class="hljs-number">2</span>删除<br>https:<span class="hljs-regexp">//</span>orc.apache.org<span class="hljs-regexp">/docs/</span>acid.html<br></code></pre></td></tr></table></figure><h2 id="针对hive的操作"><a href="#针对hive的操作" class="headerlink" title="针对hive的操作"></a>针对hive的操作</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> istari <span class="hljs-keyword">SET</span> FILEFORMAT ORC：将表新分区存储位orc<br><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> istari [<span class="hljs-keyword">PARTITION</span> partition_spec] CONCATENATE：针对分区进行合并，在stripe级别上面进行合并<br>hive <span class="hljs-comment">--orcfiledump &lt;path_to_file&gt;：orc文件信息</span><br>hive <span class="hljs-comment">--orcfiledump -d &lt;path_to_file&gt;：显示数据</span><br><br>一些参数：<br>orc.compress=ZLIB/<span class="hljs-keyword">NONE</span>/SNAPPY<br>orc.compress.size=<span class="hljs-number">262144</span>：针对每个chunk size的压缩大小<br>oorc.stripe.size=<span class="hljs-number">67108864</span>：每个stripe的大小<br>orc.<span class="hljs-keyword">row</span>.<span class="hljs-keyword">index</span>.stride=<span class="hljs-number">10000</span>：索引的条目数<br>orc.<span class="hljs-keyword">create</span>.<span class="hljs-keyword">index</span>=<span class="hljs-keyword">true</span>：是否创建索引<br>orc.bloom.<span class="hljs-keyword">filter</span>.<span class="hljs-keyword">columns</span>=&quot;&quot;：需要开启布隆过滤器的列<br>orc.bloom.<span class="hljs-keyword">filter</span>.fpp=<span class="hljs-number">0.05</span>：布隆过滤器的准确率<br><br>TBLPROPERTIES (&quot;orc.compress&quot;=&quot;NONE&quot;);<br><br>https://orc.apache.org/docs/hive-ddl.html<br>https://orc.apache.org/docs/hive-config.html<br></code></pre></td></tr></table></figure><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">1</span>、ORC是列式存储，有多种文件压缩方式，压缩比例也很高。   <br><span class="hljs-number">2</span>、文件可切分。可切分的意义在于，可以控制task的数量以及减少数据的输入。   <br><span class="hljs-number">3</span>、提供了多种索引：<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">index</span>、bloom <span class="hljs-keyword">filter</span> <span class="hljs-keyword">index</span>。   <br><span class="hljs-number">4</span>、支持复杂的数据结构   <br></code></pre></td></tr></table></figure><h2 id="为什么列式存储可以加快olap的查询速度"><a href="#为什么列式存储可以加快olap的查询速度" class="headerlink" title="为什么列式存储可以加快olap的查询速度"></a>为什么列式存储可以加快olap的查询速度</h2><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs arcade">每一列的所有元素都是顺序存储的，这样可以带来的好处就是：   <br><span class="hljs-number">1</span>、查询的时候不需要扫描全部数据，只需要扫描对应的列数据就可以，还有就是orc保存着每一列的统计信息包括<span class="hljs-built_in">min</span> <span class="hljs-built_in">max</span> <span class="hljs-built_in">sum</span>等，这样配合hive的谓词下推优化器<br>可以实现谓词下推到数据结构的底层，完美减少了数据的输入，没有必须的数据再数据读取的时候完全避免了。   <br><span class="hljs-number">2</span>、每一列的成员都是同构的，可以针对不同的数据类型使用更高效的压缩算法，进一步减少I/O。   <br><span class="hljs-number">3</span>、每一类的成员都是同构性，更加实用CPU pipeline的编码方式，减少CPU缓存失效<br></code></pre></td></tr></table></figure><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs arcade">ORC的结构中包含了负责类型和原始类型，前者包括list struck <span class="hljs-built_in">map</span> <span class="hljs-built_in">union</span> 后者有<span class="hljs-built_in">boolean</span>、整数、浮点等。struct的孩子可能包括多个孩子节点，<span class="hljs-built_in">map</span>有俩个<br>孩子节点，list有一个孩子节点。每个<span class="hljs-built_in">schema</span>根节点是哟个struck类型， 所有的column按照树的中序遍历顺序编号。<br>&gt; ORC只需要存储<span class="hljs-built_in">schema</span>树中叶子节点的值，而中间的非叶子节点只是做了一层代理，只需要负责孩子节点值的数据读取，只有真正的的叶子节点才会读取数据，然后<br>&gt; 由树节点封装成对应的数据结构返回<br></code></pre></td></tr></table></figure><h2 id="orc相对rcfile优势"><a href="#orc相对rcfile优势" class="headerlink" title="orc相对rcfile优势"></a>orc相对rcfile优势</h2><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>、每个task单独输出单一文件，减少了namenode的负载<br><span class="hljs-number">2</span>、支持复杂数据类型<br><span class="hljs-number">3</span>、文件级别的轻量化索引：可以跳过对应row group。定位行数据<br><span class="hljs-number">4</span>、根据数据类型来进行基于<span class="hljs-keyword">block的数据压缩，多种压缩编码 </span>RLE <span class="hljs-keyword">DIRECT</span><br><span class="hljs-keyword"></span><span class="hljs-number">5</span>、使用独立的recordReader并行读取同一个文件<br><span class="hljs-number">6</span>、不必扫描标识就可以对文件进行切分<br><span class="hljs-number">7</span>、限定读写需要的内存<br><span class="hljs-number">8</span>、元数据以protocol <span class="hljs-keyword">buffer存储，允许增加删除域</span><br><span class="hljs-keyword"></span><span class="hljs-number">9</span>、压缩方面支持更多的压缩方式 zlib snappy<br></code></pre></td></tr></table></figure><h2 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs clean">尾部开始读取，首先是magic <span class="hljs-keyword">code</span><br>postscript：file footer的长度、文件的压缩方式、压缩大小、元数据的长度、文件的版本号<br>file footer：stripe列表，每个stripe的行数，列的数据类型，列的聚合信息count min max sum<br>    stripe information：offset、索引的长度、数据的长度、footer的长度、以及strope的行数<br>    type information：schema信息是表大成了一颗树结构，通过先序遍历序列化protocol buffer到文件里面<br>    column statistics：行数、最大值、最小值、求和、是否有null，可能对不同的数据类型定义了不同的统计信息<br>    user metadata：这个是用户自己可以设置一些kv存储，方便读取的时候使用。<br>    file metadata：整个列的（可能多个stripe）统计信息，来再数据分片的时候就进行数据谓词下推过滤数据<br>scripe：<br>    index data<br>        列索引：最大值、最小值、以及他们的offset。<br>            行索引 索引：提供了offset，能够定位具体的数据块以及字节位置（可以直接跳过不需要的行数）<br>    row data：<br>        列数据：<br>            列的元数据：<br>            实际数据<br>    scripe footer：每个stream的信息以及编码<br>        Stream：种类、对应的id、大小、字典、行索引<br>        columnEncoding：字典编码、行程编码<br>        <br><br><br><br>和parquet类型，orc也是使用二进制方式存储的，所以不可以直接读取，orc也是自带解析的，包含很多元数据，这些元数据都是同构protoBuffer进行序列化的，<br>###ORC文件<br>保存在文件系统上的普通二进制文件，一个ORC文件中可以由多个stripe，每一个stripe包含多条记录，按照类进行独立存储，对应parquet的row group概念<br>###文件级元数据<br>包括文件描述符信息PostScript、文件meta信息、所有stript的信息和文件schema信息<br>###stipe<br>一组行形成一个stripe，每次读取的时候以行组为单位，一般HDFS的块大小，保存了每一列的索引和数据<br>###stripe元数据<br>保存stripe的位置，每一列在该stripe的统计信息以及所有stream类型和位置<br>###row group<br>索引的最小单位，一个stripe包含多个row group，默认<span class="hljs-number">10000</span>个值组成。<br>###stream<br>一个stream表示文件中一段有效的数据，包括索引和数据俩类，索引stream保存了每个row group的位置和统计信息，数据stream包括多种类型的数据，具体哪几<br>种是由该列类型和编码方式决定。<br>###统计信息<br>ORC文件中保存了三个层级的统计信息，分别为文件级别、stript级别和row group 级别的，他们都可以作为谓词下推的条件，判断是否可以跳过某些数据，在统计<br>信息中都包含数据和是否由null值，并且对于不同的类型数据设置一些特定的统计信息<br>####file level<br>在ORC文件的末尾会记录文件级别的统计信息，会记录整个文件中columns的统计信息，这些统计信息主要是堆查询的优化，也可以作为一些简单的聚合查询的输出结果<br>####stripe level<br>ORC文件会保存每个字段stripe级别的统计信息，ORC reader使用这些统计信息来确定对于查询语句来说，需要读取哪些stripe记录，比如某个stripe的字段统计<br>max(A) = <span class="hljs-number">10</span> min(A) = <span class="hljs-number">4</span>,那么针对于<span class="hljs-keyword">where</span>条件 A&gt;<span class="hljs-number">10</span> 和 A&lt;<span class="hljs-number">4</span>的话，这个stripe都不会被读取<br>####row level<br>为了进一步避免读取不必要的数据，在逻辑上将一个column的index以一个特定给定的值（<span class="hljs-number">10000</span>）分割多个index组。以<span class="hljs-number">10000</span>条记录为一个组，对数据进行统计。<br>hive的查询引擎会将<span class="hljs-keyword">where</span>条件的约束传递给ORC reader，这些reader根据组级别的统计信息，更细粒度过滤掉不需要的数据，<br></code></pre></td></tr></table></figure><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"> 读取ORC文件是从尾部开始读取的，第一次读取<span class="hljs-number">16</span>KB的大小，尽可能的把postsript和footer数据读取到内存，最后一个字节保存着postscripe的长度，他的长度<br>不会查过<span class="hljs-number">256</span>字节，postscript保存着整个文件的元数据信息，包括文件的压缩格式、文件内部的每一个压缩块的最大长度（每次分配内存的大小）、footer长度，<br>以及一些版本信息。在postscript和footer之间存储着整个文件的统计信息，这部分统计包括每一个stripe中每一列的的信息，主要统计成员数，最大值、最小值、<br>是否有空值等。   <br>  接下来读取footer信息，它包含了没有给stripe的长度和偏移量，该文件的schema信息、整个文件的统计信息以及每一个row <span class="hljs-keyword">group</span>的行数。   <br>  处理stripe时首先从footer读取每个stripe的起始位置和长度、每一个stripe的footer数据（元数据，记录了<span class="hljs-keyword">index</span>和data的长度），整个stripe被分为<br><span class="hljs-keyword">index</span>和data俩部分。stripe内部是按照row <span class="hljs-keyword">group</span>分块的（每一个row <span class="hljs-keyword">group</span>中有多少行在footer中存储），row <span class="hljs-keyword">group</span>按照列存储。每个row <span class="hljs-keyword">group</span>由多个<br>stream保存数据和索引信息。每一个stream的数据会根据该列的数据类型使用特定的压缩算法保存。   <br>  初始化全部元数据的之后，可以根据includes数组指定需要读取的列编号，可以根据SearchArgument参数指定过滤条件。这中间就通过统计信息过滤掉不需要的<br>数据，这时候也会产生比较多的零散数据，ORC会尽可能的合并小文件，减少IO次数。   <br>  ORC不支持读取特定字段类型中指定的部分。   <br>  使用ORC存储的时候，尽量使用HDFS每一个<span class="hljs-keyword">block</span>保存一个stripe，对于一个orc文件来说，stripe的大小一般设置的比<span class="hljs-keyword">block</span>小，否则的话一个stripe就会分<br>部到不同的<span class="hljs-keyword">block</span>上面去，读取的时候就需要远程读取数据。如果设置的stripe只保存在一个<span class="hljs-keyword">block</span>上面的话，如果当前的<span class="hljs-keyword">block</span>剩余空间不足可以存储一个stripe<br>,ORC的<span class="hljs-keyword">write</span>就会将数据打散保存在在<span class="hljs-keyword">block</span>剩余的空间中，直到这个<span class="hljs-keyword">block</span>存储满，这样的话下雨给stripe就可以从下一个<span class="hljs-keyword">block</span>上面开始存储。    <br>  由于ORC使用了更加精确的索引信息，使得读取数据可以从任意一行开始读取，更细粒度的统计信息可以使ORC文件跳过整个row <span class="hljs-keyword">group</span>，orc默认会对任意数据使用<br>zlib压缩，因此orc的存储空间更小。<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>列式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>orc和parquet</title>
    <link href="/2022/02/09/orc-vs-parquet/"/>
    <url>/2022/02/09/orc-vs-parquet/</url>
    
    <content type="html"><![CDATA[<h1 id="orc-vs-parquet"><a href="#orc-vs-parquet" class="headerlink" title="orc vs parquet"></a>orc vs parquet</h1><p>orc和parquet是我们在hive中常用点列式存储格式，各自有各自点特点，下面介绍一下烈士存储点优势以及它们之间点差异。</p><h2 id="列式存储的优势"><a href="#列式存储的优势" class="headerlink" title="列式存储的优势"></a>列式存储的优势</h2><p>列式存储把每列数据存在一起，同类型的列放在一起，通过LRU、字典、bit-packing等编码可以很大程度上减少数据的存储，对同类型数据的压缩效果也比混合类型的压缩好很多。每列数据在一起，我们在查询的是同可以通过映射下推有效的去除不需要的列，其实也是我们在hive、spark等olap等引擎中最常用的优化手段，通过只读取我们需要的列，可以很大程度上面减少io提高我们分析的性能。其次就是通过列式存储，我们可以对列、块、页等数据结构添加上我们需要的索引，通过这些索引和offset我们就可以很好的在数据文件上面使用谓词下推，进一步过滤掉我们不需要的数据，除了索引之外orc和parquet都使用了布隆过滤器，通过添加对字段的布隆过滤器来过滤掉无用的数据，从而使我们的计算更加高效。</p><h2 id="orc-vs-parquet-1"><a href="#orc-vs-parquet-1" class="headerlink" title="orc vs parquet"></a>orc vs parquet</h2><p>1、支持类型上，parquet通过sriping&#x2F;assembly算法完美支持嵌套结构。orc支持复杂的数据类型，对嵌套类型支持较差。<br>2、索引上，parquet和orc都有统计信息以及offset索引，并且都支持列的布隆过滤器。<br>3、文件结构上，parquet是row group、column chunk、pag。orc是stipe、row group、stream。总体结构上面比较类似<br>4、压缩上面，parquet支持snappy、gzip、lzo、brotli、lz4、zstd、lz4_raw；orc支持snappy、zlib、none三种<br>5、扩展性上，parquet支持的组件扩展性更好，对于spark也是默认的存储。而orc是rcfile的升级版本，对hive的支持性优化性更好。<br>6、parquet是cloudera开发的；orc是hortonworks开发的。现在俩家公司合并来了。<br>7、编码上：parquet支持更多的编码格式，<br>8、元数据：parquet使用thrift的TCompactProtocol进行元数据序列化，orc使用protocol buffer进行元数据的序列化</p><p>parquet相对于orc的优势：parquet通过sriping&#x2F;assembly算法完美支持嵌套类型，像json、thrift、protocolbuffer等通过defintion level和repeated leve方便对其进行编码以及压缩，parquet里面使用到的编码方式也更多：int、byte上可以使用增量编码来对数据进行编码。（如果需要支持复杂类型的旋用parquet更加有优势）。同时parqeut使cloudera和twitter创建的，拥有更广的适配性，像spark默认也是parquet存储的，适配性更好。</p><p>orc想对于parquet的优势：想对于parquet，orc支持acid以及update操作，如果在hive上面使用acid，像update merge delete这样的操作可以完美的支持，hive3对事务的支持更加完善了。因为orc就是为hadoop而生的，对hive的适配性更好，在hive上面使用拥有更好的压缩比例以及更好的查询性能。orc的拥有更细粒度的索引信息，能够更好的提高查询性能。</p>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>列式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
