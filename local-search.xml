<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>常用压缩编码</title>
    <link href="/2022/03/08/%E5%B8%B8%E7%94%A8%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81/"/>
    <url>/2022/03/08/%E5%B8%B8%E7%94%A8%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<h1 id="常用压缩算法"><a href="#常用压缩算法" class="headerlink" title="常用压缩算法"></a>常用压缩算法</h1><p>计算机的存储是通过字节来存储的，一个字节是8位，但是我们现实中需要表示的字符是各种各样的，那么如何用有限的字节来表示呢？这个时候编码就出来了，通过使用各种字节的组合来表示出我们需要的字符。而压缩算法是针对我们的存储进行压缩，减少存储的占用，通过对数据使用压缩算法可以有效对减少数据对存储，以及网络io。</p><h2 id="字典编码"><a href="#字典编码" class="headerlink" title="字典编码"></a>字典编码</h2><p>通过一个窗口，分为字典区、待匹配区，如果字段区能够匹配到待匹配区的几个字符，就把待匹配的字符使用索引来代替，窗口不断移动向前移动，直接结束。</p><h3 id="压缩过程"><a href="#压缩过程" class="headerlink" title="压缩过程"></a>压缩过程</h3><ul><li>待压缩的字符串：A B A B C B A B A B C A D   </li><li>窗口大小：10   </li><li>*：代表填充的，为了观察出窗口向前移动   </li><li>|| ||：这个里面是窗口的10个元素</li><li>|：这个是代表字典区和待匹配区的分隔符，字典去大小是6，待匹配是4<br>下面是待压缩的字符串，以及窗口分布情况。<figure class="highlight less"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs less">|| * * * * * * | <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> || <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br></code></pre></td></tr></table></figure>1、刚开始窗口里面只有4个待匹配字符，字典区是空的，所以肯定匹配不到，A就是第一次匹配之后的结果。<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* || * * * * * <span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br><span class="hljs-selector-attr">[A]</span><br></code></pre></td></tr></table></figure>2、字典区只有A，所以B也匹配不到，所以跟第一步相同，继续向前移动,B直接写入<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * || * * * * <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> | <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> || <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br><span class="hljs-selector-attr">[A,B]</span><br></code></pre></td></tr></table></figure>3、字典已经有AB字符串了，待匹配区是ABCB，这个时AB已经在字典区出现了，所以可以使用索引(4,2)表示，4代表字典区A的下表，2代表匹配了几个字符串。另外还需要把待匹配区AB后面的字符C一并写入结果，那就是(4,2)C。这个时候其实是3个字符写入了结果，所以窗口向前移动三位。<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * * * * || * <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> | <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> * * *<br><span class="hljs-selector-attr">[A,B,(4,2)C]</span><br></code></pre></td></tr></table></figure>4、这时候待匹配区是BABA，BAB正好字典是可以匹配到到，跟上面一样，使用索引代替就是[2,3]A,同时窗口向前移动4位<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * * * * * <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">D</span> || * * *<br><span class="hljs-selector-attr">[A,B,(4,2)C,(2,3)A]</span><br></code></pre></td></tr></table></figure>5、待匹配区到BC也在字典中有，所以可以得到结果(0,2)A，窗口向前移动三位<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">* * * * * * * * * || <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">A</span> <span class="hljs-selector-tag">B</span> <span class="hljs-selector-tag">C</span> <span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">D</span> * * * ||<br><span class="hljs-selector-attr">[A,B,(4,2)C,(2,3)A,(0,2)A]</span><br></code></pre></td></tr></table></figure>6、最后一步，只有D了，匹配不到直接写入<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[<span class="hljs-name">A</span>,B,(<span class="hljs-name">4</span>,<span class="hljs-number">2</span>)C,(<span class="hljs-name">2</span>,<span class="hljs-number">3</span>)A,(<span class="hljs-name">0</span>,<span class="hljs-number">2</span>)A,D]<br></code></pre></td></tr></table></figure></li></ul><p>###解压过程</p><p>解压过程其实也比较简单，其实就是把[A,B,(4,2)C,(2,3)A,(0,2)A,D]这个结果进行解释就行</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs armasm">A      : * * * * * * * <span class="hljs-title">||</span> * * * * * A <span class="hljs-title">||</span><br><span class="hljs-keyword">B</span>      : * * * * * * * <span class="hljs-title">||</span> * * * * A <span class="hljs-keyword">B</span> <span class="hljs-title">||</span><br>(<span class="hljs-number">4</span>,<span class="hljs-number">2</span>)C : * * * * * * * <span class="hljs-title">||</span> * A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C <span class="hljs-title">||</span> <br>(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)A : * * * * A <span class="hljs-keyword">B</span> A <span class="hljs-title">||</span> <span class="hljs-keyword">B</span> C <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> A <span class="hljs-title">||</span><br>(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>)A : * A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C <span class="hljs-keyword">B</span> <span class="hljs-title">||</span> A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C A <span class="hljs-title">||</span><br>D      : A <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C <span class="hljs-keyword">B</span> A <span class="hljs-title">||</span> <span class="hljs-keyword">B</span> A <span class="hljs-keyword">B</span> C A D <span class="hljs-title">||</span><br></code></pre></td></tr></table></figure><p>待补充</p>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编码</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>calcite学习</title>
    <link href="/2022/03/08/calcite%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/03/08/calcite%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="calcite学习"><a href="#calcite学习" class="headerlink" title="calcite学习"></a>calcite学习</h1><h2 id="什么是calcite？"><a href="#什么是calcite？" class="headerlink" title="什么是calcite？"></a>什么是calcite？</h2><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><pre><code class="hljs sql">calcite是一个数据库动态管理系统，它提供了一个通用的<span class="hljs-keyword">sql</span>解析、<span class="hljs-keyword">sql</span>校验、<span class="hljs-keyword">sql</span>优化、<span class="hljs-keyword">sql</span>执行的数据库的能力。架构上面是一个灵活的、嵌入式的、可扩展的<br>可以很方便的集成到其他大型项目中去。它的核心是一个关系代数（也是数据库的理论基础），把任何一个查询表示成由关系运算符组成的树。<span class="hljs-keyword">sql</span>转换成关系代数之<br>后，可以进行RBO以及CBO进行规划优化，在保证语义不变的情况下利用一些规则降低成本，来达到一个<span class="hljs-keyword">sql</span>优化的目的。<br></code></pre></td></tr></table></figure><h2 id="什么是关系代数"><a href="#什么是关系代数" class="headerlink" title="什么是关系代数"></a>什么是关系代数</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">关系代数是关系型数据库操作的理论基础，关系代数支持并、差、笛卡尔积、投影和选择等基本运算。在处理一个<span class="hljs-keyword">sql</span>语句的时候，首先将sqlzhu转换成关系表达式，<br>然后通过规则匹配进行相应的优化。<br></code></pre></td></tr></table></figure><h2 id="calcite有哪些优化器"><a href="#calcite有哪些优化器" class="headerlink" title="calcite有哪些优化器"></a>calcite有哪些优化器</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">sql</span>的俩种优化器的类型，RBO CBO,RBO对应这规则优化器根据现在的一些规则对<span class="hljs-keyword">sql</span>进行优化，CBO是成本优化器，他不只是给根据我们写好的规则还可以根据数据<br>的实际情况（包括rownumber cpu io memory）来计算所需要的资源，然后智能选取最小的资源方案。起始俩者根本的不同就是根据<span class="hljs-keyword">cost</span>在继续调整更好的规则去优化<span class="hljs-keyword">sql</span>。缺点就是可能需要一定的计算<br>资源提前算出<span class="hljs-keyword">cost</span>。calcite中的hepPlanner就是RBO优化器；VolcanoPlanner就是CBO优化器。<br></code></pre></td></tr></table></figure><h2 id="RBO"><a href="#RBO" class="headerlink" title="RBO"></a>RBO</h2><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">RBO:</span>Rule-Base Optimizer,基于规则的优化器。<br>基于规则的优化，起始就是结构的匹配和替换，通过匹配语法树上面结构，然后根据结构的特性保持语义不变的大前提下进行变幻乃至替换。RBO中有自己的一套优化<br>规则顺序，无论表数据量是怎么样的。<br></code></pre></td></tr></table></figure><h2 id="CBO"><a href="#CBO" class="headerlink" title="CBO"></a>CBO</h2><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">在RBO中因为都是人为的经验所得的规则，有写系统使用的就是在代码中人工定义了一些规则的优先级来进行<span class="hljs-keyword">bi变换，不能保证变幻之后是否比原来更优，是否性能会</span><br><span class="hljs-keyword"></span>更好，灵活性也相对差一些。而CBO是基于cost（统计信息和代价模型）的优化器。是可能不同的计算方案会有不同的成本，不同的方案其实也是基于不同规则之间的变换来得到的，计算所有<br>规则的成本，选择成本最低的方案为最后的方案得到了最优的结果。主要应用在离线场景<br></code></pre></td></tr></table></figure><h2 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">1</span>、解析<span class="hljs-keyword">sql</span>，把<span class="hljs-keyword">sql</span>转化成为AST<br><span class="hljs-number">2</span>、语法检查，根据数据库中的元数据进行语法验证<br><span class="hljs-number">3</span>、语义分析，构建逻辑执行计划<br><span class="hljs-number">4</span>、逻辑计划优化，优化器的核心，根据前面生成的逻辑计划按照相应的<span class="hljs-keyword">rule</span>进行有优化<br><span class="hljs-number">5</span>、物理执行<br></code></pre></td></tr></table></figure><h2 id="calcite的处理流程"><a href="#calcite的处理流程" class="headerlink" title="calcite的处理流程"></a>calcite的处理流程</h2><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-number">1</span>、sql <span class="hljs-number">2</span> sqlnode:吧sql转换成为抽象语法树<br><span class="hljs-number">2</span>、sqlnode validate：对转换的sqlnode进行校验<br><span class="hljs-number">3</span>、sqlnode <span class="hljs-number">2</span> relnode：把抽象语法树转换成为关系代数<br><span class="hljs-number">4</span>、decorrelate <span class="hljs-literal">and</span> trim field：去相关，去掉不用的字段<br><span class="hljs-number">5</span>、optimize replace rel.root traitset<br><span class="hljs-number">6</span>、顺序应用多个program：执行子查询消除、调用relDecorrelator 和 relFileldTrimmer进行去相关消除字段<br><span class="hljs-number">7</span>、然后会有俩次setroot：<br><span class="hljs-number">8</span>、第一次setroot<br>    a、通过深度遍历，从叶子节点往上遍历<br>    b、首先遍历到叶子节点，为该relnode创建一个relset和一个包含初始relnode的relsubset<br>    c、注册rule对初始的relnode从下往上进行<span class="hljs-keyword">match</span>，<span class="hljs-keyword">match</span>的rule放到<span class="hljs-built_in">queue</span>里面去<br>    d、在这个过程中会初始化cost和维护importance(下边还会init，不知道什么适合确切用到)<br>    e、非叶子节点不同的是会有了input，input会替换为对应的subset<br><span class="hljs-number">9</span>、change <span class="hljs-keyword">trait</span>：rel.root修改对应的<span class="hljs-keyword">trait</span>转成对应的类型，并对转换后的<span class="hljs-keyword">trait</span>添加新的subset（新添加的subset是空的）<br><span class="hljs-number">10</span>、在上面新建出来的relsubset，重新调用setroot，发现<span class="hljs-keyword">trait</span>之间的差异并注册conventer到上面空的subset里面<br><span class="hljs-number">11</span>、然后进入findbestrel<br><span class="hljs-number">12</span>、首先初始化所有的relset的importance<br><span class="hljs-number">13</span>、应用<span class="hljs-built_in">queue</span>中的规则，知道<span class="hljs-built_in">queue</span>中没有数据，找到最优的rel treecc<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>sql优化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>calcite</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hivesql优化</title>
    <link href="/2022/03/04/hivesql%E4%BC%98%E5%8C%96/"/>
    <url>/2022/03/04/hivesql%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="如何写好一个hql"><a href="#如何写好一个hql" class="headerlink" title="如何写好一个hql"></a>如何写好一个hql</h1><p>  作为一个数据开发工程师，hive sql是我们必备的技能，可能大家都知道一些基本的优化方法（例如：使用分区、小表join大表、不使用distinct、where条件尽量写到自查询里面减少数据量等等），但是你有没有想过为什么？是不是真的对执行效率有提升。</p><p>  下面为大家介绍一下hive的优化器以及一些常见的sql技巧。</p><h2 id="常见的优化器"><a href="#常见的优化器" class="headerlink" title="常见的优化器"></a>常见的优化器</h2><p>  如果你想查看hive的优化器，可以从github上面拉一份hive的源码，在org.apache.hadoop.hive.ql.optimizer目录下可以看到hive里面有哪些逻辑优化器。 </p><h3 id="列裁剪优化器"><a href="#列裁剪优化器" class="headerlink" title="列裁剪优化器"></a>列裁剪优化器</h3><p>官方解释    </p><blockquote><p>Implementation of one of the rule-based optimization steps. ColumnPruner gets<br>the current operator tree. The  tree is traversed to find out the columns<br>used for all the base tables. If all the columns for a table are not used, a<br>select is pushed on top of that table (to select only those columns). Since<br>this changes the row resolver, the tree is built again. This can be optimized<br>later to patch the tree</p></blockquote><p>  我们sql中都会用到列裁剪。所谓列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。以我们的日历记录表为例：</p><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> uid,uname,sex<br><span class="hljs-keyword">from</span> user_info<br><span class="hljs-keyword">where</span> dt <span class="hljs-operator">&gt;=</span> <span class="hljs-string">&#x27;20190201&#x27;</span> <span class="hljs-keyword">and</span> dt <span class="hljs-operator">&lt;=</span> <span class="hljs-string">&#x27;20190224&#x27;</span><br><span class="hljs-keyword">and</span> age <span class="hljs-operator">=</span> <span class="hljs-number">18</span>;<br></code></pre></td></tr></table></figure><p>  当列很多或者数据量很大时，如果select * 或者不指定分区，全列扫描和全表扫描效率都很低。</p><p>  Hive中与列裁剪优化相关的配置项是hive.optimize.cp，与分区裁剪优化相关的则是hive.optimize.pruner，默认都是true。在HiveSQL解析阶段对应的则是ColumnPruner逻辑优化器。</p><h3 id="PDD-Predicate-Pushdown"><a href="#PDD-Predicate-Pushdown" class="headerlink" title="PDD(Predicate Pushdown)"></a>PDD(Predicate Pushdown)</h3><p>  谓词下推优化器，在许多数据库中都会使用到，简单说就是把后面的查询条件前置，以下面sql来讲：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <br>    a.<span class="hljs-operator">*</span>, b.<span class="hljs-operator">*</span> <br><span class="hljs-keyword">from</span> a <span class="hljs-keyword">join</span> b <br><span class="hljs-keyword">on</span>  (a.col1 <span class="hljs-operator">=</span> b.col1) <br><span class="hljs-keyword">where</span> a.col1 <span class="hljs-operator">&gt;</span> <span class="hljs-number">20</span> <span class="hljs-keyword">and</span> b.col2 <span class="hljs-operator">&gt;</span> <span class="hljs-number">40</span><br></code></pre></td></tr></table></figure><p>  大部分人可能认为应该通过将 a.col1 &gt; 20 and b.col2 &gt; 40 放到a表和b表里做子查询，减少数据量输入，这样做没有任何问题，但是上面这种写法，通过谓词下推优化器可以实现在读取a表和b表的同时将不符合条件的数据过滤掉。所以有时候不需要通过写自查询减少数据量输入，上面这种语法更加干净整洁。</p><h3 id="mapjoin"><a href="#mapjoin" class="headerlink" title="mapjoin"></a>mapjoin</h3><blockquote><p>Map joins have restrictions on which joins can be converted  as memory restrictions</p></blockquote><p>  简单来说map join就是把小表加入到内存中，直接把相同的key进行join处理，减少shuffle过程，可以极大提高工作效率，适用于码表或者一些大表和小表join的情况。下面是执行流程图：</p><p>  1.先启动Task A；Task A启动一个MapReduce的local task；通过该local task把small table data的数据读取进来；之后会生成一个HashTable Files；之后将该文件加载到分布式缓存中；  </p><p>  2.启动MapJoin Task，读大表的数据，每读一个就会和Distributed Cache中的数据关联一次，关联上后进行输出，整个阶段中没有reduce 和 shuffle。   </p><h3 id="SkewJoinOptimizer"><a href="#SkewJoinOptimizer" class="headerlink" title="SkewJoinOptimizer"></a>SkewJoinOptimizer</h3><p>  数据倾斜优化器：主要应用在发生倾斜的任务中。数据倾斜的情况相信大家也经常遇到，如mapreduce任务进度长时间等待在99%或者一些内存溢出的情况等。产生数据倾斜的原因有很多种，倾斜的原理是很多相同的key用同一个reduce处理，导致处理的任务过大，如共有200亿数据，有100亿为男生 100亿为女生</p><p>上述情况发生了数据倾斜，两个reduce承受了所有的压力，不会有第三个reduce处理数据。   </p><p>  hive倾斜的优化器把一个shuffle拆分成两个shuffle过程： </p><p>1、第一个shuffle过程：给key增加一个随机数，因此生成的hash值也不尽相同，相同的随机数+相同的原key生成的hash值依然一样，如此数据就会放到一起；</p><p>2、第二个shuffle过程：将前边的随机数去掉，重新聚合可以得到想要的结果。</p><blockquote><p>通过分批处理解决数据倾斜问题的方案也是在spark等其他大数据计算引擎中通用且有效的方法。</p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>  hive的优化器其实很多，通过学习hive优化器的原理，让我们可以写出效率更高的sql，如果有兴趣的话可以从github下载hive源码去学习更多优化器的详细内容。</p><h2 id="hql语法进阶与常用小技巧"><a href="#hql语法进阶与常用小技巧" class="headerlink" title="hql语法进阶与常用小技巧"></a>hql语法进阶与常用小技巧</h2><h3 id="CTE查询"><a href="#CTE查询" class="headerlink" title="CTE查询"></a>CTE查询</h3><p>  通过as将查询语句作为一个临时存储表给后边的查询使用，可以使你的sql更加灵活简洁。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">WITH <br>t1 as (select * from t1 where name = &#x27;1&#x27;),<br>t2 as (select * from t2 where name = &#x27;2&#x27;)<br>select * from t1 join t2 on t1.id = t2.id<br></code></pre></td></tr></table></figure><h3 id="列匹配正则表达式"><a href="#列匹配正则表达式" class="headerlink" title="列匹配正则表达式"></a>列匹配正则表达式</h3><p>  一个表一千多个列痛苦不痛苦？通过设置  SET hive.support.quoted.identifiers &#x3D; none；可以输出正则匹配到的列并排除一些不需要的列。</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">SELECT</span> <span class="hljs-symbol">`^o.*`</span> <span class="hljs-keyword">from</span> table_name;<br></code></pre></td></tr></table></figure><h3 id="多表插入"><a href="#多表插入" class="headerlink" title="多表插入"></a>多表插入</h3><p>  有时候我们会遇到不同的查询条件或者需要将一个大表中的数据拆分到不同的数据表中，如果每个数据表写一个sql会造成资源浪费效率也比较低，<br>这时候from语法就来了，如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">from table_name<br>insert overwrite table table1 where id &gt; 100<br>insert overwrite table table2 where name = &#x27;lee&#x27;<br></code></pre></td></tr></table></figure><blockquote><p>可以一次读取table_name表数据插入不同表中</p></blockquote><h3 id="cube-rollup"><a href="#cube-rollup" class="headerlink" title="cube rollup"></a>cube rollup</h3><p>  很多时候我们除了需要在报表中罗列出每个具体项的数据，还需要进行汇总，并且是不同维度的汇总。如果在展示表格的时候汇总，可能会比较慢，我们一般是把结果计算出来之后，以’ALL’或者’总计’,’汇总’等字样作为项的名称，然后放入汇总值。如果仅仅是所有行的汇总，一次聚合就搞定。但是不同维度的汇总就会很麻烦,这时候就轮到cube rollup出场了。</p><h4 id="cube函数"><a href="#cube函数" class="headerlink" title="cube函数"></a>cube函数</h4><p>  cube(a,b,c)则首先会对(a,b,c)进行group by，然后依次是(a,b),(a,c),(a),(b,c),(b),(c),最后在对全表进行group by，他会统计所选列中值的所有组合的聚合，用cube函数就可以完成所有维度的聚合工作。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a,b,c from table_name group by a,b,c with cube<br></code></pre></td></tr></table></figure><blockquote><p>如果我们想要手动实现cube函数需要把所有维度的聚合都用union all来汇总.<br>可以说cube函数方便了用户的使用.<br>但是我并不用知道所有维度的聚合,我就想要col1,(col2,col3)的怎么办?</p></blockquote><p>grouping sets</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a,b,c from table_name group by a,b,c grouping sets(a,(b,c)); <br></code></pre></td></tr></table></figure><h4 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h4><p>  rullup函数是cube的子集,以最左侧维度为主,按照顺序依次进行聚合.<br>例如聚合的维度为 col1,col2,col3 使用rollup聚合的字段分别为 col1,(col1,col2),(col1,col3),(col1,col2,col3)。    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a,b,c from table_name group by a,b,c with rollup;<br></code></pre></td></tr></table></figure><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>  提到行转列相信大家首先想到的是explode，配合lateral view可以实现一行变多行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select a, b, c from table_name t lateral view explode(t.e)  as c;<br>--可以配合split使用<br>select a, b, c from table_name t lateral view explode(split(t.e,&#x27;,&#x27;))  as c;<br>--可以配合json<br></code></pre></td></tr></table></figure><p>  另外lateral view 还可以配置json_tuple使用，抽取json的多个字段。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">SELECT <br>    *<br>from<br>(select  *  from  table_name ) t1<br>lateral view json_tuple(json_field,&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;) state_json as a,b,c<br></code></pre></td></tr></table></figure><blockquote><p>这样会把json里面的三个value加载到已有字段后</p></blockquote><p>  行转列对应hive里面的udtf，如果现有的函数满足不了你的需求，可以开发一个udtf来实现自定义的行转列操作。</p><h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>  多行变成一行，其实就是聚合操作，通过group by操作可以实现聚合操作，有时候我们需要把字段信息也保留下来，这时候就用到了collect_ws、collect_set。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">--根据a b 聚合，对c去重<br>select a,b,collect_set(c) from table_name group by a,b;<br>--根据a b 聚合，然后把c放到一个数据<br>select a,b,collect_list(c) from table_name group by a,b;<br>--根据a b 聚合，然后使用，拼接c字段<br>select a,b,concat_ws(&quot;,&quot;,collect_list(c)) from table_name group by a,b;<br></code></pre></td></tr></table></figure><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><p>  窗口函数是用于分析的一类函数，要理解窗口函数要先从聚合函数说起。 大家都知道聚合函数是将某列中多行的值合并为一行，比如sum、count等。而窗口函数则可以在本行内做运算，得到多行的结果，即每一行对应一行的值。 通用的窗口函数可以用下面的语法来概括：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">Function() Over (Partition By Column1,Column2,Order By Column3);<br></code></pre></td></tr></table></figure><p>  窗口函数分三类：聚合型窗口函数、分析型窗口函数、取值型窗口函数</p><h4 id="聚合型"><a href="#聚合型" class="headerlink" title="聚合型"></a>聚合型</h4><p>  聚合型即SUM(),MIN(),MAX(),AVG(),COUNT()这些常见的聚合函数。 聚合函数配合窗口函数使用可以使计算更加灵活。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select *,sum/count/max/min/avg(file_name) over(partition by file_name order by field_name) from table_name<br></code></pre></td></tr></table></figure><h4 id="分析型"><a href="#分析型" class="headerlink" title="分析型"></a>分析型</h4><p>  分析型即RANk(),ROW_NUMBER(),DENSE_RANK()等常见的排序用的窗口函数，不过他们也是有区别的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">select *,<br>--俩个元素相同会跳过下一个序列号<br>rank() over (order by create_time) as user_rank,<br>--生成连续的序列号<br>row_number() over (order by create_time) as user_row_number,<br>--俩个元素相同不会跳过下一个序列号<br>dense_rank() over (order by create_time) as user_dense_rank<br>from table_name<br></code></pre></td></tr></table></figure><h4 id="取值型"><a href="#取值型" class="headerlink" title="取值型"></a>取值型</h4><p>  这几个函数可以通过字面意思可知，LAG是迟滞的意思，用于统计窗口内往上第n行值；LEAD是LAG的反义词，用于统计窗口内往下第n行值；FIRST_VALUE是该列到目前为止的首个值，而LAST_VALUE是到目前行为止的最后一个值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">--以贷款为例<br>SELECT *,<br>--取上一笔贷款的日期,缺失默认填NULL<br>LAG(orderdate, 1) OVER(PARTITION BY name ORDER BY orderdate) AS last_dt,<br>--取下一笔贷款的日期,缺失指定填&#x27;1970-1-1&#x27;<br>LEAD(orderdate, 1,&#x27;1970-1-1&#x27;) OVER(PARTITION BY name ORDER BY orderdate) AS next_dt,<br>--去第一个的日期<br>FIRST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS first_dt,<br>--取最后一个日期<br>LAST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS latest_dt<br>from table_name<br></code></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>本文主要分享了hive的优化器和SQL使用技巧。最后，不管你是SQL boy or  SQL girl，只要掌握一些技巧，相信都能够Happy SQL querying 😊。</p>]]></content>
    
    
    <categories>
      
      <category>sql优化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hive-mapjoin探索</title>
    <link href="/2022/03/02/hive-mapjoin%E6%8E%A2%E7%B4%A2/"/>
    <url>/2022/03/02/hive-mapjoin%E6%8E%A2%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="让你真正了解mapjoin参数"><a href="#让你真正了解mapjoin参数" class="headerlink" title="让你真正了解mapjoin参数"></a>让你真正了解mapjoin参数</h1><p>首先创建好small_table和big_table俩个表。直接给俩个表执行join。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs hiveql">set hive.cbo.enable=false;<br>set hive.auto.convert.join=false; <br>set hive.auto.convert.join.noconditionaltask=false; <br>explain select t1.* from small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>接下来分析执行计划，一共俩个stage，第一个stage是join，第二个是进行fetch数据</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">STAGE</span> DEPENDENCIES:<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">1</span> is a root stage<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">0</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">1</span> 其实stage0就是fetch算子，把结果拉到client端<br></code></pre></td></tr></table></figure><p>分析stage-0,看fetch数据就是fetch operator</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">Stage</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Stage-0</span><br>  <span class="hljs-attribute">Fetch Operator</span><br><span class="hljs-attribute">    limit</span><span class="hljs-punctuation">:</span> <span class="hljs-string">-1 代表所有</span><br>    <span class="hljs-attribute">Processor Tree</span><span class="hljs-punctuation">:</span><br>      ListSink <br></code></pre></td></tr></table></figure><p>分析主要的阶段stage-1，这个阶段scan俩个表的数据，同时抽取join key进行了reduce操作。</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs arcade">STAGE PLANS:<br>  Stage: Stage<span class="hljs-number">-1</span><br>    <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Reduce</span>  代表这个stage就是一个mapreduce任务<br>      <span class="hljs-built_in">Map</span> Operator Tree:  这个代表<span class="hljs-built_in">map</span>算子的阶段，<span class="hljs-built_in">map</span>针对上面sql其实就是对俩个表进行scan，同时取对应对字段<br>      <span class="hljs-built_in">Reduce</span> Operator Tree:  这个代表<span class="hljs-built_in">reduce</span>算子的阶段，针对join对字段进行聚合<br></code></pre></td></tr></table></figure><p>先看下Map Operator Tree，需要注意的是join会自动把join字段的null去掉。 id is not null，看TableScan t1和TableScan t2区别就是第一个是有个value表达式，因为select只select了t1表的字段，其他没有什么区别，都是扫描表的数据。</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>            alias: t1<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              Reduce Output Operator<br>                key expressions: id (<span class="hljs-keyword">type</span>: int)<br>                sort <span class="hljs-keyword">order</span>: +<br>                <span class="hljs-built_in">Map</span><span class="hljs-params">-reduce</span> partition columns: id (<span class="hljs-keyword">type</span>: int)<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">94</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10222</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                value expressions: name (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">string</span>),<span class="hljs-params">...</span>f7 (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">string</span>)<br>          TableScan<br>            alias: t2<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              Reduce Output Operator<br>                key expressions: id (<span class="hljs-keyword">type</span>: int)<br>                sort <span class="hljs-keyword">order</span>: +<br>                <span class="hljs-built_in">Map</span><span class="hljs-params">-reduce</span> partition columns: id (<span class="hljs-keyword">type</span>: int)<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">9552</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">1039073</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br></code></pre></td></tr></table></figure><p>最后就是reduce阶段</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs sqf">Reduce Operator Tree:<br>  <span class="hljs-built_in">Join</span> Operator<br>    condition map:<br>         Inner <span class="hljs-built_in">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>    <span class="hljs-built_in">keys</span>:<br>      <span class="hljs-number">0</span> id (<span class="hljs-built_in">type</span>: int)<br>      <span class="hljs-number">1</span> id (<span class="hljs-built_in">type</span>: int)<br>    outputColumnNames: <span class="hljs-variable">_col0</span>, <span class="hljs-variable">_col1</span>, <span class="hljs-variable">_col2</span>, <span class="hljs-variable">_col3</span>, <span class="hljs-variable">_col4</span>, <span class="hljs-variable">_col5</span>, <span class="hljs-variable">_col6</span>, <span class="hljs-variable">_col7</span>, <span class="hljs-variable">_col8</span>, <span class="hljs-variable">_col9</span>, <span class="hljs-variable">_col10</span><br>    Statistics: Num rows: <span class="hljs-number">10507</span> Data <span class="hljs-built_in">size</span>: <span class="hljs-number">1142980</span> Basic stats: COMPLETE Column stats: NONE<br>    File Output Operator<br>      compressed: <span class="hljs-literal">false</span><br>      Statistics: Num rows: <span class="hljs-number">10507</span> Data <span class="hljs-built_in">size</span>: <span class="hljs-number">1142980</span> Basic stats: COMPLETE Column stats: NONE<br>      table:<br>          input <span class="hljs-built_in">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat<br>          output <span class="hljs-built_in">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat<br>          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br></code></pre></td></tr></table></figure><p>以上就是没有开始mapjoin，让我们把mapjoin打开试一试，小表的大小是10M。测试一下开启mapjoin。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs hiveql">set hive.cbo.enable=false;<br>set hive.auto.convert.join=false;<br>set hive.auto.convert.join.noconditionaltask=false;<br>set hive.auto.convert.join=true;<br>explain select t1.* from small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>先分析一下stage之间的依赖</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">STAGE</span> DEPENDENCIES:<br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">4</span> is a root stage , consists of Stage-<span class="hljs-number">5</span>, Stage-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">5</span> has a backup stage: Stage-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">3</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">5</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">1</span><br>  <span class="hljs-attribute">Stage</span>-<span class="hljs-number">0</span> depends <span class="hljs-literal">on</span> stages: Stage-<span class="hljs-number">3</span>, Stage-<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>首先看stage4，这个stage是root，也就是最顶部的，stage Conditional Operator这是一个条件运算符，包括5和1。这快看不太懂什么意思，我们先看看看stage-1这个后边没有任何依赖。</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-1</span><br>    <span class="hljs-built_in">Map</span> Reduce<br>      <span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>          TableScan<br>      Reduce Operator Tree:<br>        <span class="hljs-keyword">Join</span> Operator<br>          condition <span class="hljs-built_in">map</span>:<br>               Inner <span class="hljs-keyword">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>          keys:<br>            <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>            <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br></code></pre></td></tr></table></figure><p>stage-1可以看出来，跟不开启mapjoin是一摸一样的，可能这个主要是现实最初的执行计划的。没有太多用处，即诶啊来看下stage-0这个主要是代表我们fetch数据的算子，也就是客户端拉取结果的</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">Stage</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Stage-0</span><br>    <span class="hljs-attribute">Fetch Operator</span><br><span class="hljs-attribute">      limit</span><span class="hljs-punctuation">:</span> <span class="hljs-string">-1</span><br>      <span class="hljs-attribute">Processor Tree</span><span class="hljs-punctuation">:</span><br>        ListSink<br></code></pre></td></tr></table></figure><p>跟原来的一样，这个stage-0依赖于3、1，1我们已经看过了，我们直接看3</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-3</span><br>    <span class="hljs-built_in">Map</span> Reduce<br>      <span class="hljs-built_in">Map</span> Operator Tree:<br>          TableScan<br>            alias: t2<br>            Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">955184</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">103902452</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>            Filter Operator<br>              predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>              Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">955184</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">103902452</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>              <span class="hljs-built_in">Map</span> <span class="hljs-keyword">Join</span> Operator<br>                condition <span class="hljs-built_in">map</span>:<br>                     Inner <span class="hljs-keyword">Join</span> <span class="hljs-number">0</span> <span class="hljs-keyword">to</span> <span class="hljs-number">1</span><br>                keys:<br>                  <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>                  <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br>                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10<br>                Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">1050702</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">114292699</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                File Output Operator<br>                  compressed: <span class="hljs-literal">false</span><br>                  Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">1050702</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">114292699</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>                  table:<br>                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat<br>                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat<br>                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br>      <span class="hljs-built_in">Local</span> Work:<br>        <span class="hljs-built_in">Map</span> Reduce <span class="hljs-built_in">Local</span> Work<br></code></pre></td></tr></table></figure><p>这个阶段就是关键点，可以看到这是一个mapreduce任务，但是没有Reduce Operator，使用一个local work替代了，而且join的条件都放在了Map Operator里面了，所以这个就是mapjoin的真正逻辑，接下来看一下local work，因为stage-3依赖于stage-5，接下来看stage-5</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Stage: Stage<span class="hljs-number">-5</span><br>   <span class="hljs-built_in">Map</span> Reduce <span class="hljs-built_in">Local</span> Work<br>     Alias -&gt; <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Local</span> Tables:<br>       t1<br>         Fetch Operator<br>           limit: <span class="hljs-number">-1</span><br>     Alias -&gt; <span class="hljs-built_in">Map</span> <span class="hljs-built_in">Local</span> Operator Tree:<br>       t1<br>         TableScan<br>           alias: t1<br>           Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">95521</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10390248</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>           Filter Operator<br>             predicate: id is <span class="hljs-literal">not</span> <span class="hljs-built_in">null</span> (<span class="hljs-keyword">type</span>: <span class="hljs-built_in">boolean</span>)<br>             Statistics: Num <span class="hljs-keyword">rows</span>: <span class="hljs-number">95521</span> <span class="hljs-built_in">Data</span> size: <span class="hljs-number">10390248</span> Basic stats: COMPLETE Column stats: <span class="hljs-literal">NONE</span><br>             HashTable Sink Operator<br>               keys:<br>                 <span class="hljs-number">0</span> id (<span class="hljs-keyword">type</span>: int)<br>                 <span class="hljs-number">1</span> id (<span class="hljs-keyword">type</span>: int)<br></code></pre></td></tr></table></figure><p>可以看到这个一个mapreduce的本地工作，跟stage-3里面的local work是能对上的，这个很好理解，就是把t1表通过本地加载，条件跟mapreduce里面的是一样的。</p><p>接下来，以后我们查看表的执行计划可以很快的分析出来。接下来让我们验证下看起cbo是否对mapjoin有影响。<br>首先我们对small小表直接插入100M数据，这个数据量不满足了mapjoin对条件，首先看关闭CBO</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.cbo.<span class="hljs-attribute">enable</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.<span class="hljs-attribute">noconditionaltask</span>=<span class="hljs-literal">false</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br>explain select t1.* <span class="hljs-keyword">from</span> small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure><p>这个结果跟不开启mapjoin是一致对，因为俩个表对大小都超过了我们设定都阈值。这边可以判断cdp读取hive的元数据没有影响mapjoin的执行，因为我看到有的文章说有时候开启cbo以后mapjoin不生效（不过别人的文章是说的使用tez执行引擎，所以这点还有待验证）</p><p>我们测试开启cbo，开启cbo成本应该是从hive都元数据里面取数据都，但是由于我们是直接往hdfs上面放都数据，元数据small表还是10M</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.cbo.<span class="hljs-attribute">enable</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.mapjoin.smalltable.<span class="hljs-attribute">filesize</span>=460000000;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.<span class="hljs-attribute">noconditionaltask</span>=<span class="hljs-literal">true</span>;<br><span class="hljs-built_in">set</span> hive.auto.convert.join.noconditionaltask.<span class="hljs-attribute">size</span>=10000000; <br><span class="hljs-built_in">set</span> hive.auto.convert.<span class="hljs-attribute">join</span>=<span class="hljs-literal">true</span>;<br>explain select t1.* <span class="hljs-keyword">from</span> small_table t1 join big_table t2 on t1.id = t2.id;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>sql优化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>parquet</title>
    <link href="/2022/03/02/parquet-md/"/>
    <url>/2022/03/02/parquet-md/</url>
    
    <content type="html"><![CDATA[<h1 id="PARQUET"><a href="#PARQUET" class="headerlink" title="PARQUET"></a>PARQUET</h1><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><figure class="highlight node-repl"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs node-repl">1、更高的压缩比可以减少磁盘的使用<br>2、更少的IO操作使用映射下推和谓词下推，可以读取需要的列，过滤掉不需要的列，减少不必要的数据扫描，尤其对于表字段比较多的时候优势明显。<br><span class="hljs-meta">&gt;</span> <span class="language-javascript">映射下推：列式存储最突出的优势，数据扫描的时候，只需要扫描需要的列，避免全表扫描</span><br><span class="hljs-meta">&gt;</span> <span class="language-javascript">谓词下推：将过滤条件在最底层执行减少结果集</span><br></code></pre></td></tr></table></figure><h2 id="如何提升parquet的查询性能"><a href="#如何提升parquet的查询性能" class="headerlink" title="如何提升parquet的查询性能"></a>如何提升parquet的查询性能</h2><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-number">1</span>、对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以更好的利用最大值最小值实现谓词下推<br><span class="hljs-number">2</span>、减少行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的<span class="hljs-built_in">I</span><span class="hljs-operator">/</span><span class="hljs-built_in">O</span>负载。<br></code></pre></td></tr></table></figure><h2 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h2><p>row group &gt; column chunk &gt; page</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs css">row group行组：每一个行组包含一定的行数，一般对应一个HDFS文件块，Parquet读写的时候会将整个行组缓存在内存中<br>column chunk列块：在一个行组中，每一列保存在一个列块中，一个列块中的类型都是相同的，不同类型的列块可以使用不同d的算法进行压缩。<br>page页：每一个列块被划分成多个页，页时最小的编码单位，在同一个列块的不同页可能使用不同的编码方式<br><br>一个parquet文件，在文件头和文件尾都是<span class="hljs-number">4</span>个字节的magic <span class="hljs-selector-tag">code</span>，校验是否是parquet文件。<br>然后尾部magic <span class="hljs-selector-tag">code</span>之前是<span class="hljs-selector-tag">footer</span> length，是文件元数据的大小，通过这个大小就可以计算出y元数据<span class="hljs-selector-tag">footer</span>的偏移量<br><span class="hljs-selector-tag">footer</span>中包括：<br><span class="hljs-number">1</span>、每个行组的元信息：由多少个列块组成；<br><span class="hljs-number">2</span>、每个列块的元信息：类型、路径、编码、第一个数据页的位置、第一个索引页的位置、压缩/未压缩大小、以及一些额外的配置kv<br><span class="hljs-number">3</span>、又有三种pag：<br>    数据页主要存储defination levels、repeatition levels、value<br>    字典页：存储列值的编码字段<br>    索引页：用来存储该页<br></code></pre></td></tr></table></figure><h2 id="parquet的编码格式"><a href="#parquet的编码格式" class="headerlink" title="parquet的编码格式"></a>parquet的编码格式</h2><p><a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">https://github.com/apache/parquet-format/blob/master/Encodings.md</a></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">0</span>、PLAIN：如果没有其他的有效编码就不进行编码，也就是plain<br><span class="hljs-number">2</span>、PLAIN_DICTIONARY /<span class="hljs-number">8</span>、RLE_DICTIONARY：原始数据的字典编码，字典存储在字典页中对应page中的字典页。字典中的值使用RLE/<span class="hljs-type">bit</span>-packing<br>回合编码存储位整数。<br>   字典页格式（PLAIN）：字典中的条数-按字典顺序-使用普通编码<br>   数据页格式：用于对存储位<span class="hljs-number">1</span>个字节条目id进行编码的位宽（最大<span class="hljs-number">32</span>位），这个id使用RLE/<span class="hljs-type">bit</span>-packing打包编码的值，在<span class="hljs-number">2.0</span>中使用RLE/<span class="hljs-keyword">DICTIONARY</span>   <br><span class="hljs-number">3</span>、Run Length <span class="hljs-keyword">Encoding</span> / <span class="hljs-type">Bit</span>-Packing Hybrid：这种编码使用位打包和运行长度编码的组合来更有效地存储重复值<br>   RLE:行程编码，重复的值计数。<br>   <span class="hljs-type">bit</span>-packing：位打包主要用来解决序列化所需要的位数<br><span class="hljs-number">4</span>、<span class="hljs-type">Bit</span>-packed：仅支持编码重复和定义级别<br><span class="hljs-number">5</span>、Delta <span class="hljs-keyword">Encoding</span>：int32、int64<br><span class="hljs-number">6</span>、Delta-length byte <span class="hljs-keyword">array</span>：BYTE_ARRAY<br><span class="hljs-number">7</span>、Delta Strings：BYTE_ARRAY<br><span class="hljs-number">9</span>、Byte Stream Split： <span class="hljs-type">FLOAT</span> <span class="hljs-type">DOUBLE</span><br></code></pre></td></tr></table></figure><h2 id="parquet元数据使用什么序列化方式"><a href="#parquet元数据使用什么序列化方式" class="headerlink" title="parquet元数据使用什么序列化方式"></a>parquet元数据使用什么序列化方式</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql">parqut的元数据是在整个<span class="hljs-type">row</span> <span class="hljs-keyword">group</span>都写完成之后再进行写入的，这时候写入的文件的元数据信息全部都保存再了内存中，如果要读取的话也是需要把元数据全部都<br>读取到内存中才行。<br>把内存中的数据序列化到磁盘中，使用了thrift的TCompactProtocol高性能序列化方式。<br>thrit相对于其他序列化方式性能比较好，兼容性可能比较差。<br></code></pre></td></tr></table></figure><blockquote><p><a href="https://blog.51cto.com/u_15080016/2620917">https://blog.51cto.com/u_15080016/2620917</a></p></blockquote><h2 id="Sriping-x2F-Assembly算法"><a href="#Sriping-x2F-Assembly算法" class="headerlink" title="Sriping&#x2F;Assembly算法"></a>Sriping&#x2F;Assembly算法</h2><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stata">打散/组装算法，主要parquet如何打散和组装数据的。这个算法主要通过repetition <span class="hljs-keyword">levels</span>和definition <span class="hljs-keyword">levels</span>来描述文件的拆分和组装。重复级别主要<br>用来描述repeated的节点，在写入的适合等于它和前面的值从那一层节点不共享的。在读取的适合根据该值可以推断出哪一层需要创建一个新的节点；定义级别主要<br>用来记录哪些值使没有的使null值。可以很好的支持嵌套类型的存储。<br>repeated <span class="hljs-keyword">levels</span>主要是用来一列的作用的，defintition <span class="hljs-keyword">levels</span>用来描述一行的数据。主要防止一列有空值导致错位。<br>不同的列跳转使用了有限状态机<br></code></pre></td></tr></table></figure><h2 id="parquet的bloomfilter"><a href="#parquet的bloomfilter" class="headerlink" title="parquet的bloomfilter"></a>parquet的bloomfilter</h2><p><a href="https://github.com/apache/parquet-format/blob/master/BloomFilter.md">https://github.com/apache/parquet-format/blob/master/BloomFilter.md</a></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">我们都知道，在parquet和orc等列式存储中，都会用到一些索引和统计信息：记录下一些列的最大值最小值，记录字典等。，来对查询进行谓词下推，<br>直接下推到文件上面，减少数据的传输。谓词下推可以说是数据库查询优化的一大手段。<br>但是在有一些没有最大值最小值，字典太大、数据基数非常大的情况下布隆过滤器可以起到很好的优化操作。<br>布隆过滤器可以过滤掉肯定不存在的数据，可能存在的数据来相应查询。可能存在的数据又可以通过构建<span class="hljs-type">bit</span>数组的大小来决定的。<span class="hljs-type">bit</span>数据占用的空间<br>非常小，以至于用很小的空间就可以达到很好的过滤效果。<br>其实布隆过滤器主要的目标就是为·高基数·的列启用谓词下推，同时使用更少的存储空间。同时如果某一列没有启用布隆过滤器也不会影响性能。<br><br>parquet使用的是一种split block布隆过滤器。就是一个列有多个block，每个block有<span class="hljs-number">8</span>个小块组成，小块的话其实就是<span class="hljs-type">bit</span>数组。然后通过<br>加盐（对应<span class="hljs-type">int</span>就是*对应一个数组保存着<span class="hljs-number">8</span>个<span class="hljs-number">32</span>位的整数一一对应）移位来取得每个<span class="hljs-type">bit</span>数组中的具体位置，然后进行插入。其中就是为了把对应的<br>值进行散列，使用了奇数*常数右移的一种散列整数方法。块越多准确率越高<br><br>针对其他类型的数据，parquet使用xxhash来做的转换<br><br>数据的话也是相当于元数据使用的thrift做的序列化<br><br>bloomfilter存储在了footer下面的<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span>的<span class="hljs-keyword">column</span> chunk下面，作为列的元数据。<br>敏感信息应该使用列加密<br></code></pre></td></tr></table></figure><h2 id="parquet文件支持哪些压缩方式"><a href="#parquet文件支持哪些压缩方式" class="headerlink" title="parquet文件支持哪些压缩方式"></a>parquet文件支持哪些压缩方式</h2><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">snappy:</span>google提供<br><span class="hljs-symbol">gzip:</span>基于RFC <span class="hljs-number">1952</span>定义的 GZIP 格式（zlib权威）<br><span class="hljs-symbol">lzo:</span><br><span class="hljs-symbol">brotli:</span><br><span class="hljs-symbol">lz4:</span><br><span class="hljs-symbol">zstd:</span><br><span class="hljs-symbol">lz4_raw:</span><br></code></pre></td></tr></table></figure><h2 id="parquet支持哪些数据类型"><a href="#parquet支持哪些数据类型" class="headerlink" title="parquet支持哪些数据类型"></a>parquet支持哪些数据类型</h2><p>文件格式的数据类型</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">BOOLEAN</span>：<span class="hljs-number">1</span> 位布尔值<br><span class="hljs-attribute">INT32</span>：<span class="hljs-number">32</span> 位有符号整数<br><span class="hljs-attribute">INT64</span>：<span class="hljs-number">64</span> 位有符号整数<br><span class="hljs-attribute">INT96</span>：<span class="hljs-number">96</span> 位有符号整数<br><span class="hljs-attribute">FLOAT</span>：IEEE <span class="hljs-number">32</span> 位浮点值<br><span class="hljs-attribute">DOUBLE</span>：IEEE <span class="hljs-number">64</span> 位浮点值<br><span class="hljs-attribute">BYTE_ARRAY</span>：任意长的字节数组。<br></code></pre></td></tr></table></figure><p>逻辑数据类型</p><h2 id="parquet的列索引"><a href="#parquet的列索引" class="headerlink" title="parquet的列索引"></a>parquet的列索引</h2><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs applescript">columnIndex和offsetIndex；索引的结构和长度存储在l了columnchunk中<br>columnIndex：这个可以根据制定的列值直接到数据页。可以用于谓词下推上面。<br>offsetindex：这个可以按照行数进行索引，这个主要用来其他列有了过滤，跳过许多数据，那么这个列也会根据<span class="hljs-built_in">offset</span>去挑过对应的数据。<br><span class="hljs-number">1</span>、索引里面右数据页的最大值和最小值<br><span class="hljs-number">2</span>、针对排序的列只需要读取包含数据相关的确定的数据页<br><span class="hljs-number">3</span>、针对于谓词下推，可以通过字典以及索引找到确定的索引页<br><span class="hljs-number">4</span>、如果不需要操作也不会带来额外的开销<br><span class="hljs-number">5</span>、如果是已经排序的，仅仅存储边界元素<br><span class="hljs-number">6</span>、对于有序的列还会运用二分查找<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>列式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>orc</title>
    <link href="/2022/03/02/orc/"/>
    <url>/2022/03/02/orc/</url>
    
    <content type="html"><![CDATA[<h1 id="ORC-Optimized-Record-Columnar"><a href="#ORC-Optimized-Record-Columnar" class="headerlink" title="ORC(Optimized Record Columnar)"></a>ORC(Optimized Record Columnar)</h1><h2 id="orc支持哪些数据类型"><a href="#orc支持哪些数据类型" class="headerlink" title="orc支持哪些数据类型"></a>orc支持哪些数据类型</h2><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><pre><code class="hljs sql">整数<br>  <span class="hljs-type">boolean</span>：<span class="hljs-number">1</span>bit<br>  tinyint：<span class="hljs-number">8</span>bit<br>  <span class="hljs-type">smallint</span>：<span class="hljs-number">16</span>bit<br>  <span class="hljs-type">int</span>：<span class="hljs-number">32</span>bit<br>  <span class="hljs-type">bigint</span>：<span class="hljs-number">64</span>bit<br>浮点<br>  <span class="hljs-type">float</span>、<span class="hljs-keyword">double</span><br>字符串<br>  string、<span class="hljs-type">char</span>、<span class="hljs-type">varchar</span><br>字节<br>  <span class="hljs-type">binary</span> blobs<br>日期<span class="hljs-operator">/</span>时间<br>  <span class="hljs-type">timestamp</span>、<span class="hljs-type">timestamp</span> <span class="hljs-keyword">with</span> <span class="hljs-keyword">local</span> <span class="hljs-type">time</span> zone、<span class="hljs-type">date</span><br>复合类型<br>  struct、list、map、<span class="hljs-keyword">union</span><br></code></pre></td></tr></table></figure><h2 id="orc支持哪些索引"><a href="#orc支持哪些索引" class="headerlink" title="orc支持哪些索引"></a>orc支持哪些索引</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">file <span class="hljs-keyword">level</span>文件级别：有关整个文件中每一列的统计信息（存储在页脚）<br>stripe <span class="hljs-keyword">level</span>条带级别：每个条带的每列值统计信息（存储在页脚）<br><span class="hljs-keyword">row</span> <span class="hljs-keyword">level</span> 行级别：每<span class="hljs-number">10000</span>h行每列中的统计信息（存储在行组）<br>布隆过滤器：针对特定的列使用布隆过滤器有效的进行谓词下推 orc.bloom.<span class="hljs-keyword">filter</span>.<span class="hljs-keyword">columns</span>，他跟<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">index</span>交错分布的，首先根据索引缩小数据范围<br>再使用布隆过滤器进一步过滤数据。<br></code></pre></td></tr></table></figure><h2 id="acid支持"><a href="#acid支持" class="headerlink" title="acid支持"></a>acid支持</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">目的不是支持OLTP操作，而是支持百万计事务的更新，不是百万次事务。同时有了acid还可以保证流式数据插入到数据表中。<br>启用acid的表会在对应的目录下面创建增量目录。delta_xxx/bucket_xxx,<br>当delta变多了之后，会自动启用一些小的压缩，将一组事务h合并位一个单独的delta。<br>当增量文件变打了之后，会启动任务去重写基础+增量文件<br>针对orc文件通过id、bucket和row id来保证数据的唯一，当数据update、<span class="hljs-keyword">delete</span>、insert就是用到这三个值，对应的序列化<span class="hljs-number">0</span> 插入 <span class="hljs-number">1</span>更新 <span class="hljs-number">2</span>删除<br>https:<span class="hljs-regexp">//</span>orc.apache.org<span class="hljs-regexp">/docs/</span>acid.html<br></code></pre></td></tr></table></figure><h2 id="针对hive的操作"><a href="#针对hive的操作" class="headerlink" title="针对hive的操作"></a>针对hive的操作</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> istari <span class="hljs-keyword">SET</span> FILEFORMAT ORC：将表新分区存储位orc<br><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> istari [<span class="hljs-keyword">PARTITION</span> partition_spec] CONCATENATE：针对分区进行合并，在stripe级别上面进行合并<br>hive <span class="hljs-comment">--orcfiledump &lt;path_to_file&gt;：orc文件信息</span><br>hive <span class="hljs-comment">--orcfiledump -d &lt;path_to_file&gt;：显示数据</span><br><br>一些参数：<br>orc.compress=ZLIB/<span class="hljs-keyword">NONE</span>/SNAPPY<br>orc.compress.size=<span class="hljs-number">262144</span>：针对每个chunk size的压缩大小<br>oorc.stripe.size=<span class="hljs-number">67108864</span>：每个stripe的大小<br>orc.<span class="hljs-keyword">row</span>.<span class="hljs-keyword">index</span>.stride=<span class="hljs-number">10000</span>：索引的条目数<br>orc.<span class="hljs-keyword">create</span>.<span class="hljs-keyword">index</span>=<span class="hljs-keyword">true</span>：是否创建索引<br>orc.bloom.<span class="hljs-keyword">filter</span>.<span class="hljs-keyword">columns</span>=&quot;&quot;：需要开启布隆过滤器的列<br>orc.bloom.<span class="hljs-keyword">filter</span>.fpp=<span class="hljs-number">0.05</span>：布隆过滤器的准确率<br><br>TBLPROPERTIES (&quot;orc.compress&quot;=&quot;NONE&quot;);<br><br>https://orc.apache.org/docs/hive-ddl.html<br>https://orc.apache.org/docs/hive-config.html<br></code></pre></td></tr></table></figure><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">1</span>、ORC是列式存储，有多种文件压缩方式，压缩比例也很高。   <br><span class="hljs-number">2</span>、文件可切分。可切分的意义在于，可以控制task的数量以及减少数据的输入。   <br><span class="hljs-number">3</span>、提供了多种索引：<span class="hljs-keyword">row</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">index</span>、bloom <span class="hljs-keyword">filter</span> <span class="hljs-keyword">index</span>。   <br><span class="hljs-number">4</span>、支持复杂的数据结构   <br></code></pre></td></tr></table></figure><h2 id="为什么列式存储可以加快olap的查询速度"><a href="#为什么列式存储可以加快olap的查询速度" class="headerlink" title="为什么列式存储可以加快olap的查询速度"></a>为什么列式存储可以加快olap的查询速度</h2><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs arcade">每一列的所有元素都是顺序存储的，这样可以带来的好处就是：   <br><span class="hljs-number">1</span>、查询的时候不需要扫描全部数据，只需要扫描对应的列数据就可以，还有就是orc保存着每一列的统计信息包括<span class="hljs-built_in">min</span> <span class="hljs-built_in">max</span> <span class="hljs-built_in">sum</span>等，这样配合hive的谓词下推优化器<br>可以实现谓词下推到数据结构的底层，完美减少了数据的输入，没有必须的数据再数据读取的时候完全避免了。   <br><span class="hljs-number">2</span>、每一列的成员都是同构的，可以针对不同的数据类型使用更高效的压缩算法，进一步减少I/O。   <br><span class="hljs-number">3</span>、每一类的成员都是同构性，更加实用CPU pipeline的编码方式，减少CPU缓存失效<br></code></pre></td></tr></table></figure><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs arcade">ORC的结构中包含了负责类型和原始类型，前者包括list struck <span class="hljs-built_in">map</span> <span class="hljs-built_in">union</span> 后者有<span class="hljs-built_in">boolean</span>、整数、浮点等。struct的孩子可能包括多个孩子节点，<span class="hljs-built_in">map</span>有俩个<br>孩子节点，list有一个孩子节点。每个<span class="hljs-built_in">schema</span>根节点是哟个struck类型， 所有的column按照树的中序遍历顺序编号。<br>&gt; ORC只需要存储<span class="hljs-built_in">schema</span>树中叶子节点的值，而中间的非叶子节点只是做了一层代理，只需要负责孩子节点值的数据读取，只有真正的的叶子节点才会读取数据，然后<br>&gt; 由树节点封装成对应的数据结构返回<br></code></pre></td></tr></table></figure><h2 id="orc相对rcfile优势"><a href="#orc相对rcfile优势" class="headerlink" title="orc相对rcfile优势"></a>orc相对rcfile优势</h2><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>、每个task单独输出单一文件，减少了namenode的负载<br><span class="hljs-number">2</span>、支持复杂数据类型<br><span class="hljs-number">3</span>、文件级别的轻量化索引：可以跳过对应row group。定位行数据<br><span class="hljs-number">4</span>、根据数据类型来进行基于<span class="hljs-keyword">block的数据压缩，多种压缩编码 </span>RLE <span class="hljs-keyword">DIRECT</span><br><span class="hljs-keyword"></span><span class="hljs-number">5</span>、使用独立的recordReader并行读取同一个文件<br><span class="hljs-number">6</span>、不必扫描标识就可以对文件进行切分<br><span class="hljs-number">7</span>、限定读写需要的内存<br><span class="hljs-number">8</span>、元数据以protocol <span class="hljs-keyword">buffer存储，允许增加删除域</span><br><span class="hljs-keyword"></span><span class="hljs-number">9</span>、压缩方面支持更多的压缩方式 zlib snappy<br></code></pre></td></tr></table></figure><h2 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs clean">尾部开始读取，首先是magic <span class="hljs-keyword">code</span><br>postscript：file footer的长度、文件的压缩方式、压缩大小、元数据的长度、文件的版本号<br>file footer：stripe列表，每个stripe的行数，列的数据类型，列的聚合信息count min max sum<br>    stripe information：offset、索引的长度、数据的长度、footer的长度、以及strope的行数<br>    type information：schema信息是表大成了一颗树结构，通过先序遍历序列化protocol buffer到文件里面<br>    column statistics：行数、最大值、最小值、求和、是否有null，可能对不同的数据类型定义了不同的统计信息<br>    user metadata：这个是用户自己可以设置一些kv存储，方便读取的时候使用。<br>    file metadata：整个列的（可能多个stripe）统计信息，来再数据分片的时候就进行数据谓词下推过滤数据<br>scripe：<br>    index data<br>        列索引：最大值、最小值、以及他们的offset。<br>            行索引 索引：提供了offset，能够定位具体的数据块以及字节位置（可以直接跳过不需要的行数）<br>    row data：<br>        列数据：<br>            列的元数据：<br>            实际数据<br>    scripe footer：每个stream的信息以及编码<br>        Stream：种类、对应的id、大小、字典、行索引<br>        columnEncoding：字典编码、行程编码<br>        <br><br><br><br>和parquet类型，orc也是使用二进制方式存储的，所以不可以直接读取，orc也是自带解析的，包含很多元数据，这些元数据都是同构protoBuffer进行序列化的，<br>###ORC文件<br>保存在文件系统上的普通二进制文件，一个ORC文件中可以由多个stripe，每一个stripe包含多条记录，按照类进行独立存储，对应parquet的row group概念<br>###文件级元数据<br>包括文件描述符信息PostScript、文件meta信息、所有stript的信息和文件schema信息<br>###stipe<br>一组行形成一个stripe，每次读取的时候以行组为单位，一般HDFS的块大小，保存了每一列的索引和数据<br>###stripe元数据<br>保存stripe的位置，每一列在该stripe的统计信息以及所有stream类型和位置<br>###row group<br>索引的最小单位，一个stripe包含多个row group，默认<span class="hljs-number">10000</span>个值组成。<br>###stream<br>一个stream表示文件中一段有效的数据，包括索引和数据俩类，索引stream保存了每个row group的位置和统计信息，数据stream包括多种类型的数据，具体哪几<br>种是由该列类型和编码方式决定。<br>###统计信息<br>ORC文件中保存了三个层级的统计信息，分别为文件级别、stript级别和row group 级别的，他们都可以作为谓词下推的条件，判断是否可以跳过某些数据，在统计<br>信息中都包含数据和是否由null值，并且对于不同的类型数据设置一些特定的统计信息<br>####file level<br>在ORC文件的末尾会记录文件级别的统计信息，会记录整个文件中columns的统计信息，这些统计信息主要是堆查询的优化，也可以作为一些简单的聚合查询的输出结果<br>####stripe level<br>ORC文件会保存每个字段stripe级别的统计信息，ORC reader使用这些统计信息来确定对于查询语句来说，需要读取哪些stripe记录，比如某个stripe的字段统计<br>max(A) = <span class="hljs-number">10</span> min(A) = <span class="hljs-number">4</span>,那么针对于<span class="hljs-keyword">where</span>条件 A&gt;<span class="hljs-number">10</span> 和 A&lt;<span class="hljs-number">4</span>的话，这个stripe都不会被读取<br>####row level<br>为了进一步避免读取不必要的数据，在逻辑上将一个column的index以一个特定给定的值（<span class="hljs-number">10000</span>）分割多个index组。以<span class="hljs-number">10000</span>条记录为一个组，对数据进行统计。<br>hive的查询引擎会将<span class="hljs-keyword">where</span>条件的约束传递给ORC reader，这些reader根据组级别的统计信息，更细粒度过滤掉不需要的数据，<br></code></pre></td></tr></table></figure><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"> 读取ORC文件是从尾部开始读取的，第一次读取<span class="hljs-number">16</span>KB的大小，尽可能的把postsript和footer数据读取到内存，最后一个字节保存着postscripe的长度，他的长度<br>不会查过<span class="hljs-number">256</span>字节，postscript保存着整个文件的元数据信息，包括文件的压缩格式、文件内部的每一个压缩块的最大长度（每次分配内存的大小）、footer长度，<br>以及一些版本信息。在postscript和footer之间存储着整个文件的统计信息，这部分统计包括每一个stripe中每一列的的信息，主要统计成员数，最大值、最小值、<br>是否有空值等。   <br>  接下来读取footer信息，它包含了没有给stripe的长度和偏移量，该文件的schema信息、整个文件的统计信息以及每一个row <span class="hljs-keyword">group</span>的行数。   <br>  处理stripe时首先从footer读取每个stripe的起始位置和长度、每一个stripe的footer数据（元数据，记录了<span class="hljs-keyword">index</span>和data的长度），整个stripe被分为<br><span class="hljs-keyword">index</span>和data俩部分。stripe内部是按照row <span class="hljs-keyword">group</span>分块的（每一个row <span class="hljs-keyword">group</span>中有多少行在footer中存储），row <span class="hljs-keyword">group</span>按照列存储。每个row <span class="hljs-keyword">group</span>由多个<br>stream保存数据和索引信息。每一个stream的数据会根据该列的数据类型使用特定的压缩算法保存。   <br>  初始化全部元数据的之后，可以根据includes数组指定需要读取的列编号，可以根据SearchArgument参数指定过滤条件。这中间就通过统计信息过滤掉不需要的<br>数据，这时候也会产生比较多的零散数据，ORC会尽可能的合并小文件，减少IO次数。   <br>  ORC不支持读取特定字段类型中指定的部分。   <br>  使用ORC存储的时候，尽量使用HDFS每一个<span class="hljs-keyword">block</span>保存一个stripe，对于一个orc文件来说，stripe的大小一般设置的比<span class="hljs-keyword">block</span>小，否则的话一个stripe就会分<br>部到不同的<span class="hljs-keyword">block</span>上面去，读取的时候就需要远程读取数据。如果设置的stripe只保存在一个<span class="hljs-keyword">block</span>上面的话，如果当前的<span class="hljs-keyword">block</span>剩余空间不足可以存储一个stripe<br>,ORC的<span class="hljs-keyword">write</span>就会将数据打散保存在在<span class="hljs-keyword">block</span>剩余的空间中，直到这个<span class="hljs-keyword">block</span>存储满，这样的话下雨给stripe就可以从下一个<span class="hljs-keyword">block</span>上面开始存储。    <br>  由于ORC使用了更加精确的索引信息，使得读取数据可以从任意一行开始读取，更细粒度的统计信息可以使ORC文件跳过整个row <span class="hljs-keyword">group</span>，orc默认会对任意数据使用<br>zlib压缩，因此orc的存储空间更小。<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>列式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>orc和parquet</title>
    <link href="/2022/02/09/orc-vs-parquet/"/>
    <url>/2022/02/09/orc-vs-parquet/</url>
    
    <content type="html"><![CDATA[<h1 id="orc-vs-parquet"><a href="#orc-vs-parquet" class="headerlink" title="orc vs parquet"></a>orc vs parquet</h1><p>orc和parquet是我们在hive中常用点列式存储格式，各自有各自点特点，下面介绍一下烈士存储点优势以及它们之间点差异。</p><h2 id="列式存储的优势"><a href="#列式存储的优势" class="headerlink" title="列式存储的优势"></a>列式存储的优势</h2><p>列式存储把每列数据存在一起，同类型的列放在一起，通过LRU、字典、bit-packing等编码可以很大程度上减少数据的存储，对同类型数据的压缩效果也比混合类型的压缩好很多。每列数据在一起，我们在查询的是同可以通过映射下推有效的去除不需要的列，其实也是我们在hive、spark等olap等引擎中最常用的优化手段，通过只读取我们需要的列，可以很大程度上面减少io提高我们分析的性能。其次就是通过列式存储，我们可以对列、块、页等数据结构添加上我们需要的索引，通过这些索引和offset我们就可以很好的在数据文件上面使用谓词下推，进一步过滤掉我们不需要的数据，除了索引之外orc和parquet都使用了布隆过滤器，通过添加对字段的布隆过滤器来过滤掉无用的数据，从而使我们的计算更加高效。</p><h2 id="orc-vs-parquet-1"><a href="#orc-vs-parquet-1" class="headerlink" title="orc vs parquet"></a>orc vs parquet</h2><p>1、支持类型上，parquet通过sriping&#x2F;assembly算法完美支持嵌套结构。orc支持复杂的数据类型，对嵌套类型支持较差。<br>2、索引上，parquet和orc都有统计信息以及offset索引，并且都支持列的布隆过滤器。<br>3、文件结构上，parquet是row group、column chunk、pag。orc是stipe、row group、stream。总体结构上面比较类似<br>4、压缩上面，parquet支持snappy、gzip、lzo、brotli、lz4、zstd、lz4_raw；orc支持snappy、zlib、none三种<br>5、扩展性上，parquet支持的组件扩展性更好，对于spark也是默认的存储。而orc是rcfile的升级版本，对hive的支持性优化性更好。<br>6、parquet是cloudera开发的；orc是hortonworks开发的。现在俩家公司合并来了。<br>7、编码上：parquet支持更多的编码格式，<br>8、元数据：parquet使用thrift的TCompactProtocol进行元数据序列化，orc使用protocol buffer进行元数据的序列化</p><p>parquet相对于orc的优势：parquet通过sriping&#x2F;assembly算法完美支持嵌套类型，像json、thrift、protocolbuffer等通过defintion level和repeated leve方便对其进行编码以及压缩，parquet里面使用到的编码方式也更多：int、byte上可以使用增量编码来对数据进行编码。（如果需要支持复杂类型的旋用parquet更加有优势）。同时parqeut使cloudera和twitter创建的，拥有更广的适配性，像spark默认也是parquet存储的，适配性更好。</p><p>orc想对于parquet的优势：想对于parquet，orc支持acid以及update操作，如果在hive上面使用acid，像update merge delete这样的操作可以完美的支持，hive3对事务的支持更加完善了。因为orc就是为hadoop而生的，对hive的适配性更好，在hive上面使用拥有更好的压缩比例以及更好的查询性能。orc的拥有更细粒度的索引信息，能够更好的提高查询性能。</p>]]></content>
    
    
    <categories>
      
      <category>数据存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>列式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
